{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build feature and output arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# lists for all data\n",
    "all_turbine_types = []\n",
    "all_hub_heights = []\n",
    "all_capacities = []\n",
    "all_commissioning_dates = []\n",
    "all_production_data = []\n",
    "\n",
    "# load JSON-file\n",
    "with open(f\"data/WPPs+production+wind.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    WPP_production_wind = json.load(file)[:10] # only use the first 10 wpps for now\n",
    "\n",
    "# collect data\n",
    "for wpp in WPP_production_wind:\n",
    "    all_turbine_types.append(wpp[\"Turbine\"])\n",
    "    all_hub_heights.append(wpp[\"Hub_height\"] if not pd.isna(wpp[\"Hub_height\"]) else 100)\n",
    "    all_capacities.append(wpp[\"Capacity\"])\n",
    "    all_commissioning_dates.append(wpp[\"Commission_date\"] if wpp[\"Commission_date\"] != \"nan\" else \"2015/06\")\n",
    "    all_production_data.append(wpp[\"Production\"])\n",
    "\n",
    "# replace NaN-values in turbine types with unambiguous names\n",
    "nan_counter = 1\n",
    "for idx, turbine in enumerate(all_turbine_types):\n",
    "    if pd.isna(turbine):\n",
    "        all_turbine_types[idx] = f\"nan{nan_counter}\"\n",
    "        nan_counter += 1\n",
    "\n",
    "# One-Hot-Encoding for turbine types\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "turbine_types_onehot = encoder.fit_transform(np.array(all_turbine_types).reshape(-1, 1))\n",
    "\n",
    "# correct data format for commissioning dates\n",
    "all_commissioning_dates = [\n",
    "    \"2015/06\" if date == \"nan\" else f\"{date}/06\" if isinstance(date, str) and \"/\" not in date else date\n",
    "    for date in all_commissioning_dates\n",
    "]\n",
    "\n",
    "# convert to datetime\n",
    "standardised_dates = pd.to_datetime(all_commissioning_dates, format='%Y/%m')\n",
    "\n",
    "# calculate age\n",
    "current_date = pd.Timestamp(\"2024-12-01\")\n",
    "ages = current_date.year * 12 + current_date.month - (standardised_dates.year * 12 + standardised_dates.month)\n",
    "\n",
    "# create combined features and output lists\n",
    "combined_features_raw = []\n",
    "output_raw = []\n",
    "\n",
    "# convert data in feature arrays\n",
    "for idx, production_data in enumerate(all_production_data):\n",
    "    num_rows = len(production_data)\n",
    "\n",
    "    # repetitions for common features\n",
    "    turbine_type_repeated = np.tile(turbine_types_onehot[idx], (num_rows, 1))\n",
    "    hub_height_repeated = np.full((num_rows, 1), all_hub_heights[idx])\n",
    "    capacity_repeated = np.full((num_rows, 1), all_capacities[idx])\n",
    "    age_repeated = np.full((num_rows, 1), ages[idx])\n",
    "\n",
    "    # extract production values and wind speeds\n",
    "    production_values = np.array([entry[1] for entry in production_data]).reshape(-1, 1)\n",
    "    wind_speeds = np.array([entry[2] for entry in production_data]).reshape(-1, 1)\n",
    "\n",
    "    # combine all features\n",
    "    combined_chunk = np.hstack((\n",
    "        turbine_type_repeated,\n",
    "        hub_height_repeated,\n",
    "        capacity_repeated,\n",
    "        age_repeated,\n",
    "        wind_speeds\n",
    "    ))\n",
    "\n",
    "    # add the data\n",
    "    combined_features_raw.append(combined_chunk)\n",
    "    output_raw.append(production_values)\n",
    "\n",
    "# combine all data chunks to one array\n",
    "combined_features_raw = np.vstack(combined_features_raw)\n",
    "output_raw = np.vstack(output_raw)\n",
    "\n",
    "# tound all values to two decimal places\n",
    "combined_features_raw = np.round(combined_features_raw, decimals=2)\n",
    "output_raw = np.round(output_raw, decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale feature vector and define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Validation Features Shape: (357969, 13)\n",
      "Train and Validation Targets Shape: (357969, 1)\n",
      "Test Features Shape: (89493, 13)\n",
      "Test Targets Shape: (89493, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "combined_features = combined_features_raw.copy()\n",
    "output = output_raw.copy()\n",
    "\n",
    "# Standardisierung der numerischen Features (individuell!)\n",
    "wind_scaler = StandardScaler()\n",
    "combined_features[:, -1] = wind_scaler.fit_transform(combined_features[:, -1].reshape(-1, 1)).flatten() # scale wind speeds\n",
    "scaler = StandardScaler()\n",
    "combined_features[:, -2] = scaler.fit_transform(combined_features[:, -2].reshape(-1, 1)).flatten() # scale ages\n",
    "combined_features[:, -3] = scaler.fit_transform(combined_features[:, -3].reshape(-1, 1)).flatten() # scale capacities\n",
    "combined_features[:, -4] = scaler.fit_transform(combined_features[:, -4].reshape(-1, 1)).flatten() # scale hub heights\n",
    "\n",
    "# Trainings- und Testaufteilung\n",
    "train_features, test_features, train_targets, test_targets = train_test_split(combined_features, output, test_size=0.2, shuffle=True)\n",
    "\n",
    "# Dataset-Klasse f√ºr PyTorch\n",
    "class WindPowerDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.targets[index]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Erstellung der PyTorch-Datasets\n",
    "train_val_dataset = WindPowerDataset(train_features, train_targets)\n",
    "test_dataset = WindPowerDataset(test_features, test_targets)\n",
    "\n",
    "# Ausgabe der Formen\n",
    "print(\"Train and Validation Features Shape:\", train_features.shape)\n",
    "print(\"Train and Validation Targets Shape:\", train_targets.shape)\n",
    "print(\"Test Features Shape:\", test_features.shape)\n",
    "print(\"Test Targets Shape:\", test_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLP, self).__init__()\n",
    "        # define layers as nn.Sequential, for better visualisability in TensorBoard and compatibility with DeepExplainer from SHAP\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, input_size):\n",
    "#         super(MLP, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, 256)\n",
    "#         self.fc2 = nn.Linear(256, 128)\n",
    "#         self.fc3 = nn.Linear(128, 64)\n",
    "#         self.fc4 = nn.Linear(64, 32)\n",
    "#         self.fc5 = nn.Linear(32, 1)\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         x = self.relu(self.fc2(x))\n",
    "#         x = self.relu(self.fc3(x))\n",
    "#         x = self.relu(self.fc4(x))\n",
    "#         x = self.fc5(x)  # No activation in the output layer for regression\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, use_dropout=False, dropout_rate=0.3, \n",
    "                 use_batch_norm=False, activation_fn=nn.ReLU):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # Erste Schicht\n",
    "        layers.append(nn.Linear(input_size, 256))\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(256))\n",
    "        layers.append(activation_fn())\n",
    "\n",
    "        # Zweite Schicht\n",
    "        layers.append(nn.Linear(256, 128))\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(128))\n",
    "        layers.append(activation_fn())\n",
    "\n",
    "        # Dritte Schicht\n",
    "        layers.append(nn.Linear(128, 64))\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(64))\n",
    "        layers.append(activation_fn())\n",
    "\n",
    "        # Vierte Schicht\n",
    "        layers.append(nn.Linear(64, 32))\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(32))\n",
    "        layers.append(activation_fn())\n",
    "\n",
    "        # Dropout nach der letzten versteckten Schicht (optional)\n",
    "        if use_dropout:\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        # Ausgabeschicht\n",
    "        layers.append(nn.Linear(32, 1))\n",
    "\n",
    "        # Modell zusammenstellen\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter search: Training, Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-20 12:41:41,099] A new study created in memory with name: no-name-564ca2f9-a46f-42b7-a74e-ee25f9645677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminated TensorBoard process\n",
      "TensorBoard started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6abf53c07a834f698af55c68aae296d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: batch_size=32, lr=0.00066, number_epochs=75, use_dropout=False, dropout_rate=0.13931621148485404, use_batch_norm=False\n",
      "  Fold 1/4\n",
      "[W 2024-12-20 12:42:23,122] Trial 0 failed with parameters: {'batch_size': 43, 'lr': 0.0006565783354276016, 'number_epochs': 75, 'use_dropout': False, 'dropout_rate': 0.13931621148485404, 'use_batch_norm': False} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_22092\\1051819355.py\", line 82, in objective\n",
      "    outputs = model(batch_x)\n",
      "              ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_22092\\2794831206.py\", line 45, in forward\n",
      "    return self.model(x)\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\container.py\", line 219, in forward\n",
      "    input = module(input)\n",
      "            ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 117, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-12-20 12:42:23,125] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[121], line 120\u001b[0m\n\u001b[0;32m    117\u001b[0m best_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    119\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m, pruner\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39mpruners\u001b[38;5;241m.\u001b[39mMedianPruner())\n\u001b[1;32m--> 120\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBeste Parameterkombination:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[121], line 82\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     81\u001b[0m     batch_x, batch_y \u001b[38;5;241m=\u001b[39m batch_x\u001b[38;5;241m.\u001b[39mto(device), batch_y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 82\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n\u001b[0;32m     84\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[120], line 45\u001b[0m, in \u001b[0;36mMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "import shutil\n",
    "import os\n",
    "import subprocess\n",
    "import platform\n",
    "import numpy as np\n",
    "import optuna\n",
    "import time\n",
    "import pynvml\n",
    "import psutil\n",
    "\n",
    "# Ressourcen√ºberwachung initialisieren\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == torch.device(\"cuda\"):\n",
    "    pynvml.nvmlInit()\n",
    "    gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # GPU 0 verwenden\n",
    "\n",
    "n_splits = 4  # Anzahl der Folds f√ºr Kreuzvalidierung\n",
    "log_dir = \"runs\"\n",
    "\n",
    "# TensorBoard-Prozess beenden\n",
    "try:\n",
    "    if platform.system() == \"Windows\":\n",
    "        subprocess.run([\"taskkill\", \"/IM\", \"tensorboard.exe\", \"/F\"], check=True)\n",
    "    else:\n",
    "        subprocess.run([\"pkill\", \"-f\", \"tensorboard\"], check=True)\n",
    "    print(\"Terminated TensorBoard process\")\n",
    "except subprocess.CalledProcessError:\n",
    "    print(\"No TensorBoard process found or could not be terminated\")\n",
    "\n",
    "# Log-Verzeichnis l√∂schen\n",
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "subprocess.Popen([\"tensorboard\", \"--logdir=runs\", \"--bind_all\"])\n",
    "print(\"TensorBoard started.\")\n",
    "\n",
    "# Bewertungsfunktion\n",
    "def objective(trial):\n",
    "    batch_size = int(2 ** round(np.log2(trial.suggest_int(\"batch_size\", 16, 128))))\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    number_epochs = trial.suggest_int(\"number_epochs\", 10, 100)\n",
    "    use_dropout = trial.suggest_categorical(\"use_dropout\", [True, False])\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    use_batch_norm = trial.suggest_categorical(\"use_batch_norm\", [True, False])\n",
    "\n",
    "    print(f\"Evaluating: batch_size={batch_size}, lr={lr:.5f}, number_epochs={number_epochs}, \"\n",
    "          f\"use_dropout={use_dropout}, dropout_rate={dropout_rate}, use_batch_norm={use_batch_norm}\")\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    len_train_val_dataset = len(train_val_dataset)\n",
    "\n",
    "    input_size = train_features.shape[1]\n",
    "    avg_val_loss = 0.0  # Durchschnittlicher Validierungsverlust\n",
    "    start_time = time.time()  # Zeitmessung starten\n",
    "    max_memory_usage = 0  # Maximale Speicher-Auslastung\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(range(len_train_val_dataset)), 1):\n",
    "        print(f\"  Fold {fold}/{kf.n_splits}\")\n",
    "\n",
    "        torch.jit.script(MLP(input_size=input_size, use_dropout=use_dropout, dropout_rate=dropout_rate, use_batch_norm=use_batch_norm)).to(device)\n",
    "        model = MLP(input_size=input_size, use_dropout=use_dropout, dropout_rate=dropout_rate, use_batch_norm=use_batch_norm).to(device)\n",
    "\n",
    "        train_fold_dataset = Subset(train_val_dataset, train_idx)\n",
    "        val_fold_dataset = Subset(train_val_dataset, val_idx)\n",
    "\n",
    "        train_loader = DataLoader(train_fold_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_fold_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        criterion = nn.HuberLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        for epoch in range(number_epochs):\n",
    "            model.train()\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if device == torch.device(\"cuda\"):\n",
    "                    memory_info = pynvml.nvmlDeviceGetMemoryInfo(gpu_handle)\n",
    "                    max_memory_usage = max(max_memory_usage, memory_info.used / 1024 ** 2)  # MB\n",
    "                else:\n",
    "                    max_memory_usage = max(max_memory_usage, psutil.virtual_memory().used / 1024 ** 2)  # MB\n",
    "\n",
    "            model.eval()\n",
    "            fold_val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in val_loader:\n",
    "                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                    val_outputs = model(batch_x)\n",
    "                    fold_val_loss += criterion(val_outputs, batch_y).item()\n",
    "            avg_val_loss += fold_val_loss / len(val_loader)\n",
    "\n",
    "    avg_val_loss /= kf.n_splits\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    weighted_score = 0.7 * avg_val_loss + 0.15 * elapsed_time + 0.15 * max_memory_usage\n",
    "\n",
    "    trial.set_user_attr(\"resource_usage\", max_memory_usage)\n",
    "    trial.set_user_attr(\"elapsed_time\", elapsed_time)\n",
    "    trial.set_user_attr(\"avg_val_loss\", avg_val_loss)\n",
    "    trial.set_user_attr(\"weighted_score\", weighted_score)\n",
    "\n",
    "    return weighted_score\n",
    "\n",
    "# Optuna-Optimierung starten\n",
    "best_val_loss = float(\"inf\")\n",
    "best_params = None\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
    "\n",
    "print(\"\\nBeste Parameterkombination:\")\n",
    "print(study.best_params)\n",
    "print(f\"Bester Validierungsverlust: {study.best_value:.4f}\")\n",
    "\n",
    "for trial in study.trials:\n",
    "    print(trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>With best hyperparameters: Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn import HuberLoss, MSELoss, L1Loss\n",
    "\n",
    "# Daten splitten\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_features, train_targets, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "# TensorBoard-Writer starten\n",
    "writer = SummaryWriter(f\"{log_dir}/final_training\")\n",
    "\n",
    "# Modell initialisieren\n",
    "model = MLP(\n",
    "    input_size=train_x.shape[1],\n",
    "    use_dropout=best_params[\"use_dropout\"],\n",
    "    dropout_rate=best_params[\"dropout_rate\"],\n",
    "    use_batch_norm=best_params[\"use_batch_norm\"]\n",
    ").to(device)\n",
    "\n",
    "# Visualisierung des Modells\n",
    "example_input = torch.randn(best_params[\"batch_size\"], train_x.shape[1]).to(device)\n",
    "writer.add_graph(model, example_input)\n",
    "\n",
    "# Trainings-Konfiguration\n",
    "criterion = HuberLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params[\"lr\"])\n",
    "\n",
    "# Daten-Loader\n",
    "train_loader = DataLoader(list(zip(train_x, train_y)), batch_size=best_params[\"batch_size\"], shuffle=True)\n",
    "test_loader = DataLoader(list(zip(test_x, test_y)), batch_size=best_params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Training\n",
    "num_epochs = best_params[\"number_epochs\"]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    writer.add_scalar(\"Training Loss\", train_loss, epoch)\n",
    "\n",
    "# Testen\n",
    "model.eval()\n",
    "mae_criterion = L1Loss()\n",
    "mse_criterion = MSELoss()\n",
    "huber_criterion = HuberLoss()\n",
    "\n",
    "test_loss_mae, test_loss_mse, test_loss_huber = 0, 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        preds = model(batch_x)\n",
    "        \n",
    "        test_loss_mae += mae_criterion(preds, batch_y).item()\n",
    "        test_loss_mse += mse_criterion(preds, batch_y).item()\n",
    "        test_loss_huber += huber_criterion(preds, batch_y).item()\n",
    "\n",
    "test_loss_mae /= len(test_loader)\n",
    "test_loss_mse /= len(test_loader)\n",
    "test_loss_huber /= len(test_loader)\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "print(f\"Endg√ºltiger MAE: {test_loss_mae:.4f}\")\n",
    "print(f\"Endg√ºltiger MSE: {test_loss_mse:.4f}\")\n",
    "print(f\"Endg√ºltiger Huber Loss: {test_loss_huber:.4f}\")\n",
    "\n",
    "# TensorBoard-Writer schlie√üen\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>With all data: Training for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell initialisieren\n",
    "model = MLP(\n",
    "    input_size=train_features.shape[1],\n",
    "    use_dropout=best_params[\"use_dropout\"],\n",
    "    dropout_rate=best_params[\"dropout_rate\"],\n",
    "    use_batch_norm=best_params[\"use_batch_norm\"]\n",
    ").to(device)\n",
    "\n",
    "# Trainings-Konfiguration\n",
    "criterion = HuberLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params[\"lr\"])\n",
    "\n",
    "# Daten-Loader f√ºr alle Trainingsdaten\n",
    "train_loader = DataLoader(list(zip(train_features, train_targets)), batch_size=best_params[\"batch_size\"], shuffle=True)\n",
    "\n",
    "# Training\n",
    "num_epochs = best_params[\"number_epochs\"]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    writer.add_scalar(\"Final Training Loss\", train_loss, epoch)\n",
    "\n",
    "# Modell speichern\n",
    "torch.save(model.state_dict(), \"trained_model_for_deployment.pth\")\n",
    "print(\"Modell f√ºr Deployment gespeichert!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "old code for hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "import shutil\n",
    "import os\n",
    "import subprocess\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import platform\n",
    "\n",
    "# Hyperparameter-Raum definieren\n",
    "# param_space = {\n",
    "#     \"hidden_size\": [32, 64, 128, 256],\n",
    "#     \"batch_size\": [16, 32, 64],\n",
    "#     \"lr\": [1e-2, 1e-3, 1e-4],\n",
    "#     \"number_epochs\": [20, 50, 100],\n",
    "# }\n",
    "param_space = {\n",
    "    \"hidden_size\": [32],\n",
    "    \"batch_size\": [32],\n",
    "    \"lr\": [1e-3],\n",
    "    \"number_epochs\": [20],\n",
    "    \"n_splits\": [2]\n",
    "}\n",
    "\n",
    "# Funktion zur Auswahl eines zuf√§lligen Parametersets\n",
    "def random_search(param_space, n_trials):\n",
    "    trials = []\n",
    "    for _ in range(n_trials):\n",
    "        trial = {key: random.choice(values) for key, values in param_space.items()}\n",
    "        trials.append(trial)\n",
    "    return trials\n",
    "\n",
    "# Generiere zuf√§llige Parameterkombinationen\n",
    "n_trials = 1\n",
    "params = random_search(param_space, n_trials)[0]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# KFold-Objekt\n",
    "kf = KFold(n_splits=params[\"n_splits\"], shuffle=True, random_state=42)\n",
    "len_train_val_dataset = len(train_val_dataset)\n",
    "\n",
    "# Ergebnis-Tracking\n",
    "best_val_loss = float(\"inf\")\n",
    "best_params = None\n",
    "results = []\n",
    "\n",
    "input_size = train_features.shape[1]\n",
    "\n",
    "# TensorBoard-Prozess beenden\n",
    "try:\n",
    "    if platform.system() == \"Windows\":\n",
    "        subprocess.run([\"taskkill\", \"/IM\", \"tensorboard.exe\", \"/F\"], check=True)\n",
    "    else:\n",
    "        subprocess.run([\"pkill\", \"-f\", \"tensorboard\"], check=True)\n",
    "    print(\"Terminated TensorBoard process\")\n",
    "except subprocess.CalledProcessError:\n",
    "    print(\"No TensorBoard process found or could not be terminated\")\n",
    "\n",
    "log_dir = \"runs\"\n",
    "\n",
    "# L√∂schen, wenn der Ordner existiert\n",
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)\n",
    "\n",
    "# TensorBoard neu starten\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "subprocess.Popen([\"tensorboard\", \"--logdir=runs\", \"--bind_all\"])\n",
    "print(\"TensorBoard started.\")\n",
    "\n",
    "avg_val_loss = 0.0  # average validation loss over all folds\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(range(len_train_val_dataset)), 1):\n",
    "\n",
    "    print(f\"  Fold {fold}/{kf.n_splits}\")\n",
    "    writer = SummaryWriter(f\"{log_dir}/fold_{fold}\")\n",
    "\n",
    "    # model\n",
    "    example_input = torch.randn(params[\"batch_size\"], input_size).to(device)\n",
    "    model = MLP(input_size=input_size).to(device)\n",
    "\n",
    "    # visualisation of model architecture\n",
    "    writer.add_graph(model, example_input)\n",
    "\n",
    "    train_fold_dataset = Subset(train_val_dataset, train_idx)\n",
    "    val_fold_dataset = Subset(train_val_dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_fold_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_fold_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    # loss and optimiser\n",
    "    criterion = nn.HuberLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "\n",
    "    # training\n",
    "    for epoch in range(params[\"number_epochs\"]):\n",
    "        print(f\"    Epoch {epoch+1}/{params['number_epochs']}\")\n",
    "        model.train()\n",
    "        training_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            training_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()  # Gradienten zur√ºcksetzen\n",
    "            loss.backward()        # Gradienten berechnen\n",
    "            optimizer.step()       # Parameter aktualisieren\n",
    "\n",
    "        # protocol training loss\n",
    "        writer.add_scalar(\"Training Loss\", training_loss / len(train_loader), epoch)\n",
    "\n",
    "        print(f\"    Fold Training Loss: {training_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        fold_val_loss = 0.0\n",
    "        MAE_loss = 0.0\n",
    "        MSE_loss = 0.0\n",
    "\n",
    "        # # for later visualisation\n",
    "        # batch_x_all = []\n",
    "        # batch_y_all = []\n",
    "        # val_outputs_all = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "\n",
    "                # batch_x_all.append(batch_x.detach().cpu().numpy())\n",
    "                # batch_y_all.append(batch_y.detach().cpu().numpy())\n",
    "\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                val_outputs = model(batch_x)\n",
    "\n",
    "                # val_outputs_all.append(val_outputs.detach().cpu().numpy())\n",
    "\n",
    "                fold_val_loss += criterion(val_outputs, batch_y).item()\n",
    "        \n",
    "        # batch_x_all = np.concatenate(batch_x_all)\n",
    "        # batch_y_all = np.concatenate(batch_y_all)\n",
    "        # val_outputs_all = np.concatenate(val_outputs_all)\n",
    "\n",
    "        fold_val_loss /= len(val_loader)\n",
    "        MAE_loss /= len(val_loader)\n",
    "        MSE_loss /= len(val_loader)\n",
    "        writer.add_scalar(\"Validation Loss\", fold_val_loss, epoch)\n",
    "        print(f\"    Fold Validation Loss: {fold_val_loss:.4f}\")\n",
    "    \n",
    "    avg_val_loss += fold_val_loss\n",
    "\n",
    "    # TensorBoard schlie√üen\n",
    "    writer.close()\n",
    "\n",
    "avg_val_loss /= kf.n_splits\n",
    "print(f\"  Trial Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# Ergebnisse speichern\n",
    "results.append({\"params\": params, \"avg_val_loss\": avg_val_loss})\n",
    "\n",
    "# Bestes Ergebnis aktualisieren\n",
    "if avg_val_loss < best_val_loss:\n",
    "    best_val_loss = avg_val_loss\n",
    "    best_params = params\n",
    "\n",
    "print(f\"\\nBest Parameters: {best_params}\")\n",
    "print(f\"Best Validation Loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Visualisation of validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modell auf Evaluation setzen\n",
    "model.eval()\n",
    "\n",
    "# Vorhersagen f√ºr alle Daten generieren\n",
    "predictions, true_values = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in DataLoader(list(zip(train_features, train_targets)), batch_size=best_params[\"batch_size\"], shuffle=False):\n",
    "        batch_x = batch_x.to(device)\n",
    "        preds = model(batch_x).cpu().numpy()\n",
    "        predictions.extend(preds)\n",
    "        true_values.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(true_values, label=\"Tats√§chliche Werte\", marker='o')\n",
    "plt.plot(predictions, label=\"Vorhergesagte Werte\", marker='x')\n",
    "plt.title(\"Tats√§chliche vs. Vorhergesagte Werte\")\n",
    "plt.xlabel(\"Datenpunkt-Index\")\n",
    "plt.ylabel(\"Wert\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reconstruct original wind speeds\n",
    "wind_speeds_validation = wind_scaler.inverse_transform(batch_x_all[:, -1].reshape(-1, 1)).flatten()\n",
    "\n",
    "# reconstruct original turbine types\n",
    "turbine_types_validation = np.argmax(batch_x_all[:, 0:-4], axis=1)\n",
    "turbine_types_validation = encoder.categories_[0][turbine_types_validation]\n",
    "\n",
    "# Erstellen eines Farbschemas f√ºr verschiedene Turbinentypen\n",
    "unique_turbine_types = np.unique(turbine_types_validation)\n",
    "colors = plt.cm.get_cmap('hsv', 2*len(unique_turbine_types))\n",
    "\n",
    "# Erstellen des Scatterplots mit verschiedenen Farben f√ºr Turbinentypen\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# F√ºr jeden Turbinentyp einen Scatter-Plot mit einer anderen Farbe\n",
    "for i, turbine_type in enumerate(unique_turbine_types):\n",
    "    mask = turbine_types_validation == turbine_type  # Maske f√ºr den jeweiligen Turbinentyp\n",
    "    plt.scatter(wind_speeds_validation[mask], batch_y_all[mask], s=0.5, color=colors(i), label=f\"Turbine Type {turbine_type} - Actual\")\n",
    "    plt.scatter(wind_speeds_validation[mask], val_outputs_all[mask], s=0.5, color=colors(len(unique_turbine_types)+i), marker='x', label=f\"Turbine Type {turbine_type} - Predicted\")\n",
    "\n",
    "plt.xlabel('Wind speed (m/s)')\n",
    "plt.ylabel('Power (MW)')\n",
    "plt.title('Actual and predicted power over wind speed by turbine type')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reconstruct original wind speeds\n",
    "wind_speeds_validation = wind_scaler.inverse_transform(batch_x_all[:, -1].reshape(-1, 1)).flatten()\n",
    "\n",
    "# reconstruct original turbine types\n",
    "turbine_types_validation = np.argmax(batch_x_all[:, 0:-4], axis=1)\n",
    "turbine_types_validation = encoder.categories_[0][turbine_types_validation]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(wind_speeds_validation, batch_y_all, s=0.5, color='red', label=\"Actual values\")\n",
    "plt.scatter(wind_speeds_validation, val_outputs_all, s=0.5, color='blue', label=\"Predicted values\")\n",
    "plt.xlabel('Wind speed (m/s)')\n",
    "plt.ylabel('Power (MW)')\n",
    "plt.title('Actual and predicted power over wind speed')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(wind_speeds_validation, (val_outputs_all-batch_y_all) / batch_y_all, color='blue', s=0.5)\n",
    "plt.ylim(-10, 50)\n",
    "plt.xlabel('Wind speed (m/s)')\n",
    "plt.ylabel('Relative error')\n",
    "plt.title('Relative error over wind speed')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(wind_speeds_validation, val_outputs_all-batch_y_all, color='blue', s=0.5)\n",
    "plt.xlabel('Wind speed (m/s)')\n",
    "plt.ylabel('Power (MW)')\n",
    "plt.title('Predicted - actual power over wind speed')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Tabelle erstellen und anzeigen\n",
    "results_df = pd.DataFrame({\n",
    "    \"wind speed\": wind_speeds_validation,\n",
    "    \"actual values\": batch_y_all.flatten(),\n",
    "    \"predicted values\": val_outputs_all.flatten()\n",
    "})\n",
    "print(\"last ten validation values:\\n\", results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms showing error distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fehlerberechnung\n",
    "absolute_errors = val_outputs_all - batch_y_all\n",
    "relative_errors = (val_outputs_all - batch_y_all) / (batch_y_all + 1e-8)\n",
    "\n",
    "# Berechnung von Mittelwert und Standardabweichung\n",
    "mean_abs_error = np.mean(absolute_errors)\n",
    "std_abs_error = np.std(absolute_errors)\n",
    "\n",
    "mean_rel_error = np.mean(relative_errors)\n",
    "std_rel_error = np.std(relative_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Absoluter Fehler: Histogramm um 0 zentrieren\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.histplot(absolute_errors, bins=200, kde=True, color='skyblue')\n",
    "plt.xlim(mean_abs_error - 3*std_abs_error, mean_abs_error + 3*std_abs_error)\n",
    "plt.axvline(0, color='red', linestyle='--', label=\"Zero Error\")\n",
    "plt.title(\"Zentriertes Histogramm der absoluten Fehler\")\n",
    "plt.xlabel(\"Absoluter Fehler (MW)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Relativer Fehler: Histogramm um 0 zentrieren\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.histplot(relative_errors, bins=2000, kde=True, color='orange')\n",
    "plt.xlim(mean_rel_error - std_rel_error, mean_rel_error + std_rel_error)\n",
    "plt.axvline(0, color='red', linestyle='--', label=\"Zero Error\")\n",
    "plt.title(\"Zentriertes Histogramm der relativen Fehler\")\n",
    "plt.xlabel(\"Relativer Fehler (Bruchteil)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Evaluate Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Feature-Namen definieren\n",
    "feature_names = np.array((\n",
    "    list(encoder.get_feature_names_out([\"Turbine\"])) + \n",
    "    [\"Hub_height\", \"Capacity\", \"Age\", \"Wind_speed\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. shapley-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import shap\n",
    "\n",
    "# Maske mit zuf√§lligen Indizes erstellen\n",
    "random_indices = np.random.choice(len(combined_features), size=1000, replace=False)\n",
    "\n",
    "# Werte aus combined_features basierend auf der Maske ausw√§hlen\n",
    "masked_features = combined_features[random_indices]\n",
    "\n",
    "# Eingabe f√ºr den SHAP-Explainer vorbereiten\n",
    "input_tensor = torch.tensor(masked_features, dtype=torch.float32).to(device)\n",
    "\n",
    "# SHAP-Explainer erstellen\n",
    "explainer = shap.GradientExplainer(model, input_tensor)\n",
    "\n",
    "# SHAP-Werte berechnen\n",
    "shap_values = explainer.shap_values(input_tensor)\n",
    "\n",
    "# SHAP-Werte umformen\n",
    "shap_values_reshaped = shap_values.reshape(shap_values.shape[0], shap_values.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_reshaped, masked_features, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "\n",
    "# Modellvorhersagen auf dem Eingabedatensatz berechnen\n",
    "base_value = model(torch.tensor(masked_features, dtype=torch.float32).to(device)).mean().item()\n",
    "\n",
    "shap.plots.force(base_value, shap_values_reshaped, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Base Value in ein Array umwandeln\n",
    "base_values_array = np.full((shap_values_reshaped.shape[0],), base_value)\n",
    "\n",
    "# SHAP Explanation-Objekt erstellen\n",
    "shap_values_explanation = shap.Explanation(\n",
    "    values=shap_values_reshaped,\n",
    "    base_values=base_values_array,  # Array statt float\n",
    "    data=masked_features,  # Eingabedaten\n",
    "    feature_names=feature_names  # Feature-Namen\n",
    ")\n",
    "\n",
    "# Bar-Plot erstellen\n",
    "shap.plots.bar(shap_values_explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. gradient method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell in Evaluierungsmodus\n",
    "model.eval()\n",
    "\n",
    "# Eingabegr√∂√üe reduzieren\n",
    "inputs = torch.tensor(combined_features[:100], dtype=torch.float32, requires_grad=True).to(device)\n",
    "\n",
    "# Vorhersage berechnen\n",
    "outputs = model(inputs)\n",
    "\n",
    "# Mittleren Output f√ºr den Gradienten berechnen\n",
    "outputs.mean().backward()\n",
    "\n",
    "# Gradienten extrahieren und sortieren\n",
    "feature_importance = inputs.grad.abs().mean(dim=0).cpu().detach().numpy()\n",
    "\n",
    "# Nach Wichtigkeit sortieren\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(np.array(feature_names)[sorted_idx], feature_importance[sorted_idx])\n",
    "plt.xlabel(\"Average Gradient Magnitude\")\n",
    "plt.title(\"Feature Importance using Gradients\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Spearman Correlation Coefficient Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "num_turbine_type_columns = turbine_types_onehot.shape[1]\n",
    "\n",
    "# DataFrame ohne Turbinentypen erstellen\n",
    "df = pd.DataFrame(\n",
    "    combined_features[:, num_turbine_type_columns:], \n",
    "    columns=[\"Hub Height\", \"Capacity\", \"Commissioning Date\", \"Wind Speed\"]\n",
    ")\n",
    "\n",
    "# Ausgabe hinzuf√ºgen\n",
    "df['Power'] = output.flatten()\n",
    "\n",
    "# Spearman-Korrelation berechnen\n",
    "spearman_corr_matrix = df.corr(method='spearman')\n",
    "\n",
    "# Farbcodierte Tabelle anzeigen\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(spearman_corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title(\"Spearman Correlation Heatmap\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webapp_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
