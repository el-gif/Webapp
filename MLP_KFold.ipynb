{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Build feature and output arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# lists for all data\n",
    "all_turbine_types = []\n",
    "all_hub_heights = []\n",
    "all_capacities = []\n",
    "all_commissioning_dates = []\n",
    "all_production_data = []\n",
    "\n",
    "# load JSON-file\n",
    "with open(f\"data/WPPs+production+wind.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    WPP_production_wind = json.load(file)[:10] # only use the first 10 wpps for now\n",
    "\n",
    "# collect data\n",
    "for wpp in WPP_production_wind:\n",
    "    all_turbine_types.append(wpp[\"Turbine\"])\n",
    "    all_hub_heights.append(wpp[\"Hub_height\"] if not pd.isna(wpp[\"Hub_height\"]) else 100)\n",
    "    all_capacities.append(wpp[\"Capacity\"])\n",
    "    all_commissioning_dates.append(wpp[\"Commission_date\"] if wpp[\"Commission_date\"] != \"nan\" else \"2015/06\")\n",
    "    all_production_data.append(wpp[\"Production\"])\n",
    "\n",
    "# replace NaN-values in turbine types with unambiguous names\n",
    "nan_counter = 1\n",
    "for idx, turbine in enumerate(all_turbine_types):\n",
    "    if pd.isna(turbine):\n",
    "        all_turbine_types[idx] = f\"nan{nan_counter}\"\n",
    "        nan_counter += 1\n",
    "\n",
    "# One-Hot-Encoding for turbine types\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "turbine_types_onehot = encoder.fit_transform(np.array(all_turbine_types).reshape(-1, 1))\n",
    "\n",
    "# correct data format for commissioning dates\n",
    "all_commissioning_dates = [\n",
    "    \"2015/06\" if date == \"nan\" else f\"{date}/06\" if isinstance(date, str) and \"/\" not in date else date\n",
    "    for date in all_commissioning_dates\n",
    "]\n",
    "\n",
    "# convert to datetime\n",
    "standardised_dates = pd.to_datetime(all_commissioning_dates, format='%Y/%m')\n",
    "\n",
    "# calculate age\n",
    "current_date = pd.Timestamp(\"2024-12-01\")\n",
    "ages = current_date.year * 12 + current_date.month - (standardised_dates.year * 12 + standardised_dates.month)\n",
    "\n",
    "# create combined features and output lists\n",
    "combined_features_raw = []\n",
    "output_raw = []\n",
    "\n",
    "# convert data in feature arrays\n",
    "for idx, production_data in enumerate(all_production_data):\n",
    "    num_rows = len(production_data)\n",
    "\n",
    "    # repetitions for common features\n",
    "    turbine_type_repeated = np.tile(turbine_types_onehot[idx], (num_rows, 1))\n",
    "    hub_height_repeated = np.full((num_rows, 1), all_hub_heights[idx])\n",
    "    capacity_repeated = np.full((num_rows, 1), all_capacities[idx])\n",
    "    age_repeated = np.full((num_rows, 1), ages[idx])\n",
    "\n",
    "    # extract production values and wind speeds\n",
    "    production_values = np.array([entry[1] for entry in production_data]).reshape(-1, 1)\n",
    "    wind_speeds = np.array([entry[2] for entry in production_data]).reshape(-1, 1)\n",
    "\n",
    "    # combine all features\n",
    "    combined_chunk = np.hstack((\n",
    "        turbine_type_repeated,\n",
    "        hub_height_repeated,\n",
    "        capacity_repeated,\n",
    "        age_repeated,\n",
    "        wind_speeds\n",
    "    ))\n",
    "\n",
    "    # add the data\n",
    "    combined_features_raw.append(combined_chunk)\n",
    "    output_raw.append(production_values)\n",
    "\n",
    "# combine all data chunks to one array\n",
    "combined_features_raw = np.vstack(combined_features_raw)\n",
    "output_raw = np.vstack(output_raw)\n",
    "\n",
    "# tound all values to two decimal places\n",
    "combined_features_raw = np.round(combined_features_raw, decimals=2)\n",
    "output_raw = np.round(output_raw, decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Scale feature vector and define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "combined_features = combined_features_raw.copy()\n",
    "output = output_raw.copy()\n",
    "\n",
    "# Standardisierung der numerischen Features (individuell!)\n",
    "wind_scaler = StandardScaler()\n",
    "combined_features[:, -1] = wind_scaler.fit_transform(combined_features[:, -1].reshape(-1, 1)).flatten() # scale wind speeds\n",
    "scaler = StandardScaler()\n",
    "combined_features[:, -2] = scaler.fit_transform(combined_features[:, -2].reshape(-1, 1)).flatten() # scale ages\n",
    "combined_features[:, -3] = scaler.fit_transform(combined_features[:, -3].reshape(-1, 1)).flatten() # scale capacities\n",
    "combined_features[:, -4] = scaler.fit_transform(combined_features[:, -4].reshape(-1, 1)).flatten() # scale hub heights\n",
    "\n",
    "# Trainings- und Testaufteilung\n",
    "train_features, test_features, train_targets, test_targets = train_test_split(combined_features, output, test_size=0.2, shuffle=True)\n",
    "\n",
    "# Dataset-Klasse für PyTorch\n",
    "class WindPowerDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.targets[index]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Erstellung der PyTorch-Datasets\n",
    "train_val_dataset = WindPowerDataset(train_features, train_targets)\n",
    "test_dataset = WindPowerDataset(test_features, test_targets)\n",
    "\n",
    "# Ausgabe der Formen\n",
    "print(\"Train and Validation Features Shape:\", train_features.shape)\n",
    "print(\"Train and Validation Targets Shape:\", train_targets.shape)\n",
    "print(\"Test Features Shape:\", test_features.shape)\n",
    "print(\"Test Targets Shape:\", test_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLP, self).__init__()\n",
    "        # define layers as nn.Sequential, for better visualisability in TensorBoard and compatibility with DeepExplainer from SHAP\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, input_size):\n",
    "#         super(MLP, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, 256)\n",
    "#         self.fc2 = nn.Linear(256, 128)\n",
    "#         self.fc3 = nn.Linear(128, 64)\n",
    "#         self.fc4 = nn.Linear(64, 32)\n",
    "#         self.fc5 = nn.Linear(32, 1)\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         x = self.relu(self.fc2(x))\n",
    "#         x = self.relu(self.fc3(x))\n",
    "#         x = self.relu(self.fc4(x))\n",
    "#         x = self.fc5(x)  # No activation in the output layer for regression\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, use_dropout=False, dropout_rate=0.3, \n",
    "                 use_batch_norm=False, activation_fn=nn.ReLU):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # Erste Schicht\n",
    "        layers.append(nn.Linear(input_size, 256))\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(256))\n",
    "        layers.append(activation_fn())\n",
    "\n",
    "        # Zweite Schicht\n",
    "        layers.append(nn.Linear(256, 128))\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(128))\n",
    "        layers.append(activation_fn())\n",
    "\n",
    "        # Dritte Schicht\n",
    "        layers.append(nn.Linear(128, 64))\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(64))\n",
    "        layers.append(activation_fn())\n",
    "\n",
    "        # Vierte Schicht\n",
    "        layers.append(nn.Linear(64, 32))\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(32))\n",
    "        layers.append(activation_fn())\n",
    "\n",
    "        # Dropout nach der letzten versteckten Schicht (optional)\n",
    "        if use_dropout:\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        # Ausgabeschicht\n",
    "        layers.append(nn.Linear(32, 1))\n",
    "\n",
    "        # Modell zusammenstellen\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1. Hyperparameter search: Training, Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "import shutil\n",
    "import os\n",
    "import subprocess\n",
    "import platform\n",
    "import numpy as np\n",
    "import optuna\n",
    "import time\n",
    "import pynvml\n",
    "import psutil\n",
    "\n",
    "# Ressourcenüberwachung initialisieren\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == torch.device(\"cuda\"):\n",
    "    pynvml.nvmlInit()\n",
    "    gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # GPU 0 verwenden\n",
    "\n",
    "n_splits = 4  # Anzahl der Folds für Kreuzvalidierung\n",
    "\n",
    "# Bewertungsfunktion\n",
    "def objective(trial):\n",
    "    batch_size = int(2 ** round(np.log2(trial.suggest_int(\"batch_size\", 16, 128))))\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    number_epochs = trial.suggest_int(\"number_epochs\", 10, 100)\n",
    "    use_dropout = trial.suggest_categorical(\"use_dropout\", [True, False])\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    use_batch_norm = trial.suggest_categorical(\"use_batch_norm\", [True, False])\n",
    "\n",
    "    print(f\"Evaluating: batch_size={batch_size}, lr={lr:.5f}, number_epochs={number_epochs}, \"\n",
    "          f\"use_dropout={use_dropout}, dropout_rate={dropout_rate}, use_batch_norm={use_batch_norm}\")\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    len_train_val_dataset = len(train_val_dataset)\n",
    "\n",
    "    input_size = train_features.shape[1]\n",
    "    avg_val_loss = 0.0  # Durchschnittlicher Validierungsverlust\n",
    "    start_time = time.time()  # Zeitmessung starten\n",
    "    max_memory_usage = 0  # Maximale Speicher-Auslastung\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(range(len_train_val_dataset)), 1):\n",
    "        print(f\"  Fold {fold}/{kf.n_splits}\")\n",
    "\n",
    "        model = torch.jit.script(MLP(input_size=input_size, use_dropout=use_dropout, dropout_rate=dropout_rate, use_batch_norm=use_batch_norm)).to(device)\n",
    "\n",
    "        train_fold_dataset = Subset(train_val_dataset, train_idx)\n",
    "        val_fold_dataset = Subset(train_val_dataset, val_idx)\n",
    "\n",
    "        train_loader = DataLoader(train_fold_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_fold_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        criterion = nn.HuberLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        for epoch in range(number_epochs):\n",
    "            model.train()\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if device == torch.device(\"cuda\"):\n",
    "                    memory_info = pynvml.nvmlDeviceGetMemoryInfo(gpu_handle)\n",
    "                    max_memory_usage = max(max_memory_usage, memory_info.used / 1024 ** 2)  # MB\n",
    "                else:\n",
    "                    max_memory_usage = max(max_memory_usage, psutil.virtual_memory().used / 1024 ** 2)  # MB\n",
    "\n",
    "            model.eval()\n",
    "            fold_val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in val_loader:\n",
    "                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                    val_outputs = model(batch_x)\n",
    "                    fold_val_loss += criterion(val_outputs, batch_y).item()\n",
    "            avg_val_loss += fold_val_loss / len(val_loader)\n",
    "\n",
    "    avg_val_loss /= kf.n_splits\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    weighted_score = 0.7 * avg_val_loss + 0.15 * elapsed_time + 0.15 * max_memory_usage\n",
    "\n",
    "    trial.set_user_attr(\"resource_usage\", max_memory_usage)\n",
    "    trial.set_user_attr(\"elapsed_time\", elapsed_time)\n",
    "    trial.set_user_attr(\"avg_val_loss\", avg_val_loss)\n",
    "    trial.set_user_attr(\"weighted_score\", weighted_score)\n",
    "\n",
    "    return weighted_score\n",
    "\n",
    "# Optuna-Optimierung starten\n",
    "best_val_loss = float(\"inf\")\n",
    "best_params = None\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
    "\n",
    "print(\"\\nBeste Parameterkombination:\")\n",
    "print(study.best_params)\n",
    "print(f\"Bester Validierungsverlust: {study.best_value:.4f}\")\n",
    "\n",
    "for trial in study.trials:\n",
    "    print(trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. With best hyperparameters: Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn import HuberLoss, MSELoss, L1Loss\n",
    "\n",
    "log_dir = \"runs\"\n",
    "\n",
    "# TensorBoard-Prozess beenden\n",
    "try:\n",
    "    if platform.system() == \"Windows\":\n",
    "        subprocess.run([\"taskkill\", \"/IM\", \"tensorboard.exe\", \"/F\"], check=True)\n",
    "    else:\n",
    "        subprocess.run([\"pkill\", \"-f\", \"tensorboard\"], check=True)\n",
    "    print(\"Terminated TensorBoard process\")\n",
    "except subprocess.CalledProcessError:\n",
    "    print(\"No TensorBoard process found or could not be terminated\")\n",
    "\n",
    "# Log-Verzeichnis löschen\n",
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "subprocess.Popen([\"tensorboard\", \"--logdir=runs\", \"--bind_all\"])\n",
    "print(\"TensorBoard started.\")\n",
    "\n",
    "# TensorBoard-Writer starten\n",
    "writer = SummaryWriter(f\"{log_dir}/final_training\")\n",
    "\n",
    "# Daten splitten\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_features, train_targets, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "# Modell initialisieren\n",
    "model = MLP(\n",
    "    input_size=train_x.shape[1],\n",
    "    use_dropout=best_params[\"use_dropout\"],\n",
    "    dropout_rate=best_params[\"dropout_rate\"],\n",
    "    use_batch_norm=best_params[\"use_batch_norm\"]\n",
    ").to(device)\n",
    "\n",
    "# Visualisierung des Modells\n",
    "example_input = torch.randn(best_params[\"batch_size\"], train_x.shape[1]).to(device)\n",
    "writer.add_graph(model, example_input)\n",
    "\n",
    "# Trainings-Konfiguration\n",
    "criterion = HuberLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params[\"lr\"])\n",
    "\n",
    "# Daten-Loader\n",
    "train_loader = DataLoader(list(zip(train_x, train_y)), batch_size=best_params[\"batch_size\"], shuffle=True)\n",
    "test_loader = DataLoader(list(zip(test_x, test_y)), batch_size=best_params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Training\n",
    "num_epochs = best_params[\"number_epochs\"]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    writer.add_scalar(\"Training Loss\", train_loss, epoch)\n",
    "\n",
    "# Testen\n",
    "model.eval()\n",
    "mae_criterion = L1Loss()\n",
    "mse_criterion = MSELoss()\n",
    "huber_criterion = HuberLoss()\n",
    "\n",
    "test_loss_mae, test_loss_mse, test_loss_huber = 0, 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        preds = model(batch_x)\n",
    "        \n",
    "        test_loss_mae += mae_criterion(preds, batch_y).item()\n",
    "        test_loss_mse += mse_criterion(preds, batch_y).item()\n",
    "        test_loss_huber += huber_criterion(preds, batch_y).item()\n",
    "\n",
    "test_loss_mae /= len(test_loader)\n",
    "test_loss_mse /= len(test_loader)\n",
    "test_loss_huber /= len(test_loader)\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "print(f\"Endgültiger MAE: {test_loss_mae:.4f}\")\n",
    "print(f\"Endgültiger MSE: {test_loss_mse:.4f}\")\n",
    "print(f\"Endgültiger Huber Loss: {test_loss_huber:.4f}\")\n",
    "\n",
    "# TensorBoard-Writer schließen\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. With all data: Training for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell initialisieren\n",
    "model = MLP(\n",
    "    input_size=train_features.shape[1],\n",
    "    use_dropout=best_params[\"use_dropout\"],\n",
    "    dropout_rate=best_params[\"dropout_rate\"],\n",
    "    use_batch_norm=best_params[\"use_batch_norm\"]\n",
    ").to(device)\n",
    "\n",
    "# Trainings-Konfiguration\n",
    "criterion = HuberLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params[\"lr\"])\n",
    "\n",
    "# Daten-Loader für alle Trainingsdaten\n",
    "train_loader = DataLoader(list(zip(train_features, train_targets)), batch_size=best_params[\"batch_size\"], shuffle=True)\n",
    "\n",
    "# Training\n",
    "num_epochs = best_params[\"number_epochs\"]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    writer.add_scalar(\"Final Training Loss\", train_loss, epoch)\n",
    "\n",
    "# Modell speichern\n",
    "torch.save(model.state_dict(), \"trained_model_for_deployment.pth\")\n",
    "print(\"Modell für Deployment gespeichert!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "old code for hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "import shutil\n",
    "import os\n",
    "import subprocess\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import platform\n",
    "\n",
    "# Hyperparameter-Raum definieren\n",
    "# param_space = {\n",
    "#     \"hidden_size\": [32, 64, 128, 256],\n",
    "#     \"batch_size\": [16, 32, 64],\n",
    "#     \"lr\": [1e-2, 1e-3, 1e-4],\n",
    "#     \"number_epochs\": [20, 50, 100],\n",
    "# }\n",
    "param_space = {\n",
    "    \"hidden_size\": [32],\n",
    "    \"batch_size\": [32],\n",
    "    \"lr\": [1e-3],\n",
    "    \"number_epochs\": [20],\n",
    "    \"n_splits\": [2]\n",
    "}\n",
    "\n",
    "# Funktion zur Auswahl eines zufälligen Parametersets\n",
    "def random_search(param_space, n_trials):\n",
    "    trials = []\n",
    "    for _ in range(n_trials):\n",
    "        trial = {key: random.choice(values) for key, values in param_space.items()}\n",
    "        trials.append(trial)\n",
    "    return trials\n",
    "\n",
    "# Generiere zufällige Parameterkombinationen\n",
    "n_trials = 1\n",
    "params = random_search(param_space, n_trials)[0]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# KFold-Objekt\n",
    "kf = KFold(n_splits=params[\"n_splits\"], shuffle=True, random_state=42)\n",
    "len_train_val_dataset = len(train_val_dataset)\n",
    "\n",
    "# Ergebnis-Tracking\n",
    "best_val_loss = float(\"inf\")\n",
    "best_params = None\n",
    "results = []\n",
    "\n",
    "input_size = train_features.shape[1]\n",
    "\n",
    "# TensorBoard-Prozess beenden\n",
    "try:\n",
    "    if platform.system() == \"Windows\":\n",
    "        subprocess.run([\"taskkill\", \"/IM\", \"tensorboard.exe\", \"/F\"], check=True)\n",
    "    else:\n",
    "        subprocess.run([\"pkill\", \"-f\", \"tensorboard\"], check=True)\n",
    "    print(\"Terminated TensorBoard process\")\n",
    "except subprocess.CalledProcessError:\n",
    "    print(\"No TensorBoard process found or could not be terminated\")\n",
    "\n",
    "log_dir = \"runs\"\n",
    "\n",
    "# Löschen, wenn der Ordner existiert\n",
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)\n",
    "\n",
    "# TensorBoard neu starten\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "subprocess.Popen([\"tensorboard\", \"--logdir=runs\", \"--bind_all\"])\n",
    "print(\"TensorBoard started.\")\n",
    "\n",
    "avg_val_loss = 0.0  # average validation loss over all folds\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(range(len_train_val_dataset)), 1):\n",
    "\n",
    "    print(f\"  Fold {fold}/{kf.n_splits}\")\n",
    "    writer = SummaryWriter(f\"{log_dir}/fold_{fold}\")\n",
    "\n",
    "    # model\n",
    "    example_input = torch.randn(params[\"batch_size\"], input_size).to(device)\n",
    "    model = MLP(input_size=input_size).to(device)\n",
    "\n",
    "    # visualisation of model architecture\n",
    "    writer.add_graph(model, example_input)\n",
    "\n",
    "    train_fold_dataset = Subset(train_val_dataset, train_idx)\n",
    "    val_fold_dataset = Subset(train_val_dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_fold_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_fold_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    # loss and optimiser\n",
    "    criterion = nn.HuberLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "\n",
    "    # training\n",
    "    for epoch in range(params[\"number_epochs\"]):\n",
    "        print(f\"    Epoch {epoch+1}/{params['number_epochs']}\")\n",
    "        model.train()\n",
    "        training_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            training_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()  # Gradienten zurücksetzen\n",
    "            loss.backward()        # Gradienten berechnen\n",
    "            optimizer.step()       # Parameter aktualisieren\n",
    "\n",
    "        # protocol training loss\n",
    "        writer.add_scalar(\"Training Loss\", training_loss / len(train_loader), epoch)\n",
    "\n",
    "        print(f\"    Fold Training Loss: {training_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        fold_val_loss = 0.0\n",
    "        MAE_loss = 0.0\n",
    "        MSE_loss = 0.0\n",
    "\n",
    "        # # for later visualisation\n",
    "        # batch_x_all = []\n",
    "        # batch_y_all = []\n",
    "        # val_outputs_all = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "\n",
    "                # batch_x_all.append(batch_x.detach().cpu().numpy())\n",
    "                # batch_y_all.append(batch_y.detach().cpu().numpy())\n",
    "\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                val_outputs = model(batch_x)\n",
    "\n",
    "                # val_outputs_all.append(val_outputs.detach().cpu().numpy())\n",
    "\n",
    "                fold_val_loss += criterion(val_outputs, batch_y).item()\n",
    "        \n",
    "        # batch_x_all = np.concatenate(batch_x_all)\n",
    "        # batch_y_all = np.concatenate(batch_y_all)\n",
    "        # val_outputs_all = np.concatenate(val_outputs_all)\n",
    "\n",
    "        fold_val_loss /= len(val_loader)\n",
    "        MAE_loss /= len(val_loader)\n",
    "        MSE_loss /= len(val_loader)\n",
    "        writer.add_scalar(\"Validation Loss\", fold_val_loss, epoch)\n",
    "        print(f\"    Fold Validation Loss: {fold_val_loss:.4f}\")\n",
    "    \n",
    "    avg_val_loss += fold_val_loss\n",
    "\n",
    "    # TensorBoard schließen\n",
    "    writer.close()\n",
    "\n",
    "avg_val_loss /= kf.n_splits\n",
    "print(f\"  Trial Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# Ergebnisse speichern\n",
    "results.append({\"params\": params, \"avg_val_loss\": avg_val_loss})\n",
    "\n",
    "# Bestes Ergebnis aktualisieren\n",
    "if avg_val_loss < best_val_loss:\n",
    "    best_val_loss = avg_val_loss\n",
    "    best_params = params\n",
    "\n",
    "print(f\"\\nBest Parameters: {best_params}\")\n",
    "print(f\"Best Validation Loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Visualisation of validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modell auf Evaluation setzen\n",
    "model.eval()\n",
    "\n",
    "# Vorhersagen für alle Daten generieren\n",
    "predictions, true_values = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in DataLoader(list(zip(train_features, train_targets)), batch_size=best_params[\"batch_size\"], shuffle=False):\n",
    "        batch_x = batch_x.to(device)\n",
    "        preds = model(batch_x).cpu().numpy()\n",
    "        predictions.extend(preds)\n",
    "        true_values.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(true_values, label=\"Tatsächliche Werte\", marker='o')\n",
    "plt.plot(predictions, label=\"Vorhergesagte Werte\", marker='x')\n",
    "plt.title(\"Tatsächliche vs. Vorhergesagte Werte\")\n",
    "plt.xlabel(\"Datenpunkt-Index\")\n",
    "plt.ylabel(\"Wert\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reconstruct original wind speeds\n",
    "wind_speeds_validation = wind_scaler.inverse_transform(batch_x_all[:, -1].reshape(-1, 1)).flatten()\n",
    "\n",
    "# reconstruct original turbine types\n",
    "turbine_types_validation = np.argmax(batch_x_all[:, 0:-4], axis=1)\n",
    "turbine_types_validation = encoder.categories_[0][turbine_types_validation]\n",
    "\n",
    "# Erstellen eines Farbschemas für verschiedene Turbinentypen\n",
    "unique_turbine_types = np.unique(turbine_types_validation)\n",
    "colors = plt.cm.get_cmap('hsv', 2*len(unique_turbine_types))\n",
    "\n",
    "# Erstellen des Scatterplots mit verschiedenen Farben für Turbinentypen\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Für jeden Turbinentyp einen Scatter-Plot mit einer anderen Farbe\n",
    "for i, turbine_type in enumerate(unique_turbine_types):\n",
    "    mask = turbine_types_validation == turbine_type  # Maske für den jeweiligen Turbinentyp\n",
    "    plt.scatter(wind_speeds_validation[mask], batch_y_all[mask], s=0.5, color=colors(i), label=f\"Turbine Type {turbine_type} - Actual\")\n",
    "    plt.scatter(wind_speeds_validation[mask], val_outputs_all[mask], s=0.5, color=colors(len(unique_turbine_types)+i), marker='x', label=f\"Turbine Type {turbine_type} - Predicted\")\n",
    "\n",
    "plt.xlabel('Wind speed (m/s)')\n",
    "plt.ylabel('Power (MW)')\n",
    "plt.title('Actual and predicted power over wind speed by turbine type')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reconstruct original wind speeds\n",
    "wind_speeds_validation = wind_scaler.inverse_transform(batch_x_all[:, -1].reshape(-1, 1)).flatten()\n",
    "\n",
    "# reconstruct original turbine types\n",
    "turbine_types_validation = np.argmax(batch_x_all[:, 0:-4], axis=1)\n",
    "turbine_types_validation = encoder.categories_[0][turbine_types_validation]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(wind_speeds_validation, batch_y_all, s=0.5, color='red', label=\"Actual values\")\n",
    "plt.scatter(wind_speeds_validation, val_outputs_all, s=0.5, color='blue', label=\"Predicted values\")\n",
    "plt.xlabel('Wind speed (m/s)')\n",
    "plt.ylabel('Power (MW)')\n",
    "plt.title('Actual and predicted power over wind speed')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(wind_speeds_validation, (val_outputs_all-batch_y_all) / batch_y_all, color='blue', s=0.5)\n",
    "plt.ylim(-10, 50)\n",
    "plt.xlabel('Wind speed (m/s)')\n",
    "plt.ylabel('Relative error')\n",
    "plt.title('Relative error over wind speed')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(wind_speeds_validation, val_outputs_all-batch_y_all, color='blue', s=0.5)\n",
    "plt.xlabel('Wind speed (m/s)')\n",
    "plt.ylabel('Power (MW)')\n",
    "plt.title('Predicted - actual power over wind speed')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Tabelle erstellen und anzeigen\n",
    "results_df = pd.DataFrame({\n",
    "    \"wind speed\": wind_speeds_validation,\n",
    "    \"actual values\": batch_y_all.flatten(),\n",
    "    \"predicted values\": val_outputs_all.flatten()\n",
    "})\n",
    "print(\"last ten validation values:\\n\", results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms showing error distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fehlerberechnung\n",
    "absolute_errors = val_outputs_all - batch_y_all\n",
    "relative_errors = (val_outputs_all - batch_y_all) / (batch_y_all + 1e-8)\n",
    "\n",
    "# Berechnung von Mittelwert und Standardabweichung\n",
    "mean_abs_error = np.mean(absolute_errors)\n",
    "std_abs_error = np.std(absolute_errors)\n",
    "\n",
    "mean_rel_error = np.mean(relative_errors)\n",
    "std_rel_error = np.std(relative_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Absoluter Fehler: Histogramm um 0 zentrieren\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.histplot(absolute_errors, bins=200, kde=True, color='skyblue')\n",
    "plt.xlim(mean_abs_error - 3*std_abs_error, mean_abs_error + 3*std_abs_error)\n",
    "plt.axvline(0, color='red', linestyle='--', label=\"Zero Error\")\n",
    "plt.title(\"Zentriertes Histogramm der absoluten Fehler\")\n",
    "plt.xlabel(\"Absoluter Fehler (MW)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Relativer Fehler: Histogramm um 0 zentrieren\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.histplot(relative_errors, bins=2000, kde=True, color='orange')\n",
    "plt.xlim(mean_rel_error - std_rel_error, mean_rel_error + std_rel_error)\n",
    "plt.axvline(0, color='red', linestyle='--', label=\"Zero Error\")\n",
    "plt.title(\"Zentriertes Histogramm der relativen Fehler\")\n",
    "plt.xlabel(\"Relativer Fehler (Bruchteil)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Evaluate Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Feature-Namen definieren\n",
    "feature_names = np.array((\n",
    "    list(encoder.get_feature_names_out([\"Turbine\"])) + \n",
    "    [\"Hub_height\", \"Capacity\", \"Age\", \"Wind_speed\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Maske mit zufälligen Indizes erstellen\n",
    "random_indices = np.random.choice(len(combined_features), size=1000, replace=False)\n",
    "\n",
    "# Werte aus combined_features basierend auf der Maske auswählen\n",
    "masked_features = combined_features[random_indices]\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. shapley-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import shap\n",
    "\n",
    "# Eingabe für den SHAP-Explainer vorbereiten\n",
    "input_tensor = torch.tensor(masked_features, dtype=torch.float32).to(device)\n",
    "\n",
    "# SHAP-Explainer erstellen\n",
    "explainer = shap.GradientExplainer(model, input_tensor)\n",
    "\n",
    "# SHAP-Werte berechnen\n",
    "shap_values = explainer.shap_values(input_tensor)\n",
    "\n",
    "# SHAP-Werte umformen\n",
    "shap_values_reshaped = shap_values.reshape(shap_values.shape[0], shap_values.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "shap.summary_plot(shap_values_reshaped, masked_features, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import torch\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "# Modellvorhersagen auf dem Eingabedatensatz berechnen\n",
    "base_value = model(torch.tensor(masked_features, dtype=torch.float32).to(device)).mean().item()\n",
    "\n",
    "shap.plots.force(base_value, shap_values_reshaped, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "# Base Value in ein Array umwandeln\n",
    "base_values_array = np.full((shap_values_reshaped.shape[0],), base_value)\n",
    "\n",
    "# SHAP Explanation-Objekt erstellen\n",
    "shap_values_explanation = shap.Explanation(\n",
    "    values=shap_values_reshaped,\n",
    "    base_values=base_values_array,  # Array statt float\n",
    "    data=masked_features,  # Eingabedaten\n",
    "    feature_names=feature_names  # Feature-Namen\n",
    ")\n",
    "\n",
    "# Bar-Plot erstellen\n",
    "shap.plots.bar(shap_values_explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. gradient method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modell in Evaluierungsmodus\n",
    "model.eval()\n",
    "\n",
    "# Eingabegröße reduzieren\n",
    "input_tensor = torch.tensor(masked_features, dtype=torch.float32, requires_grad=True).to(device)\n",
    "\n",
    "# Vorhersage berechnen\n",
    "outputs = model(input_tensor)\n",
    "\n",
    "# Mittleren Output für den Gradienten berechnen\n",
    "outputs.mean().backward()\n",
    "\n",
    "# Gradienten extrahieren und sortieren\n",
    "feature_importance = input_tensor.grad.abs().mean(dim=0).cpu().detach().numpy()\n",
    "\n",
    "# Nach Wichtigkeit sortieren\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(np.array(feature_names)[sorted_idx], feature_importance[sorted_idx])\n",
    "plt.xlabel(\"Average Gradient Magnitude\")\n",
    "plt.title(\"Feature Importance using Gradients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Spearman Correlation Coefficient Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "num_turbine_type_columns = turbine_types_onehot.shape[1]\n",
    "\n",
    "# DataFrame ohne Turbinentypen erstellen\n",
    "df = pd.DataFrame(\n",
    "    combined_features[:, num_turbine_type_columns:], \n",
    "    columns=[\"Hub Height\", \"Capacity\", \"Commissioning Date\", \"Wind Speed\"]\n",
    ")\n",
    "\n",
    "# Ausgabe hinzufügen\n",
    "df['Power'] = output.flatten()\n",
    "\n",
    "# Spearman-Korrelation berechnen\n",
    "spearman_corr_matrix = df.corr(method='spearman')\n",
    "\n",
    "# Farbcodierte Tabelle anzeigen\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(spearman_corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title(\"Spearman Correlation Heatmap\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webapp_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
