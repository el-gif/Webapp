{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Load crowdsourced data and fetch weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 19:13:33,158 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-03-03 19:13:33,161 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping ERA5 reanalysis data for time_series_20250227_141730.xlsx, already exists.\n",
      "Downloading ECMWF reforecast data for time_series_20250227_141730.xlsx...\n",
      "2025-03-03 19:13:33 ECMWF API python library 1.6.3\n",
      "2025-03-03 19:13:33 ECMWF API at https://api.ecmwf.int/v1\n",
      "2025-03-03 19:13:34 Welcome Alexander Peters\n",
      "2025-03-03 19:13:36 In case of problems, please check https://confluence.ecmwf.int/display/WEBAPI/Web+API+FAQ or contact servicedesk@ecmwf.int\n",
      "2025-03-03 19:13:37 Request submitted\n",
      "2025-03-03 19:13:37 Request id: 67c5f152f86a24e21f09aab7\n",
      "2025-03-03 19:13:37 Request is submitted\n",
      "2025-03-03 19:13:40 Request is active\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181339 - Welcome to MARS\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181339 - MARS Client build stamp: 20240618101041\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181339 - MARS Client bundle version: 6.33.19.4\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181339 - package mars-client version: 6.33.19\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181339 - package mir version: 1.21.0\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181339 - package odc version: 1.5.0\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181339 - package fdb version: 5.12.1\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181339 - package metkit version: 1.11.12\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181339 - package eckit version: 1.26.2\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181339 - package eccodes version: 2.35.3\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181339 - Maximum retrieval size is 75.00 G\n",
      "2025-03-03 19:28:00 retrieve,levelist=100,stream=oper,area=53.5/10.0/53.5/10.0,levtype=sfc,expver=1,dataset=oper,padding=0,step=0/3/6/9/12/15/18/21/24/27/30/33/36/39/42/45/48/51/54/57/60/63/66/69/72/75/78/81/84/87/90/93/96/99/102/105/108/111/114/117/120/123/126/129/132/135/138/141/144,grid=0.25/0.25,param=165.128/166.128,time=00:00/12:00,date=2024-11-01/to/2024-11-31,type=fc,class=odmars - INFO   - 20250303.181339 - Automatic split on dates is on\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181339 - Request has been split into 2 monthly retrievals\n",
      "2025-03-03 19:28:00 \n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181339 - Processing request 1\n",
      "2025-03-03 19:28:00 \n",
      "2025-03-03 19:28:00 RETRIEVE,\n",
      "2025-03-03 19:28:00     DATASET    = oper,\n",
      "2025-03-03 19:28:00     CLASS      = OD,\n",
      "2025-03-03 19:28:00     TYPE       = FC,\n",
      "2025-03-03 19:28:00     STREAM     = OPER,\n",
      "2025-03-03 19:28:00     EXPVER     = 0001,\n",
      "2025-03-03 19:28:00     REPRES     = GG,\n",
      "2025-03-03 19:28:00     LEVTYPE    = SFC,\n",
      "2025-03-03 19:28:00     PARAM      = 165.128/166.128,\n",
      "2025-03-03 19:28:00     TIME       = 0000/1200,\n",
      "2025-03-03 19:28:00     STEP       = 0/3/6/9/12/15/18/21/24/27/30/33/36/39/42/45/48/51/54/57/60/63/66/69/72/75/78/81/84/87/90/93/96/99/102/105/108/111/114/117/120/123/126/129/132/135/138/141/144,\n",
      "2025-03-03 19:28:00     DOMAIN     = G,\n",
      "2025-03-03 19:28:00     RESOL      = AUTO,\n",
      "2025-03-03 19:28:00     AREA       = 53.5/10.0/53.5/10.0,\n",
      "2025-03-03 19:28:00     GRID       = 0.25/0.25,\n",
      "2025-03-03 19:28:00     PADDING    = 0,\n",
      "2025-03-03 19:28:00     DATE       = 20241101/20241102/20241103/20241104/20241105/20241106/20241107/20241108/20241109/20241110/20241111/20241112/20241113/20241114/20241115/20241116/20241117/20241118/20241119/20241120/20241121/20241122/20241123/20241124/20241125/20241126/20241127/20241128/20241129/20241130\n",
      "2025-03-03 19:28:00 \n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181339 - Web API request id: 67c5f152f86a24e21f09aab7\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181339 - Requesting 5880 fields\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181339 - Setting SO_SNDBUF to 33554432 (32.00 M)\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181339 - Current value is 8192 (8.00 K)\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181339 - Setting SO_RCVBUF to 33554432 (32.00 M)\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181339 - Current value is 43690 (42.67 K)\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181339 - Calling mars on 'fdbprod', local port is 42559\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181339 - Server task is 550 [ATOS FDB]\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181339 - Retrieving from FDB [ATOS FDB]\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181339 - Looking up FDB indexes: 0.390647 second elapsed, 0.251173 second cpu [ATOS FDB]\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181339 - Calling mars on 'marsod-core', local port is 55102\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181340 - Server task is 7696 [marsod]\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181340 - Request cost: 5,880 fields, 72.3107 Gbytes online, nodes: mvr021 mvr022 [marsod]\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181340 - The efficiency of your requests in the last 12 hours is 100% [marsod]\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181340 - Transfering 77642977440 bytes\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181342 - Cache file /data/ec_coeff/mir/weights/16/linear/R1280-8eadc35e89cffa3b706236fca69532c3-89.9462:0:-89.9462:359.93/LL-0.25x0.25-53.5:10:53.5:10-406b1a25ffe3882fd5b96a7d5467a1b8.mat does not exist\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181342 - Creating cache file /data/ec_coeff/mir/weights/16/linear/R1280-8eadc35e89cffa3b706236fca69532c3-89.9462:0:-89.9462:359.93/LL-0.25x0.25-53.5:10:53.5:10-406b1a25ffe3882fd5b96a7d5467a1b8.mat\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.181342 - CacheManager creating file /data/ec_coeff/mir/weights/16/linear/R1280-8eadc35e89cffa3b706236fca69532c3-89.9462:0:-89.9462:359.93/LL-0.25x0.25-53.5:10:53.5:10-406b1a25ffe3882fd5b96a7d5467a1b8.mat\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182625 - 5880 fields retrieved from 'marsod'\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182625 - 5880 fields have been interpolated\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182625 - Request time:  wall: 12 min 46 sec  cpu: 9 min 26 sec\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182625 -   Visiting marsod: wall: 12 min 45 sec\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182625 -   Read from network: 72.31 Gbyte(s) in 4 min 40 sec  [264.83 Mbyte/sec]\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182625 -   Post-processing: wall: 7 min 57 sec cpu: 7 min 56 sec\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182625 -   Writing to target file: 631.64 Kbyte(s) in < 1 sec [29.32 Mbyte/sec]\n",
      "2025-03-03 19:28:00 \n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182625 - Processing request 2\n",
      "2025-03-03 19:28:00 \n",
      "2025-03-03 19:28:00 RETRIEVE,\n",
      "2025-03-03 19:28:00     DATASET    = oper,\n",
      "2025-03-03 19:28:00     CLASS      = OD,\n",
      "2025-03-03 19:28:00     TYPE       = FC,\n",
      "2025-03-03 19:28:00     STREAM     = OPER,\n",
      "2025-03-03 19:28:00     EXPVER     = 0001,\n",
      "2025-03-03 19:28:00     REPRES     = GG,\n",
      "2025-03-03 19:28:00     LEVTYPE    = SFC,\n",
      "2025-03-03 19:28:00     PARAM      = 165.128/166.128,\n",
      "2025-03-03 19:28:00     TIME       = 0000/1200,\n",
      "2025-03-03 19:28:00     STEP       = 0/3/6/9/12/15/18/21/24/27/30/33/36/39/42/45/48/51/54/57/60/63/66/69/72/75/78/81/84/87/90/93/96/99/102/105/108/111/114/117/120/123/126/129/132/135/138/141/144,\n",
      "2025-03-03 19:28:00     DOMAIN     = G,\n",
      "2025-03-03 19:28:00     RESOL      = AUTO,\n",
      "2025-03-03 19:28:00     AREA       = 53.5/10.0/53.5/10.0,\n",
      "2025-03-03 19:28:00     GRID       = 0.25/0.25,\n",
      "2025-03-03 19:28:00     PADDING    = 0,\n",
      "2025-03-03 19:28:00     DATE       = 20241201\n",
      "2025-03-03 19:28:00 \n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182625 - Requesting 196 fields\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182625 - Calling mars on 'fdbprod', local port is 47773\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182625 - Server task is 116 [ATOS FDB]\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182625 - Retrieving from FDB [ATOS FDB]\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182625 - Looking up FDB indexes: 0.021998 second elapsed, 0.011511 second cpu [ATOS FDB]\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182625 - Calling mars on 'marsod-core', local port is 47623\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182625 - Server task is 3716 [marsod]\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182625 - Request cost: 196 fields, 2.41036 Gbytes online, nodes: mvr001 mvr004 mvr005 mvr007 mvr013 mvr018 mvr021 mvr022 [marsod]\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182625 - The efficiency of your requests in the last 12 hours is 100% [marsod]\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182625 - Transfering 2588099248 bytes\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182646 - 196 fields retrieved from 'marsod'\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182646 - 196 fields have been interpolated\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182646 - Request time:  wall: 21 sec  cpu: 17 sec\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182646 -   Visiting marsod: wall: 21 sec cpu: 17 sec\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182646 -   Read from network: 2.41 Gbyte(s) in 6 sec  [437.08 Mbyte/sec] cpu: 2 sec\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182646 -   Post-processing: wall: 15 sec cpu: 15 sec\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182646 -   Writing to target file: 21.05 Kbyte(s) in < 1 sec [6.32 Mbyte/sec]\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182646 - Memory used: 2.85 Gbyte(s)\n",
      "2025-03-03 19:28:00 mars - INFO   - 20250303.182646 - No errors reported\n",
      "2025-03-03 19:28:00 Process '['nice', 'mars', '/tmp/20250303-1810/00/tmp-_mars-8JRrau.req']' finished\n",
      "2025-03-03 19:28:00 Request is complete\n",
      "2025-03-03 19:28:00 Transfering 652.695 Kbytes into data/weather_crowdsourcing\\time_series_20250227_141730.xlsx_reforecast.grib\n",
      "2025-03-03 19:28:00 From https://apps.ecmwf.int/api/streaming/private/blue/02/20250303-1810/b2/_mars-bol-webmars-private-svc-blue-003-4a73a881a8d5eead47db9eff2f9935a4-vs_gPd.grib\n",
      "2025-03-03 19:28:02 Transfer rate 297.024 Kbytes/s\n",
      "2025-03-03 19:28:03 Done\n",
      "Saved reforecast data: data/weather_crowdsourcing\\time_series_20250227_141730.xlsx_reforecast.grib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 19:28:04,706 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-03-03 19:28:04,708 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ERA5 reanalysis data for time_series_20250301_182711.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 19:28:05,098 INFO Request ID is 74d02e8f-a5bf-4bfd-866f-20555b1e5192\n",
      "2025-03-03 19:28:05,194 INFO status has been updated to accepted\n",
      "2025-03-03 19:28:13,925 INFO status has been updated to running\n",
      "2025-03-03 19:28:19,062 INFO status has been updated to accepted\n",
      "2025-03-03 19:28:26,728 INFO status has been updated to running\n",
      "2025-03-03 19:28:55,347 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c28528bbb7949469f96d12e70ecb407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cd9300515129f9708102ce0b3cff3dac.grib:   0%|          | 0.00/101k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved reanalysis data: data/weather_crowdsourcing\\time_series_20250301_182711.xlsx_reanalysis.grib\n",
      "Downloading ECMWF reforecast data for time_series_20250301_182711.xlsx...\n",
      "2025-03-03 19:28:57 ECMWF API python library 1.6.3\n",
      "2025-03-03 19:28:57 ECMWF API at https://api.ecmwf.int/v1\n",
      "2025-03-03 19:28:58 Welcome Alexander Peters\n",
      "2025-03-03 19:29:02 In case of problems, please check https://confluence.ecmwf.int/display/WEBAPI/Web+API+FAQ or contact servicedesk@ecmwf.int\n",
      "2025-03-03 19:29:03 Request submitted\n",
      "2025-03-03 19:29:03 Request id: 67c5f4effa807164d909a9b5\n",
      "2025-03-03 19:29:03 Request is submitted\n",
      "2025-03-03 19:29:06 Request is active\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182904 - Welcome to MARS\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182904 - MARS Client build stamp: 20240618101041\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182904 - MARS Client bundle version: 6.33.19.4\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182904 - package mars-client version: 6.33.19\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182904 - package mir version: 1.21.0\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182904 - package odc version: 1.5.0\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182904 - package fdb version: 5.12.1\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182904 - package metkit version: 1.11.12\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182904 - package eckit version: 1.26.2\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182904 - package eccodes version: 2.35.3\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182904 - Maximum retrieval size is 75.00 G\n",
      "2025-03-03 19:41:25 retrieve,levelist=100,stream=oper,area=52.85/17.7/52.85/17.7,levtype=sfc,expver=1,dataset=oper,padding=0,step=0/3/6/9/12/15/18/21/24/27/30/33/36/39/42/45/48/51/54/57/60/63/66/69/72/75/78/81/84/87/90/93/96/99/102/105/108/111/114/117/120/123/126/129/132/135/138/141/144,grid=0.25/0.25,param=165.128/166.128,time=00:00/12:00,date=2024-11-01/to/2024-11-31,type=fc,class=odmars - INFO   - 20250303.182904 - Automatic split on dates is on\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182904 - Request has been split into 2 monthly retrievals\n",
      "2025-03-03 19:41:25 \n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182904 - Processing request 1\n",
      "2025-03-03 19:41:25 \n",
      "2025-03-03 19:41:25 RETRIEVE,\n",
      "2025-03-03 19:41:25     DATASET    = oper,\n",
      "2025-03-03 19:41:25     CLASS      = OD,\n",
      "2025-03-03 19:41:25     TYPE       = FC,\n",
      "2025-03-03 19:41:25     STREAM     = OPER,\n",
      "2025-03-03 19:41:25     EXPVER     = 0001,\n",
      "2025-03-03 19:41:25     REPRES     = GG,\n",
      "2025-03-03 19:41:25     LEVTYPE    = SFC,\n",
      "2025-03-03 19:41:25     PARAM      = 165.128/166.128,\n",
      "2025-03-03 19:41:25     TIME       = 0000/1200,\n",
      "2025-03-03 19:41:25     STEP       = 0/3/6/9/12/15/18/21/24/27/30/33/36/39/42/45/48/51/54/57/60/63/66/69/72/75/78/81/84/87/90/93/96/99/102/105/108/111/114/117/120/123/126/129/132/135/138/141/144,\n",
      "2025-03-03 19:41:25     DOMAIN     = G,\n",
      "2025-03-03 19:41:25     RESOL      = AUTO,\n",
      "2025-03-03 19:41:25     AREA       = 52.85/17.7/52.85/17.7,\n",
      "2025-03-03 19:41:25     GRID       = 0.25/0.25,\n",
      "2025-03-03 19:41:25     PADDING    = 0,\n",
      "2025-03-03 19:41:25     DATE       = 20241101/20241102/20241103/20241104/20241105/20241106/20241107/20241108/20241109/20241110/20241111/20241112/20241113/20241114/20241115/20241116/20241117/20241118/20241119/20241120/20241121/20241122/20241123/20241124/20241125/20241126/20241127/20241128/20241129/20241130\n",
      "2025-03-03 19:41:25 \n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182904 - Web API request id: 67c5f4effa807164d909a9b5\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182905 - Requesting 5880 fields\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182905 - Setting SO_SNDBUF to 33554432 (32.00 M)\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182905 - Current value is 8192 (8.00 K)\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182905 - Setting SO_RCVBUF to 33554432 (32.00 M)\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182905 - Current value is 43690 (42.67 K)\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182905 - Calling mars on 'fdbprod', local port is 58795\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182905 - Server task is 482 [ATOS FDB]\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182905 - Retrieving from FDB [ATOS FDB]\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182905 - Looking up FDB indexes: 0.3523 second elapsed, 0.252859 second cpu [ATOS FDB]\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182905 - Calling mars on 'marsod-core', local port is 57855\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182905 - Server task is 3506 [marsod]\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182905 - Request cost: 5,880 fields, 72.3107 Gbytes online, nodes: mvr021 mvr022 [marsod]\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182905 - The efficiency of your requests in the last 12 hours is 100% [marsod]\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182906 - Transfering 77642977440 bytes\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182907 - Cache file /data/ec_coeff/mir/weights/16/linear/R1280-8eadc35e89cffa3b706236fca69532c3-89.9462:0:-89.9462:359.93/LL-0.25x0.25-52.85:17.7:52.85:17.7-6530f141b7eeab931f9130875aef180a.mat does not exist\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182907 - Creating cache file /data/ec_coeff/mir/weights/16/linear/R1280-8eadc35e89cffa3b706236fca69532c3-89.9462:0:-89.9462:359.93/LL-0.25x0.25-52.85:17.7:52.85:17.7-6530f141b7eeab931f9130875aef180a.mat\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.182907 - CacheManager creating file /data/ec_coeff/mir/weights/16/linear/R1280-8eadc35e89cffa3b706236fca69532c3-89.9462:0:-89.9462:359.93/LL-0.25x0.25-52.85:17.7:52.85:17.7-6530f141b7eeab931f9130875aef180a.mat\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184018 - 5880 fields retrieved from 'marsod'\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184018 - 5880 fields have been interpolated\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184018 - Request time:  wall: 11 min 13 sec  cpu: 9 min 37 sec\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184018 -   Visiting marsod: wall: 11 min 13 sec\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184018 -   Read from network: 72.31 Gbyte(s) in 2 min 47 sec  [442.08 Mbyte/sec]\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184018 -   Post-processing: wall: 8 min 16 sec cpu: 8 min 15 sec\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184018 -   Writing to target file: 631.64 Kbyte(s) in < 1 sec [28.49 Mbyte/sec]\n",
      "2025-03-03 19:41:25 \n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184018 - Processing request 2\n",
      "2025-03-03 19:41:25 \n",
      "2025-03-03 19:41:25 RETRIEVE,\n",
      "2025-03-03 19:41:25     DATASET    = oper,\n",
      "2025-03-03 19:41:25     CLASS      = OD,\n",
      "2025-03-03 19:41:25     TYPE       = FC,\n",
      "2025-03-03 19:41:25     STREAM     = OPER,\n",
      "2025-03-03 19:41:25     EXPVER     = 0001,\n",
      "2025-03-03 19:41:25     REPRES     = GG,\n",
      "2025-03-03 19:41:25     LEVTYPE    = SFC,\n",
      "2025-03-03 19:41:25     PARAM      = 165.128/166.128,\n",
      "2025-03-03 19:41:25     TIME       = 0000/1200,\n",
      "2025-03-03 19:41:25     STEP       = 0/3/6/9/12/15/18/21/24/27/30/33/36/39/42/45/48/51/54/57/60/63/66/69/72/75/78/81/84/87/90/93/96/99/102/105/108/111/114/117/120/123/126/129/132/135/138/141/144,\n",
      "2025-03-03 19:41:25     DOMAIN     = G,\n",
      "2025-03-03 19:41:25     RESOL      = AUTO,\n",
      "2025-03-03 19:41:25     AREA       = 52.85/17.7/52.85/17.7,\n",
      "2025-03-03 19:41:25     GRID       = 0.25/0.25,\n",
      "2025-03-03 19:41:25     PADDING    = 0,\n",
      "2025-03-03 19:41:25     DATE       = 20241201\n",
      "2025-03-03 19:41:25 \n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184018 - Requesting 196 fields\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184018 - Calling mars on 'fdbprod', local port is 52194\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184018 - Server task is 249 [ATOS FDB]\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184018 - Retrieving from FDB [ATOS FDB]\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184018 - Looking up FDB indexes: 0.014968 second elapsed, 0.011284 second cpu [ATOS FDB]\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184018 - Calling mars on 'marsod-core', local port is 40624\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184018 - Server task is 3312 [marsod]\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184018 - Request cost: 196 fields, 2.41036 Gbytes online, nodes: mvr001 mvr004 mvr005 mvr007 mvr013 mvr018 mvr021 mvr022 [marsod]\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184018 - The efficiency of your requests in the last 12 hours is 100% [marsod]\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184018 - Transfering 2588099248 bytes\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184040 - 196 fields retrieved from 'marsod'\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184040 - 196 fields have been interpolated\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184040 - Request time:  wall: 22 sec  cpu: 19 sec\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184040 -   Visiting marsod: wall: 22 sec cpu: 19 sec\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184040 -   Read from network: 2.41 Gbyte(s) in 5 sec  [482.04 Mbyte/sec] cpu: 2 sec\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184040 -   Post-processing: wall: 17 sec cpu: 17 sec\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184040 -   Writing to target file: 21.05 Kbyte(s) in < 1 sec [5.68 Mbyte/sec]\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184040 - Memory used: 2.85 Gbyte(s)\n",
      "2025-03-03 19:41:25 mars - INFO   - 20250303.184040 - No errors reported\n",
      "2025-03-03 19:41:25 Process '['nice', 'mars', '/tmp/20250303-1820/9d/tmp-_mars-1A3Zqz.req']' finished\n",
      "2025-03-03 19:41:25 Request is complete\n",
      "2025-03-03 19:41:25 Transfering 652.695 Kbytes into data/weather_crowdsourcing\\time_series_20250301_182711.xlsx_reforecast.grib\n",
      "2025-03-03 19:41:25 From https://apps.ecmwf.int/api/streaming/private/blue/04/20250303-1820/8a/_mars-bol-webmars-private-svc-blue-004-7a527896970b09a4fc90fa37bf98d3ff-_xu1No.grib\n",
      "2025-03-03 19:41:26 Transfer rate 632.782 Kbytes/s\n",
      "2025-03-03 19:41:27 Done\n",
      "Saved reforecast data: data/weather_crowdsourcing\\time_series_20250301_182711.xlsx_reforecast.grib\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/crowdsourced_data/time_series_20250303_115110.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     14\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_dir, file)\n\u001b[1;32m---> 15\u001b[0m     sheets \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTime Series\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     time_series_data \u001b[38;5;241m=\u001b[39m sheets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime Series\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     17\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m sheets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1557\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/crowdsourced_data/time_series_20250303_115110.xlsx'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cdsapi  # for reanalysis data\n",
    "from ecmwfapi import ECMWFService  # for reforecast data\n",
    "\n",
    "input_dir = \"data/crowdsourced_data/\"\n",
    "output_dir = \"data/weather_crowdsourcing\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for file in os.listdir(input_dir):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        file_path = os.path.join(input_dir, file)\n",
    "        sheets = pd.read_excel(file_path, sheet_name=[\"Time Series\", \"Metadata\"])\n",
    "        time_series_data = sheets[\"Time Series\"]\n",
    "        metadata = sheets[\"Metadata\"]\n",
    "\n",
    "        timestamp = file.removeprefix(\"time_series_\").removesuffix(\".xlsx\")\n",
    "\n",
    "        time_series_data[\"Date\"] = pd.to_datetime(time_series_data[\"Date\"], errors='coerce')\n",
    "\n",
    "        # Extract unique years, months, days, and hours\n",
    "        years = sorted(set(time_series_data[\"Date\"].dt.year.dropna().astype(str)))\n",
    "        months = sorted(set(time_series_data[\"Date\"].dt.month.dropna().astype(str).str.zfill(2)))\n",
    "        days = sorted(set(time_series_data[\"Date\"].dt.day.dropna().astype(str).str.zfill(2)))\n",
    "        hours = sorted(set(time_series_data[\"Date\"].dt.hour.dropna().astype(str).str.zfill(2)))\n",
    "\n",
    "        # Extract location\n",
    "        lat, lon = metadata.iloc[0][\"Latitude\"], metadata.iloc[0][\"Longitude\"]\n",
    "\n",
    "        # =================== REANALYSIS DATA (ERA5) ===================\n",
    "\n",
    "        # API key saved in .cdsapirc\n",
    "        client = cdsapi.Client()\n",
    "        reanalysis_file = os.path.join(output_dir, f\"reanalysis_{timestamp}.grib\")\n",
    "\n",
    "        if not os.path.exists(reanalysis_file):  # Skip if already downloaded\n",
    "            print(f\"Downloading ERA5 reanalysis data for {timestamp}...\")\n",
    "\n",
    "            request = {\n",
    "                \"product_type\": \"reanalysis\",\n",
    "                \"variable\": [\"100m_u_component_of_wind\", \"100m_v_component_of_wind\"],\n",
    "                \"year\": years,\n",
    "                \"month\": months,\n",
    "                \"day\": days,\n",
    "                \"time\": hours,\n",
    "                \"format\": \"grib\",\n",
    "                \"area\": [lat+1, lon-1, lat-1, lon+1],  # N/W/S/E\n",
    "            }\n",
    "\n",
    "            client.retrieve(\"reanalysis-era5-single-levels\", request, reanalysis_file)\n",
    "            print(f\"Saved reanalysis data: {reanalysis_file}\")\n",
    "        else:\n",
    "            print(f\"Skipping ERA5 reanalysis data for {timestamp}, already exists.\")\n",
    "\n",
    "        # =================== REFORECAST DATA (ECMWF) ===================\n",
    "\n",
    "        # API key under https://api.ecmwf.int/v1/key/, saved in .ecmwfapirc\n",
    "        server = ECMWFService(\"mars\")\n",
    "        reforecast_file = os.path.join(output_dir, f\"reforecast_{timestamp}.grib\")\n",
    "\n",
    "        if not os.path.exists(reforecast_file):  # Skip if already downloaded\n",
    "            print(f\"Downloading ECMWF reforecast data for {timestamp}...\")\n",
    "\n",
    "            server.execute(\n",
    "                {\n",
    "                    \"class\": \"od\",\n",
    "                    \"dataset\": \"oper\",\n",
    "                    \"expver\": \"1\",\n",
    "                    \"stream\": \"oper\",\n",
    "                    \"type\": \"fc\",\n",
    "                    \"levtype\": \"sfc\",\n",
    "                    \"param\": \"165.128/166.128\",  # U/V wind components\n",
    "                    \"levelist\": \"100\",  # Wind at 100 meters\n",
    "                    \"date\": f\"{min(years)}-{min(months)}-01/to/{max(years)}-{max(months)}-31\",  # Ensure correct format\n",
    "                    \"time\": [\"00:00\", \"12:00\"],  # Forecast times\n",
    "                    \"step\": [str(i) for i in range(0, 145, 3)],  # Convert to list of strings\n",
    "                    \"grid\": \"0.25/0.25\",\n",
    "                    \"area\": [lat, lon, lat, lon],  # N/W/S/E\n",
    "                    \"format\": \"grib2\",\n",
    "                },\n",
    "                reforecast_file\n",
    "            )\n",
    "            print(f\"Saved reforecast data: {reforecast_file}\")\n",
    "        else:\n",
    "            print(f\"Skipping ECMWF reforecast data for {timestamp}, already exists.\")\n",
    "\n",
    "print(\"All downloads completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Build datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy.interpolate import interp2d\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Define lead times\n",
    "lead_times = list(range(0, 145, 3))\n",
    "\n",
    "# Directories\n",
    "reforecast_dir = \"data/crowdsourcing_weather\"\n",
    "output_dir = \"data/crowdsourcing_weather/json_lead_times\"\n",
    "input_dir = \"data/crowdsourced_data\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load crowdsourced WPP production data\n",
    "wpp_files = [f for f in os.listdir(input_dir) if f.endswith(\".xlsx\")]\n",
    "wpp_data = []\n",
    "\n",
    "for file in wpp_files:\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    sheets = pd.read_excel(file_path, sheet_name=[\"Time Series\", \"Metadata\"])\n",
    "    time_series_data = sheets[\"Time Series\"]\n",
    "    metadata = sheets[\"Metadata\"]\n",
    "\n",
    "    time_series_data[\"Date\"] = pd.to_datetime(time_series_data[\"Date\"], errors='coerce')\n",
    "    lat, lon = metadata.iloc[0][\"Latitude\"], metadata.iloc[0][\"Longitude\"]\n",
    "\n",
    "    wpp_data.append({\n",
    "        \"Name\": metadata.iloc[0][\"Name\"],\n",
    "        \"Latitude\": lat,\n",
    "        \"Longitude\": lon,\n",
    "        \"Production\": time_series_data.values.tolist()  # Store entire time series\n",
    "    })\n",
    "\n",
    "# Process each reforecast file\n",
    "for file in os.listdir(reforecast_dir):\n",
    "    if file.endswith(\".grib\"):\n",
    "        year, month = file.split(\"_\")[1:3]\n",
    "        grib_path = os.path.join(reforecast_dir, file)\n",
    "        json_output_path = os.path.join(output_dir, f\"WPPs+production+wind_lead_times_{year}_{month}.json\")\n",
    "\n",
    "        if os.path.exists(json_output_path):\n",
    "            print(f\"Skipping {json_output_path}, already exists.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing {file}...\")\n",
    "        ds = xr.open_dataset(grib_path, engine=\"cfgrib\", chunks={\"time\": 100})\n",
    "        times = pd.to_datetime(ds[\"valid_time\"].values)\n",
    "        latitudes = ds[\"latitude\"].values\n",
    "        longitudes = ds[\"longitude\"].values\n",
    "        u = ds[\"u100\"].values\n",
    "        v = ds[\"v100\"].values\n",
    "\n",
    "        lead_time_dicts = {str(lt): {} for lt in lead_times}\n",
    "\n",
    "        for wpp in wpp_data:\n",
    "            lon, lat = wpp[\"Longitude\"], wpp[\"Latitude\"]\n",
    "            production = wpp[\"Production\"]\n",
    "\n",
    "            interpolated_data = []\n",
    "\n",
    "            for entry in production:\n",
    "                time_str, production_value = entry\n",
    "                timestamp = pd.to_datetime(time_str)\n",
    "\n",
    "                for j, forecast_time in enumerate(times):\n",
    "                    if timestamp in forecast_time:\n",
    "                        time_index = forecast_time.get_loc(timestamp)\n",
    "                        lead_time = lead_times[time_index]\n",
    "\n",
    "                        wind_speeds = np.sqrt(u[j]**2 + v[j]**2)\n",
    "                        spatial_interpolator = interp2d(longitudes, latitudes, wind_speeds, kind=\"linear\")\n",
    "                        wind_speed_value = spatial_interpolator(lon, lat)[0]\n",
    "                        interpolated_data.append([lead_time, time_str, production_value, round(wind_speed_value, 2)])\n",
    "\n",
    "            for lead_time in lead_times:\n",
    "                lead_time_str = str(lead_time)\n",
    "                lead_time_data = [entry for entry in interpolated_data if entry[0] == lead_time]\n",
    "\n",
    "                if not lead_time_data:\n",
    "                    continue\n",
    "\n",
    "                lead_time_dicts[lead_time_str][wpp[\"Name\"]] = {\n",
    "                    \"Latitude\": wpp[\"Latitude\"],\n",
    "                    \"Longitude\": wpp[\"Longitude\"],\n",
    "                    \"Time Series\": [[entry[1], entry[2], entry[3]] for entry in lead_time_data]\n",
    "                }\n",
    "\n",
    "        with open(json_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(lead_time_dicts, f, indent=4)\n",
    "        print(f\"Saved reforecast JSON: {json_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add weather data (suggestion ChatGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy.interpolate import interp2d\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Define lead times\n",
    "lead_times = list(range(0, 145, 3))\n",
    "\n",
    "# Directories\n",
    "reforecast_dir = \"data/weather_crowdsourcing\"\n",
    "output_dir = \"data/weather_crowdsourcing/json_lead_times\"\n",
    "input_dir = \"data/crowdsourced_data\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load crowdsourced WPP production data\n",
    "wpp_files = [f for f in os.listdir(input_dir) if f.endswith(\".xlsx\")]\n",
    "wpp_data = []\n",
    "\n",
    "for file in wpp_files:\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    sheets = pd.read_excel(file_path, sheet_name=[\"Time Series\", \"Metadata\"])\n",
    "    time_series_data = sheets[\"Time Series\"]\n",
    "    metadata = sheets[\"Metadata\"]\n",
    "\n",
    "    time_series_data[\"Date\"] = pd.to_datetime(time_series_data[\"Date\"], errors='coerce')\n",
    "    lat, lon = metadata.iloc[0][\"Latitude\"], metadata.iloc[0][\"Longitude\"]\n",
    "\n",
    "    wpp_data.append({\n",
    "        \"Name\": metadata.iloc[0][\"Name\"],\n",
    "        \"Latitude\": lat,\n",
    "        \"Longitude\": lon,\n",
    "        \"Production\": time_series_data.values.tolist()  # Store entire time series\n",
    "    })\n",
    "\n",
    "# Process each reforecast file\n",
    "for file in os.listdir(reforecast_dir):\n",
    "    if file.endswith(\".grib\"):\n",
    "        year, month = file.split(\"_\")[1:3]\n",
    "        grib_path = os.path.join(reforecast_dir, file)\n",
    "        json_output_path = os.path.join(output_dir, f\"WPPs+production+wind_lead_times_{year}_{month}.json\")\n",
    "\n",
    "        if os.path.exists(json_output_path):\n",
    "            print(f\"Skipping {json_output_path}, already exists.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing {file}...\")\n",
    "        ds = xr.open_dataset(grib_path, engine=\"cfgrib\", chunks={\"time\": 100})\n",
    "        times = pd.to_datetime(ds[\"valid_time\"].values)\n",
    "        latitudes = ds[\"latitude\"].values\n",
    "        longitudes = ds[\"longitude\"].values\n",
    "        u = ds[\"u100\"].values\n",
    "        v = ds[\"v100\"].values\n",
    "\n",
    "        lead_time_dicts = {str(lt): {} for lt in lead_times}\n",
    "\n",
    "        for wpp in wpp_data:\n",
    "            lon, lat = wpp[\"Longitude\"], wpp[\"Latitude\"]\n",
    "            production = wpp[\"Production\"]\n",
    "\n",
    "            interpolated_data = []\n",
    "\n",
    "            for entry in production:\n",
    "                time_str, production_value = entry\n",
    "                timestamp = pd.to_datetime(time_str)\n",
    "\n",
    "                for j, forecast_time in enumerate(times):\n",
    "                    if timestamp in forecast_time:\n",
    "                        time_index = forecast_time.get_loc(timestamp)\n",
    "                        lead_time = lead_times[time_index]\n",
    "\n",
    "                        wind_speeds = np.sqrt(u[j]**2 + v[j]**2)\n",
    "                        spatial_interpolator = interp2d(longitudes, latitudes, wind_speeds, kind=\"linear\")\n",
    "                        wind_speed_value = spatial_interpolator(lon, lat)[0]\n",
    "                        interpolated_data.append([lead_time, time_str, production_value, round(wind_speed_value, 2)])\n",
    "\n",
    "            for lead_time in lead_times:\n",
    "                lead_time_str = str(lead_time)\n",
    "                lead_time_data = [entry for entry in interpolated_data if entry[0] == lead_time]\n",
    "\n",
    "                if not lead_time_data:\n",
    "                    continue\n",
    "\n",
    "                lead_time_dicts[lead_time_str][wpp[\"Name\"]] = {\n",
    "                    \"Latitude\": wpp[\"Latitude\"],\n",
    "                    \"Longitude\": wpp[\"Longitude\"],\n",
    "                    \"Time Series\": [[entry[1], entry[2], entry[3]] for entry in lead_time_data]\n",
    "                }\n",
    "\n",
    "        with open(json_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(lead_time_dicts, f, indent=4)\n",
    "        print(f\"Saved reforecast JSON: {json_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "old Crowdsourcing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        # Read GRIB file using xarray and cfgrib\n",
    "        ds = xr.open_dataset(file_name, engine='cfgrib')\n",
    "\n",
    "        # Extract u and v wind components, latitudes, longitudes, and times\n",
    "        u_wind = ds[\"u100\"].values\n",
    "        v_wind = ds[\"v100\"].values\n",
    "        latitudes = ds[\"latitude\"].values\n",
    "        longitudes = ds[\"longitude\"].values\n",
    "        times = pd.to_datetime(ds[\"time\"].values)\n",
    "\n",
    "        # Prepare static features (shared across timestamps)\n",
    "        hub_height = metadata.iloc[0][\"Hub Height\"]\n",
    "        commissioning_year = metadata.iloc[0][\"Commissioning Year\"]\n",
    "        commissioning_month = metadata.iloc[0][\"Commissioning Month\"]\n",
    "        ref_date = pd.Timestamp(\"2024-12-01\")\n",
    "        age_months = (ref_date.year - commissioning_year) * 12 + (ref_date.month - commissioning_month)\n",
    "        capacity = metadata.iloc[0][\"Capacity (MW)\"]\n",
    "        turbine_type = metadata.iloc[0][\"Turbine Type\"]\n",
    "\n",
    "        scaled_hub_height = scalers[\"hub_heights\"].transform([[hub_height]])[0][0]\n",
    "        scaled_age = scalers[\"ages\"].transform([[age_months]])[0][0]\n",
    "\n",
    "        num_rows = len(time_series_data[\"Date\"])\n",
    "        hub_height_repeated = np.full((num_rows, 1), scaled_hub_height)\n",
    "        age_repeated = np.full((num_rows, 1), scaled_age)\n",
    "\n",
    "        turbine_type_encoded = np.zeros(len(known_turbine_types))\n",
    "        if turbine_type in known_turbine_types:\n",
    "            turbine_type_encoded[np.where(known_turbine_types == turbine_type)[0][0]] = 1\n",
    "        turbine_type_repeated = np.tile(turbine_type_encoded, (num_rows, 1))\n",
    "\n",
    "        interpolated_production = []\n",
    "\n",
    "        # Iterate over each timestamp in the uploaded data\n",
    "        for _, row in time_series_data.iterrows():\n",
    "            timestamp = row[\"Date\"]\n",
    "            production_value = row[\"Production (kW)\"] / 1e3 # convert to MW\n",
    "\n",
    "            if timestamp in times.values:\n",
    "                time_index = times.get_loc(timestamp)\n",
    "\n",
    "                u = u_wind[time_index]\n",
    "                v = v_wind[time_index]\n",
    "                wind_speed = np.sqrt(u**2 + v**2)\n",
    "\n",
    "                interpolator = interp2d(longitudes, latitudes, wind_speed, kind='linear')\n",
    "                wind_speed_value = interpolator(lon, lat)[0]\n",
    "                wind_speed_value = round(wind_speed_value, 2)\n",
    "\n",
    "                interpolated_production.append([timestamp, production_value, wind_speed_value])\n",
    "\n",
    "        productions = np.array([interpolated_value[1] for interpolated_value in interpolated_production]).reshape(-1, 1)\n",
    "        wind_speeds = np.array([interpolated_value[2] for interpolated_value in interpolated_production])\n",
    "        \n",
    "        scaled_wind_speeds = scalers[\"winds\"].transform(wind_speeds.reshape(-1, 1))\n",
    "\n",
    "        # Prepare input features\n",
    "        input_features = np.hstack([turbine_type_repeated, hub_height_repeated, age_repeated, scaled_wind_speeds])\n",
    "        input_tensor = torch.tensor(input_features, dtype=torch.float32).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(input_tensor)\n",
    "\n",
    "        huber_loss = nn.HuberLoss()(predictions, torch.tensor(productions)).item()\n",
    "\n",
    "        # Update model if the huber loss is < 1\n",
    "        if huber_loss < 1:\n",
    "            model.train()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "            for epoch in range(10):\n",
    "                optimizer.zero_grad()\n",
    "                output = model(input_tensor)\n",
    "                loss = nn.HuberLoss()(output, torch.tensor(productions, dtype=torch.float32).to(device))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Save updated model\n",
    "            timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            updated_model_path = os.path.join(model_path, f\"trained_parameters_{timestamp_str}.pth\")\n",
    "            torch.save(model.state_dict(), updated_model_path)\n",
    "            print(f\"Updated model saved at {updated_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copied code to train model C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.nn import HuberLoss, MSELoss, L1Loss\n",
    "\n",
    "# Lists to store models and scalers\n",
    "models = {}\n",
    "scalers = {}\n",
    "encoders = {}\n",
    "input_sizes = {}\n",
    "metrics = {}\n",
    "\n",
    "# Define MLP class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3366)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# PyTorch Dataset Class\n",
    "class WindPowerDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.targets[index]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "input_dir = r\"E:\\MA_data\\WPPs+production+reforecast\"\n",
    "\n",
    "for file in os.listdir(input_dir):\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    if os.path.isfile(file_path):  # Ensure it's a file (not a folder)\n",
    "        lead_time = int(file.split(\"_\")[-1].replace(\".json\", \"\"))\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            forecast_data = json.load(file)\n",
    "\n",
    "    print(f\"Processing lead time: {lead_time}\")\n",
    "\n",
    "    print(f\"    Data preparation\")\n",
    "\n",
    "    all_turbine_types = []\n",
    "    all_hub_heights = []\n",
    "    all_capacities = []\n",
    "    all_commissioning_dates = []\n",
    "    all_production_data = []\n",
    "\n",
    "    for unique_key, wpp in forecast_data.items():\n",
    "        all_turbine_types.append(str(wpp[\"Turbine\"]))\n",
    "        all_hub_heights.append(wpp[\"Hub_height\"])\n",
    "        all_capacities.append(wpp[\"Capacity\"])\n",
    "        all_commissioning_dates.append(f\"{wpp['Commissioning_date']}/06\" if isinstance(wpp[\"Commissioning_date\"], str) and \"/\" not in wpp[\"Commissioning_date\"] else wpp[\"Commissioning_date\"])\n",
    "        all_production_data.append(wpp[\"Time Series\"])\n",
    "\n",
    "    # One-Hot-Encoding for turbine types\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    turbine_types_onehot = encoder.fit_transform(np.array(all_turbine_types).reshape(-1, 1))\n",
    "\n",
    "    # convert to datetime\n",
    "    standardised_dates = pd.to_datetime(all_commissioning_dates, format='%Y/%m')\n",
    "\n",
    "    # calculate age\n",
    "    ref_date = pd.Timestamp(\"2024-12-01\")\n",
    "    ages = ref_date.year * 12 + ref_date.month - (standardised_dates.year * 12 + standardised_dates.month)\n",
    "\n",
    "    # create combined features and output lists\n",
    "    combined_features_raw = []\n",
    "    output_raw = []\n",
    "    \n",
    "    # convert data in feature arrays\n",
    "    for idx, production_data in enumerate(all_production_data):\n",
    "        num_rows = len(production_data)\n",
    "\n",
    "        # Repetitions for common features\n",
    "        turbine_type_repeated = np.tile(turbine_types_onehot[idx], (num_rows, 1))\n",
    "        hub_height_repeated = np.full((num_rows, 1), float(all_hub_heights[idx]))\n",
    "        age_repeated = np.full((num_rows, 1), ages[idx])\n",
    "\n",
    "        # Extract production values and wind speeds\n",
    "        production_values = np.array([entry[1] for entry in production_data]).reshape(-1, 1) / all_capacities[idx]\n",
    "        wind_speeds = np.array([entry[2] for entry in production_data]).reshape(-1, 1)\n",
    "\n",
    "        # combine all features\n",
    "        combined_chunk = np.hstack((\n",
    "            turbine_type_repeated,\n",
    "            hub_height_repeated,\n",
    "            age_repeated,\n",
    "            wind_speeds\n",
    "        ))\n",
    "\n",
    "        # add the data\n",
    "        combined_features_raw.append(combined_chunk)\n",
    "        output_raw.append(production_values)\n",
    "\n",
    "    # combine all data chunks to one array\n",
    "    combined_features = np.vstack(combined_features_raw)\n",
    "    output = np.vstack(output_raw)\n",
    "\n",
    "    # Interpolate missing values (linear interpolation) in pandas\n",
    "    wind_speed_series = pd.Series(combined_features[:, -1])\n",
    "    wind_speed_series.interpolate(method='linear', inplace=True)\n",
    "    combined_features[:, -1] = wind_speed_series.to_numpy()\n",
    "\n",
    "    # round all values to four decimal places\n",
    "    combined_features = np.round(combined_features, decimals=4)\n",
    "    output = np.round(output, decimals=4)\n",
    "        \n",
    "    # Normalise numerical features\n",
    "    scaler_wind = StandardScaler()\n",
    "    scaler_ages = StandardScaler()\n",
    "    scaler_hub_heights = StandardScaler()\n",
    "\n",
    "    # Skalieren der einzelnen Features\n",
    "    combined_features[:, -1] = scaler_wind.fit_transform(combined_features[:, -1].reshape(-1, 1)).flatten() # scale wind speeds\n",
    "    combined_features[:, -2] = scaler_ages.fit_transform(combined_features[:, -2].reshape(-1, 1)).flatten()  # scale ages\n",
    "    combined_features[:, -3] = scaler_hub_heights.fit_transform(combined_features[:, -3].reshape(-1, 1)).flatten()  # scale hub heights\n",
    "    \n",
    "    # Convert to PyTorch Dataset\n",
    "    dataset = WindPowerDataset(combined_features, output)\n",
    "    \n",
    "    params = {\"batch_size\": 128,\n",
    "              \"lr\": 0.00010155,\n",
    "              \"number_epochs\": 10}\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # shuffling matters here\n",
    "    data_loader = DataLoader(dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    \n",
    "    # Model setup\n",
    "    input_size = combined_features.shape[1]\n",
    "\n",
    "    # use static instead of dynamic computational graphs\n",
    "    model = torch.jit.script(MLP(input_size=input_size)).to(device)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Trainings-Konfiguration\n",
    "    mae_criterion = L1Loss()\n",
    "    mse_criterion = MSELoss()\n",
    "    huber_criterion = HuberLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "\n",
    "    # Training\n",
    "    print(f\"    Training\")\n",
    "    for epoch in range(params[\"number_epochs\"]):\n",
    "        print(f\"        Epoch {epoch + 1}/{params['number_epochs']}\")\n",
    "        model.train()\n",
    "        train_loss_mae, train_loss_mse, train_loss_huber = 0, 0, 0\n",
    "\n",
    "        for batch_x, batch_y in data_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            \n",
    "            # Calculate metrics for each criterion\n",
    "            loss_mae = mae_criterion(outputs, batch_y)\n",
    "            loss_mse = mse_criterion(outputs, batch_y)\n",
    "            loss_huber = huber_criterion(outputs, batch_y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_huber.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate metrics for logging\n",
    "            train_loss_mae += loss_mae.item()\n",
    "            train_loss_mse += loss_mse.item()\n",
    "            train_loss_huber += loss_huber.item()\n",
    "\n",
    "        train_loss_mae /= len(data_loader)\n",
    "        train_loss_mse /= len(data_loader)\n",
    "        train_loss_huber /= len(data_loader)\n",
    "    \n",
    "    models[lead_time] = model.state_dict()\n",
    "    \n",
    "    scalers[lead_time] = {\n",
    "        \"winds\": scaler_wind,\n",
    "        \"ages\": scaler_ages,\n",
    "        \"hub_heights\": scaler_hub_heights\n",
    "    }\n",
    "\n",
    "    encoders[lead_time] = encoder\n",
    "\n",
    "    input_sizes[lead_time] = input_size\n",
    "\n",
    "    metrics[lead_time] = {\n",
    "        \"Huber\": train_loss_huber,\n",
    "        \"MAE\": train_loss_mae,\n",
    "        \"MSE\":train_loss_mse,\n",
    "        \"RMSE\": np.sqrt(train_loss_mse)\n",
    "    }\n",
    "\n",
    "# Save all parameters\n",
    "torch.save(models, \"parameters_deployment/models.pth\")\n",
    "joblib.dump(scalers, \"parameters_deployment/scalers.pkl\")\n",
    "joblib.dump(encoders, \"parameters_deployment/encoders.pkl\")\n",
    "joblib.dump(input_sizes, \"parameters_deployment/input_sizes.pkl\")\n",
    "joblib.dump(metrics, \"parameters_deployment/metrics.pkl\")\n",
    "print(\"All parameters saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webapp_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
