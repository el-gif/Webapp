{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 21:56:11,638 INFO [2024-09-28T00:00:00] **Welcome to the New Climate Data Store (CDS)!** This new system is in its early days of full operations and still undergoing enhancements and fine tuning. Some disruptions are to be expected. Your \n",
      "[feedback](https://jira.ecmwf.int/plugins/servlet/desk/portal/1/create/202) is key to improve the user experience on the new CDS for the benefit of everyone. Thank you.\n",
      "2025-01-13 21:56:11,639 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-01-13 21:56:11,640 INFO [2024-09-16T00:00:00] Remember that you need to have an ECMWF account to use the new CDS. **Your old CDS credentials will not work in new CDS!**\n",
      "2025-01-13 21:56:11,641 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2025-01-13 21:56:12,272 INFO [2025-01-09T00:00:00] Please be aware that ERA5 data from 1st January 2025 was degraded and is being corrected. Watch the [Forum announcement](https://forum.ecmwf.int/t/era5-data-from-1st-january-2025-was-degraded-and-is-being-corrected/10689) for updates.\n",
      "2025-01-13 21:56:12,274 INFO Request ID is a796dabe-fe89-491d-b1aa-18fbeffda1dd\n",
      "2025-01-13 21:56:12,353 INFO status has been updated to accepted\n",
      "2025-01-13 21:56:17,395 INFO status has been updated to running\n",
      "2025-01-13 21:56:20,844 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fdb3a71871643fba4191bcf1dafe1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "71f06b7eb4bc88ec4e084a07c992fdd8.grib:   0%|          | 0.00/88.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring index file 'wind_data_time_series_20250113_160903.xlsx.grib.5b7b6.idx' older than GRIB file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated model saved at model2/trained_parameters_20250113_215624.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cdsapi\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from scipy.interpolate import interp2d\n",
    "import xarray as xr\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Define MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, use_dropout=False, dropout_rate=0.3, use_batch_norm=False, activation_fn=nn.ReLU):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = [\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.BatchNorm1d(256) if use_batch_norm else nn.Identity(),\n",
    "            activation_fn(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128) if use_batch_norm else nn.Identity(),\n",
    "            activation_fn(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64) if use_batch_norm else nn.Identity(),\n",
    "            activation_fn(),\n",
    "        ]\n",
    "        if use_dropout:\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "        layers.append(nn.Linear(64, 1))\n",
    "        self.model = nn.Sequential(*[layer for layer in layers if not isinstance(layer, nn.Identity)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Load model files\n",
    "model_path = \"model2/\"\n",
    "input_size = torch.load(os.path.join(model_path, \"input_size\"), weights_only=True)\n",
    "scalers = joblib.load(os.path.join(model_path, \"scalers.pkl\"))\n",
    "model_params_path = os.path.join(model_path, \"trained_parameters.pth\")\n",
    "known_turbine_types = np.load(os.path.join(model_path, \"turbine_types_order.npy\"))\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MLP(input_size=input_size).to(device)\n",
    "model.load_state_dict(torch.load(model_params_path, weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "data_dir = \"crowdsourced_data/\"\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        sheets = pd.read_excel(file_path, sheet_name=[\"Time Series\", \"Metadata\"])\n",
    "        time_series_data = sheets[\"Time Series\"]\n",
    "        metadata = sheets[\"Metadata\"]\n",
    "\n",
    "        time_series_data[\"Date\"] = pd.to_datetime(time_series_data[\"Date\"], errors='coerce')\n",
    "\n",
    "        years = list(set(time_series_data[\"Date\"].dt.year.dropna().astype(str)))\n",
    "        months = list(set(time_series_data[\"Date\"].dt.month.dropna().astype(str).str.zfill(2)))\n",
    "        days = list(set(time_series_data[\"Date\"].dt.day.dropna().astype(str).str.zfill(2)))\n",
    "        hours = list(set(time_series_data[\"Date\"].dt.hour.dropna().astype(str).str.zfill(2)))\n",
    "\n",
    "        # Prepare API request for the full period\n",
    "        lat, lon = metadata.iloc[0][\"Latitude\"], metadata.iloc[0][\"Longitude\"]\n",
    "        client = cdsapi.Client()\n",
    "        request = {\n",
    "            \"product_type\": \"reanalysis\",\n",
    "            \"variable\": [\"100m_u_component_of_wind\", \"100m_v_component_of_wind\"],\n",
    "            \"year\": years,\n",
    "            \"month\": months,\n",
    "            \"day\": days,\n",
    "            \"time\": hours,\n",
    "            \"data_format\": \"grib\",\n",
    "            \"area\": [lat + 1, lon - 1, lat - 1, lon + 1],\n",
    "        }\n",
    "\n",
    "        # Download all data at once\n",
    "        file_name = f\"wind_data_{file}.grib\"\n",
    "        client.retrieve(\"reanalysis-era5-single-levels\", request).download(file_name)\n",
    "\n",
    "        # Read GRIB file using xarray and cfgrib\n",
    "        ds = xr.open_dataset(file_name, engine='cfgrib')\n",
    "\n",
    "        # Extract u and v wind components, latitudes, longitudes, and times\n",
    "        u_wind = ds[\"u100\"].values\n",
    "        v_wind = ds[\"v100\"].values\n",
    "        latitudes = ds[\"latitude\"].values\n",
    "        longitudes = ds[\"longitude\"].values\n",
    "        times = pd.to_datetime(ds[\"time\"].values)\n",
    "\n",
    "        # Prepare static features (shared across timestamps)\n",
    "        hub_height = metadata.iloc[0][\"Hub Height\"]\n",
    "        commissioning_year = metadata.iloc[0][\"Commissioning Year\"]\n",
    "        commissioning_month = metadata.iloc[0][\"Commissioning Month\"]\n",
    "        ref_date = pd.Timestamp(\"2024-12-01\")\n",
    "        age_months = (ref_date.year - commissioning_year) * 12 + (ref_date.month - commissioning_month)\n",
    "        capacity = metadata.iloc[0][\"Capacity (MW)\"]\n",
    "        turbine_type = metadata.iloc[0][\"Turbine Type\"]\n",
    "\n",
    "        scaled_hub_height = scalers[\"hub_heights\"].transform([[hub_height]])[0][0]\n",
    "        scaled_age = scalers[\"ages\"].transform([[age_months]])[0][0]\n",
    "\n",
    "        num_rows = len(time_series_data[\"Date\"])\n",
    "        hub_height_repeated = np.full((num_rows, 1), scaled_hub_height)\n",
    "        age_repeated = np.full((num_rows, 1), scaled_age)\n",
    "\n",
    "        turbine_type_encoded = np.zeros(len(known_turbine_types))\n",
    "        if turbine_type in known_turbine_types:\n",
    "            turbine_type_encoded[np.where(known_turbine_types == turbine_type)[0][0]] = 1\n",
    "        turbine_type_repeated = np.tile(turbine_type_encoded, (num_rows, 1))\n",
    "\n",
    "        interpolated_production = []\n",
    "\n",
    "        # Iterate over each timestamp in the uploaded data\n",
    "        for _, row in time_series_data.iterrows():\n",
    "            timestamp = row[\"Date\"]\n",
    "            production_value = row[\"Production (kW)\"] / 1e3 # convert to MW\n",
    "\n",
    "            if timestamp in times.values:\n",
    "                time_index = times.get_loc(timestamp)\n",
    "\n",
    "                u = u_wind[time_index]\n",
    "                v = v_wind[time_index]\n",
    "                wind_speed = np.sqrt(u**2 + v**2)\n",
    "\n",
    "                interpolator = interp2d(longitudes, latitudes, wind_speed, kind='linear')\n",
    "                wind_speed_value = interpolator(lon, lat)[0]\n",
    "                wind_speed_value = round(wind_speed_value, 2)\n",
    "\n",
    "                interpolated_production.append([timestamp, production_value, wind_speed_value])\n",
    "\n",
    "        productions = np.array([interpolated_value[1] for interpolated_value in interpolated_production]).reshape(-1, 1)\n",
    "        wind_speeds = np.array([interpolated_value[2] for interpolated_value in interpolated_production])\n",
    "        \n",
    "        scaled_wind_speeds = scalers[\"winds\"].transform(wind_speeds.reshape(-1, 1))\n",
    "\n",
    "        # Prepare input features\n",
    "        input_features = np.hstack([turbine_type_repeated, hub_height_repeated, age_repeated, scaled_wind_speeds])\n",
    "        input_tensor = torch.tensor(input_features, dtype=torch.float32).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(input_tensor)\n",
    "\n",
    "        huber_loss = nn.HuberLoss()(predictions, torch.tensor(productions)).item()\n",
    "\n",
    "        # Update model if the huber loss is < 1\n",
    "        if huber_loss < 1:\n",
    "            model.train()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "            for epoch in range(10):\n",
    "                optimizer.zero_grad()\n",
    "                output = model(input_tensor)\n",
    "                loss = nn.HuberLoss()(output, torch.tensor(productions, dtype=torch.float32).to(device))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Save updated model\n",
    "            timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            updated_model_path = os.path.join(model_path, f\"trained_parameters_{timestamp_str}.pth\")\n",
    "            torch.save(model.state_dict(), updated_model_path)\n",
    "            print(f\"Updated model saved at {updated_model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webapp_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
