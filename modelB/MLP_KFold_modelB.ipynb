{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Build feature and output arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import joblib\n",
    "\n",
    "# lists for all data\n",
    "all_turbine_types = []\n",
    "all_hub_heights = []\n",
    "all_capacities = []\n",
    "all_commissioning_dates = []\n",
    "all_production_data = []\n",
    "\n",
    "with open(r\"E:\\MA_data\\WPPs+production+reanalysis\\WPPs+production+wind_new.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    WPP_production_wind = json.load(file)\n",
    "\n",
    "# collect data\n",
    "for wpp in WPP_production_wind:\n",
    "    all_turbine_types.append(str(wpp[\"Turbine\"]))\n",
    "    all_hub_heights.append(wpp[\"Hub_height\"])\n",
    "    all_capacities.append(wpp[\"Capacity\"])\n",
    "    all_commissioning_dates.append(f\"{wpp['Commissioning_date']}/06\" if isinstance(wpp[\"Commissioning_date\"], str) and \"/\" not in wpp[\"Commissioning_date\"] else wpp[\"Commissioning_date\"])\n",
    "    all_production_data.append(wpp[\"Production\"])\n",
    "\n",
    "# One-Hot-Encoding for turbine types\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "turbine_types_onehot = encoder.fit_transform(np.array(all_turbine_types).reshape(-1, 1))\n",
    "\n",
    "# convert to datetime\n",
    "standardised_dates = pd.to_datetime(all_commissioning_dates, format='%Y/%m')\n",
    "\n",
    "# calculate age\n",
    "ref_date = pd.Timestamp(\"2024-12-01\")\n",
    "ages = ref_date.year * 12 + ref_date.month - (standardised_dates.year * 12 + standardised_dates.month)\n",
    "\n",
    "# create combined features and output lists\n",
    "combined_features_raw = []\n",
    "output_raw = []\n",
    "\n",
    "# convert data in feature arrays\n",
    "for idx, production_data in enumerate(all_production_data):\n",
    "    num_rows = len(production_data)\n",
    "\n",
    "    # repetitions for common features\n",
    "    turbine_type_repeated = np.tile(turbine_types_onehot[idx], (num_rows, 1))\n",
    "    hub_height_repeated = np.full((num_rows, 1), all_hub_heights[idx])\n",
    "    age_repeated = np.full((num_rows, 1), ages[idx])\n",
    "    capacities_repeated = np.full((num_rows, 1), all_capacities[idx])\n",
    "\n",
    "    # extract production values and wind speeds\n",
    "    production_values = np.array([entry[1] for entry in production_data]).reshape(-1, 1) / all_capacities[idx]\n",
    "    wind_speeds = np.array([entry[2] for entry in production_data]).reshape(-1, 1)\n",
    "\n",
    "    # combine all features\n",
    "    combined_chunk = np.hstack((\n",
    "        turbine_type_repeated,\n",
    "        hub_height_repeated,\n",
    "        age_repeated,\n",
    "        wind_speeds\n",
    "    ))\n",
    "\n",
    "    # add the data\n",
    "    combined_features_raw.append(combined_chunk)\n",
    "    output_raw.append(production_values)\n",
    "\n",
    "joblib.dump(encoder, \"parameters/encoder.pkl\")\n",
    "\n",
    "# combine all data chunks to one array\n",
    "combined_features_raw = np.vstack(combined_features_raw)\n",
    "output_raw = np.vstack(output_raw)\n",
    "\n",
    "# round all values to two decimal places\n",
    "combined_features_raw = np.round(combined_features_raw, decimals=4)\n",
    "output_raw = np.round(output_raw, decimals=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Scale feature vector and define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Erstelle eine Maske mit 10.000 zufälligen Indizes\n",
    "random_indices = np.random.choice(combined_features_raw.shape[0], 10000, replace=False)\n",
    "\n",
    "combined_features_subset = combined_features_raw.copy()\n",
    "output_subset = output_raw.copy()\n",
    "\n",
    "# Extrahiere die entsprechenden Werte aus den Arrays\n",
    "combined_features_subset = combined_features_subset[random_indices]\n",
    "output_subset = output_subset[random_indices]\n",
    "\n",
    "choice = 0\n",
    "if choice == 0:\n",
    "    combined_features = combined_features_raw.copy()\n",
    "    output = output_raw.copy()\n",
    "elif choice == 1:\n",
    "    combined_features = combined_features_subset.copy()\n",
    "    output = output_subset.copy()\n",
    "\n",
    "\n",
    "# Separate Scaler für jedes Feature\n",
    "scaler_wind = StandardScaler()\n",
    "scaler_ages = StandardScaler()\n",
    "scaler_hub_heights = StandardScaler()\n",
    "\n",
    "# Skalieren der einzelnen Features\n",
    "combined_features[:, -1] = scaler_wind.fit_transform(combined_features[:, -1].reshape(-1, 1)).flatten() # scale wind speeds\n",
    "combined_features[:, -2] = scaler_ages.fit_transform(combined_features[:, -2].reshape(-1, 1)).flatten()  # scale ages\n",
    "combined_features[:, -3] = scaler_hub_heights.fit_transform(combined_features[:, -3].reshape(-1, 1)).flatten()  # scale hub heights\n",
    "\n",
    "# Speichere alle Scaler in einem Dictionary\n",
    "scalers = {\n",
    "    \"winds\": scaler_wind,\n",
    "    \"ages\": scaler_ages,\n",
    "    \"hub_heights\": scaler_hub_heights,\n",
    "}\n",
    "\n",
    "# Speichere das Dictionary mit Joblib\n",
    "joblib.dump(scalers, \"parameters/scalers.pkl\")\n",
    "\n",
    "# Dataset-Klasse für PyTorch\n",
    "class WindPowerDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.targets[index]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Erstellung der PyTorch-Datasets\n",
    "dataset = WindPowerDataset(combined_features, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3366)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)  # No activation in the output layer for regression\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training and Testing with reanalysis data</h1>\n",
    "The computing resources on the personal PC are\n",
    "\n",
    "• 4 physical CPU cores, with Hyper-Threading 4 additional virtual CPU cores --> 8 logical CPU cores\n",
    "\n",
    "• 1 integrated GPU, that can be used with import torch_directml, device = torch_directml.device(), but it is much slower than the CPUs (and doesn't support HuberLoss: this calculation must be outsourced to the CPU. And no float64, only float32 datatypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "Training Losses\n",
      "    Huber: 0.016687246595590552\n",
      "    MSE: 0.03337471886359991\n",
      "    MAE: 0.1301842309129482\n",
      "    RMSE: 0.03337471886359991\n",
      "Training Losses\n",
      "    Huber: 0.01599478508873228\n",
      "    MSE: 0.03198958444470639\n",
      "    MAE: 0.12471857018331882\n",
      "    RMSE: 0.17885632346860536\n",
      "Model parameters saved\n",
      "Input size saved\n",
      "Losses saved\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import HuberLoss, MSELoss, L1Loss\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "params = {\"batch_size\": 128,\n",
    "          \"lr\": 0.00010155,\n",
    "          \"number_epochs\": 10}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Model\n",
    "input_size = combined_features.shape[1]\n",
    "\n",
    "# use static instead of dynamic computational graphs\n",
    "model = torch.jit.script(MLP(input_size=input_size)).to(device)\n",
    "model.to(device)\n",
    "\n",
    "# Trainings-Konfiguration\n",
    "mae_criterion = L1Loss()\n",
    "mse_criterion = MSELoss()\n",
    "huber_criterion = HuberLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "\n",
    "# shuffling doesn't matter here, has already taken place during random_split\n",
    "train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "test_loader = DataLoader(val_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Training\n",
    "for epoch in range(params[\"number_epochs\"]):\n",
    "    print(f\"Epoch {epoch + 1}/{params['number_epochs']}\")\n",
    "    model.train()\n",
    "    train_loss_mae, train_loss_mse, train_loss_huber = 0, 0, 0\n",
    "\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        \n",
    "        # Calculate metrics for each criterion\n",
    "        loss_mae = mae_criterion(outputs, batch_y)\n",
    "        loss_mse = mse_criterion(outputs, batch_y)\n",
    "        loss_huber = huber_criterion(outputs, batch_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_huber.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate metrics for logging\n",
    "        train_loss_mae += loss_mae.item()\n",
    "        train_loss_mse += loss_mse.item()\n",
    "        train_loss_huber += loss_huber.item()\n",
    "\n",
    "    train_loss_mae /= len(train_loader)\n",
    "    train_loss_mse /= len(train_loader)\n",
    "    train_loss_huber /= len(train_loader)\n",
    "\n",
    "print(f\"Training metrics\")\n",
    "print(f\"    Huber: {train_loss_huber}\")\n",
    "print(f\"    MSE: {train_loss_mse}\")\n",
    "print(f\"    MAE: {train_loss_mae}\")\n",
    "print(f\"    RMSE: {train_loss_mse}\")\n",
    "\n",
    "# Testen\n",
    "model.eval()\n",
    "\n",
    "test_loss_mae, test_loss_mse, test_loss_huber = 0, 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        preds = model(batch_x)\n",
    "        \n",
    "        test_loss_mae += mae_criterion(preds, batch_y).item()\n",
    "        test_loss_mse += mse_criterion(preds, batch_y).item()\n",
    "        test_loss_huber += huber_criterion(preds, batch_y).item()\n",
    "\n",
    "test_loss_mae /= len(test_loader)\n",
    "test_loss_mse /= len(test_loader)\n",
    "test_loss_huber /= len(test_loader)\n",
    "\n",
    "print(f\"Training metrics\")\n",
    "print(f\"    Huber: {test_loss_huber}\")\n",
    "print(f\"    MSE: {test_loss_mse}\")\n",
    "print(f\"    MAE: {test_loss_mae}\")\n",
    "print(f\"    RMSE: {np.sqrt(test_loss_mse)}\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"parameters/model.pth\")\n",
    "print(f\"Model parameters saved\")\n",
    "\n",
    "torch.save(input_size, \"parameters/input_size.pkl\")\n",
    "print(f\"Input size saved\")\n",
    "\n",
    "\n",
    "metrics = {\n",
    "    \"Training\": {\n",
    "        \"Huber\": train_loss_huber,\n",
    "        \"MAE\": train_loss_mae,\n",
    "        \"MSE\": train_loss_mse,\n",
    "        \"RMSE\": np.sqrt(test_loss_mse)\n",
    "\n",
    "    },\n",
    "    \"Testing\": {\n",
    "        \"Huber\": test_loss_huber,\n",
    "        \"MAE\": test_loss_mae,\n",
    "        \"MSE\": test_loss_mse,\n",
    "        \"RMSE\": np.sqrt(test_loss_mse)\n",
    "\n",
    "    }\n",
    "}\n",
    "\n",
    "joblib.dump(metrics, f\"parameters/metrics.pkl\")\n",
    "print(f\"metrics saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training on all reanalysis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "Model parameters saved\n",
      "Input size saved\n",
      "Training Losses\n",
      "    Huber: 0.016676704020100366\n",
      "    MSE: 0.03335354720986296\n",
      "    MAE: 0.13082658460584123\n",
      "    RMSE: 0.03335354720986296\n",
      "Losses saved\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import HuberLoss, MSELoss, L1Loss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "params = {\"batch_size\": 128,\n",
    "          \"lr\": 0.00010155,\n",
    "          \"number_epochs\": 10}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# DataLoader\n",
    "data_loader = DataLoader(dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "\n",
    "# Model\n",
    "input_size = combined_features[0].shape[1]\n",
    "\n",
    "# use static instead of dynamic computational graphs\n",
    "model = torch.jit.script(MLP(input_size=input_size)).to(device)\n",
    "model.to(device)\n",
    "\n",
    "# Trainings-Konfiguration\n",
    "mae_criterion = L1Loss()\n",
    "mse_criterion = MSELoss()\n",
    "huber_criterion = HuberLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "\n",
    "# Training\n",
    "for epoch in range(params[\"number_epochs\"]):\n",
    "    print(f\"Epoch {epoch + 1}/{params['number_epochs']}\")\n",
    "    model.train()\n",
    "    train_loss_mae, train_loss_mse, train_loss_huber = 0, 0, 0\n",
    "\n",
    "    for batch_x, batch_y in data_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        \n",
    "        # Calculate metrics for each criterion\n",
    "        loss_mae = mae_criterion(outputs, batch_y)\n",
    "        loss_mse = mse_criterion(outputs, batch_y)\n",
    "        loss_huber = huber_criterion(outputs, batch_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_huber.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate metrics for logging\n",
    "        train_loss_mae += loss_mae.item()\n",
    "        train_loss_mse += loss_mse.item()\n",
    "        train_loss_huber += loss_huber.item()\n",
    "\n",
    "    train_loss_mae /= len(data_loader)\n",
    "    train_loss_mse /= len(data_loader)\n",
    "    train_loss_huber /= len(data_loader)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"parameters_deployment/model.pth\")\n",
    "print(f\"Model parameters saved\")\n",
    "\n",
    "torch.save(input_size, \"parameters_deployment/input_size.pkl\")\n",
    "print(f\"Input size saved\")\n",
    "\n",
    "print(f\"Training metrics\")\n",
    "print(f\"    Huber: {train_loss_huber}\")\n",
    "print(f\"    MSE: {train_loss_mse}\")\n",
    "print(f\"    MAE: {train_loss_mae}\")\n",
    "print(f\"    RMSE: {train_loss_mse}\")\n",
    "\n",
    "metrics = {\n",
    "    \"Huber\": train_loss_huber,\n",
    "    \"MAE\": train_loss_mae,\n",
    "    \"MSE\": train_loss_mse,\n",
    "    \"RMSE\": np.sqrt(train_loss_mse)\n",
    "}\n",
    "\n",
    "joblib.dump(metrics, f\"parameters_deployment/metrics.pkl\")\n",
    "print(f\"metrics saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Load reforecast data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lead time: 0\n",
      "Processing lead time: 102\n",
      "Processing lead time: 105\n",
      "Processing lead time: 108\n",
      "Processing lead time: 111\n",
      "Processing lead time: 114\n",
      "Processing lead time: 117\n",
      "Processing lead time: 12\n",
      "Processing lead time: 120\n",
      "Processing lead time: 123\n",
      "Processing lead time: 126\n",
      "Processing lead time: 129\n",
      "Processing lead time: 132\n",
      "Processing lead time: 135\n",
      "Processing lead time: 138\n",
      "Processing lead time: 141\n",
      "Processing lead time: 144\n",
      "Processing lead time: 15\n",
      "Processing lead time: 18\n",
      "Processing lead time: 21\n",
      "Processing lead time: 24\n",
      "Processing lead time: 27\n",
      "Processing lead time: 3\n",
      "Processing lead time: 30\n",
      "Processing lead time: 33\n",
      "Processing lead time: 36\n",
      "Processing lead time: 39\n",
      "Processing lead time: 42\n",
      "Processing lead time: 45\n",
      "Processing lead time: 48\n",
      "Processing lead time: 51\n",
      "Processing lead time: 54\n",
      "Processing lead time: 57\n",
      "Processing lead time: 6\n",
      "Processing lead time: 60\n",
      "Processing lead time: 63\n",
      "Processing lead time: 66\n",
      "Processing lead time: 69\n",
      "Processing lead time: 72\n",
      "Processing lead time: 75\n",
      "Processing lead time: 78\n",
      "Processing lead time: 81\n",
      "Processing lead time: 84\n",
      "Processing lead time: 87\n",
      "Processing lead time: 9\n",
      "Processing lead time: 90\n",
      "Processing lead time: 93\n",
      "Processing lead time: 96\n",
      "Processing lead time: 99\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "input_dir = r\"E:\\MA_data\\WPPs+production+reforecast\"\n",
    "dataset_lead_times = {}\n",
    "\n",
    "for file in os.listdir(input_dir):\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    if os.path.isfile(file_path):  # Ensure it's a file (not a folder)\n",
    "        lead_time = int(file.split(\"_\")[-1].replace(\".json\", \"\"))\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            forecast_data = json.load(file)\n",
    "\n",
    "    print(f\"Processing lead time: {lead_time}\")\n",
    "\n",
    "    all_turbine_types = []\n",
    "    all_hub_heights = []\n",
    "    all_capacities = []\n",
    "    all_commissioning_dates = []\n",
    "    all_production_data = []\n",
    "\n",
    "    for unique_key, wpp in forecast_data.items():\n",
    "        all_turbine_types.append(str(wpp[\"Turbine\"]))\n",
    "        all_hub_heights.append(wpp[\"Hub_height\"])\n",
    "        all_capacities.append(wpp[\"Capacity\"])\n",
    "        all_commissioning_dates.append(f\"{wpp['Commissioning_date']}/06\" if isinstance(wpp[\"Commissioning_date\"], str) and \"/\" not in wpp[\"Commissioning_date\"] else wpp[\"Commissioning_date\"])\n",
    "        all_production_data.append(wpp[\"Time Series\"])\n",
    "\n",
    "    # One-Hot-Encoding for turbine types\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    turbine_types_onehot = encoder.fit_transform(np.array(all_turbine_types).reshape(-1, 1))\n",
    "\n",
    "    # convert to datetime\n",
    "    standardised_dates = pd.to_datetime(all_commissioning_dates, format='%Y/%m')\n",
    "\n",
    "    # calculate age\n",
    "    ref_date = pd.Timestamp(\"2024-12-01\")\n",
    "    ages = ref_date.year * 12 + ref_date.month - (standardised_dates.year * 12 + standardised_dates.month)\n",
    "\n",
    "    # create combined features and output lists\n",
    "    combined_features_raw = []\n",
    "    output_raw = []\n",
    "    \n",
    "    # convert data in feature arrays\n",
    "    for idx, production_data in enumerate(all_production_data):\n",
    "        num_rows = len(production_data)\n",
    "\n",
    "        # Repetitions for common features\n",
    "        turbine_type_repeated = np.tile(turbine_types_onehot[idx], (num_rows, 1))\n",
    "        hub_height_repeated = np.full((num_rows, 1), float(all_hub_heights[idx]))\n",
    "        age_repeated = np.full((num_rows, 1), ages[idx])\n",
    "\n",
    "        # Extract production values and wind speeds\n",
    "        production_values = np.array([entry[1] for entry in production_data]).reshape(-1, 1) / all_capacities[idx]\n",
    "        wind_speeds = np.array([entry[2] for entry in production_data]).reshape(-1, 1)\n",
    "\n",
    "        # combine all features\n",
    "        combined_chunk = np.hstack((\n",
    "            turbine_type_repeated,\n",
    "            hub_height_repeated,\n",
    "            age_repeated,\n",
    "            wind_speeds\n",
    "        ))\n",
    "\n",
    "        # add the data\n",
    "        combined_features_raw.append(combined_chunk)\n",
    "        output_raw.append(production_values)\n",
    "\n",
    "    # combine all data chunks to one array\n",
    "    combined_features = np.vstack(combined_features_raw)\n",
    "    output = np.vstack(output_raw)\n",
    "\n",
    "    # Interpolate missing values (linear interpolation) in pandas\n",
    "    wind_speed_series = pd.Series(combined_features[:, -1])\n",
    "    wind_speed_series.interpolate(method='linear', inplace=True)\n",
    "    combined_features[:, -1] = wind_speed_series.to_numpy()\n",
    "\n",
    "    # round all values to two decimal places\n",
    "    combined_features = np.round(combined_features, decimals=4)\n",
    "    output = np.round(output, decimals=4)\n",
    "\n",
    "    # Skalieren der einzelnen Features\n",
    "    combined_features[:, -1] = scaler_wind.transform(combined_features[:, -1].reshape(-1, 1)).flatten() # scale wind speeds\n",
    "    combined_features[:, -2] = scaler_ages.transform(combined_features[:, -2].reshape(-1, 1)).flatten()  # scale ages\n",
    "    combined_features[:, -3] = scaler_hub_heights.transform(combined_features[:, -3].reshape(-1, 1)).flatten()  # scale hub heights\n",
    "    \n",
    "    # Convert to PyTorch Dataset\n",
    "    dataset_lead_times[f\"lead_time_{lead_time}\"] = WindPowerDataset(combined_features, output)\n",
    "\n",
    "combined_features = combined_features_raw\n",
    "output = output_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Testing of previous model on reforecast data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for lead time 0\n",
      "Testing for lead time 102\n",
      "Testing for lead time 105\n",
      "Testing for lead time 108\n",
      "Testing for lead time 111\n",
      "Testing for lead time 114\n",
      "Testing for lead time 117\n",
      "Testing for lead time 12\n",
      "Testing for lead time 120\n",
      "Testing for lead time 123\n",
      "Testing for lead time 126\n",
      "Testing for lead time 129\n",
      "Testing for lead time 132\n",
      "Testing for lead time 135\n",
      "Testing for lead time 138\n",
      "Testing for lead time 141\n",
      "Testing for lead time 144\n",
      "Testing for lead time 15\n",
      "Testing for lead time 18\n",
      "Testing for lead time 21\n",
      "Testing for lead time 24\n",
      "Testing for lead time 27\n",
      "Testing for lead time 3\n",
      "Testing for lead time 30\n",
      "Testing for lead time 33\n",
      "Testing for lead time 36\n",
      "Testing for lead time 39\n",
      "Testing for lead time 42\n",
      "Testing for lead time 45\n",
      "Testing for lead time 48\n",
      "Testing for lead time 51\n",
      "Testing for lead time 54\n",
      "Testing for lead time 57\n",
      "Testing for lead time 6\n",
      "Testing for lead time 60\n",
      "Testing for lead time 63\n",
      "Testing for lead time 66\n",
      "Testing for lead time 69\n",
      "Testing for lead time 72\n",
      "Testing for lead time 75\n",
      "Testing for lead time 78\n",
      "Testing for lead time 81\n",
      "Testing for lead time 84\n",
      "Testing for lead time 87\n",
      "Testing for lead time 9\n",
      "Testing for lead time 90\n",
      "Testing for lead time 93\n",
      "Testing for lead time 96\n",
      "Testing for lead time 99\n",
      "Losses saved\n"
     ]
    }
   ],
   "source": [
    "model_state_dict = torch.load(\"parameters_deployment/model.pth\", weights_only=True)\n",
    "model = MLP(input_size)\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.eval()\n",
    "\n",
    "# Trainings-Konfiguration\n",
    "mae_criterion = L1Loss()\n",
    "mse_criterion = MSELoss()\n",
    "huber_criterion = HuberLoss()\n",
    "\n",
    "params = {\"batch_size\": 128,\n",
    "        \"lr\": 0.00010155,\n",
    "        \"number_epochs\": 10}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "for key, dataset in dataset_lead_times.items():\n",
    "    lead_time = int(key.split(\"_\")[-1])\n",
    "    print(f\"Testing for lead time {lead_time}\")\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    test_loss_mae, test_loss_mse, test_loss_huber = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in data_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            preds = model(batch_x)\n",
    "            \n",
    "            test_loss_mae += mae_criterion(preds, batch_y).item()\n",
    "            test_loss_mse += mse_criterion(preds, batch_y).item()\n",
    "            test_loss_huber += huber_criterion(preds, batch_y).item()\n",
    "\n",
    "    test_loss_mae /= len(data_loader)\n",
    "    test_loss_mse /= len(data_loader)\n",
    "    test_loss_huber /= len(data_loader)\n",
    "\n",
    "    metrics[lead_time] = {\n",
    "        \"Huber\": test_loss_huber,\n",
    "        \"MAE\": test_loss_mae,\n",
    "        \"MSE\": test_loss_mse,\n",
    "        \"RMSE\": np.sqrt(test_loss_mse)\n",
    "    }\n",
    "\n",
    "joblib.dump(metrics, f\"metrics_reforecast/metrics.pkl\")\n",
    "print(f\"metrics saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>TensorBoard graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SummaryWriter\n",
    "\n",
    "dummy_input = torch.rand(1, input_size)  # Batch size of 1, with \"input_size\" features\n",
    "\n",
    "# Instantiate your model\n",
    "model = MLP(input_size=input_size)\n",
    "\n",
    "# Initialize the TensorBoard writer\n",
    "writer = SummaryWriter(\"runs/mlp_model\")\n",
    "\n",
    "# Add the model graph to TensorBoard\n",
    "writer.add_graph(model, dummy_input)\n",
    "\n",
    "# Close the writer\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webapp_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
