{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate spearman correlation coefficient matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Beispiel-Daten erstellen\n",
    "data = {\n",
    "    'wind_speed': [1, 2, 3, 4, 5],\n",
    "    'temperature': [5, 6, 7, 8, 7],\n",
    "    'pressure': [10, 9, 2, 4, 3],\n",
    "    'production': []\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Spearman-Korrelationsmatrix berechnen\n",
    "spearman_corr_matrix = df.corr(method='spearman')\n",
    "\n",
    "print(\"Spearman-Korrelationsmatrix:\")\n",
    "print(spearman_corr_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load historical weather data (wind speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "from netCDF4 import num2date\n",
    "\n",
    "wind_speed_file = \"data/weather_history/COSMO_REA6/WS_100m.2D.201501.nc4\"\n",
    "\n",
    "wind_speed = nc.Dataset(wind_speed_file)\n",
    "\n",
    "number_hours = 48\n",
    "\n",
    "lons = wind_speed['RLON'][:].filled(np.nan)\n",
    "lons = np.where(lons > 180, lons - 360, lons)\n",
    "lats = wind_speed['RLAT'][:].filled(np.nan)\n",
    "time_unit = wind_speed['time'].units\n",
    "times = num2date(wind_speed['time'][:number_hours], time_unit)\n",
    "times = np.array([np.datetime64(t.strftime('%Y-%m-%dT%H:%M:%S')) for t in times])\n",
    "wind_speeds = wind_speed['wind_speed'][:number_hours,:,:].filled(np.nan) # convert masked array to regular array by converting masked values to nan values\n",
    "\n",
    "np.save(\"data/weather_history/COSMO_REA6/lons.npy\", lons)\n",
    "np.save(\"data/weather_history/COSMO_REA6/lats.npy\", lats)\n",
    "np.save(\"data/weather_history/COSMO_REA6/times.npy\", times)\n",
    "np.save(\"data/weather_history/COSMO_REA6/wind_speeds.npy\", wind_speeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save WPPs in parquet file (The Wind Power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lade die Excel-Datei nur einmal, filtere die relevanten Daten und speichere sie als Parquet-Datei\n",
    "WPP_file = \"data/WPPs/Windfarms_Europe_20241123.xlsx\"\n",
    "df = pd.read_excel(WPP_file, sheet_name='Windfarms', na_values=[\"#ND\"])\n",
    "df = df.drop(0) # remove first row (after headlines)\n",
    "\n",
    "# Wähle nur die benötigten Spalten aus\n",
    "df = df[['ID', 'Name', '2nd name', 'Latitude', 'Longitude', 'Manufacturer', 'Turbine', 'Hub height', 'Number of turbines', 'Total power', 'Developer', 'Operator', 'Owner', 'Commissioning date', 'Status']]\n",
    "\n",
    "# Entferne Zeilen, bei denen Name, Total power, Latitude oder Longitude NaN ist\n",
    "df = df.dropna(subset=['Name', 'Total power', 'Latitude', 'Longitude'])\n",
    "\n",
    "# Behalte nur Zeilen, bei denen Status == \"Production\"\n",
    "df = df.loc[df['Status'] == 'Production']\n",
    "\n",
    "# Konvertiere Spalten explizit in ihre entsprechenden Datentypen\n",
    "df['ID'] = df['ID'].astype(int)\n",
    "df['Name'] = df['Name'].astype(str)\n",
    "df['2nd name'] = df['2nd name'].astype(str)\n",
    "df['Latitude'] = df['Latitude'].astype(float)\n",
    "df['Longitude'] = df['Longitude'].astype(float)\n",
    "df['Manufacturer'] = df['Manufacturer'].astype(str)\n",
    "df['Turbine'] = df['Turbine'].astype(str)\n",
    "df['Hub height'] = df['Hub height'].astype(float)\n",
    "df['Number of turbines'] = df['Number of turbines'].fillna(0).astype(int)\n",
    "df['Total power'] = df['Total power'].astype(float)\n",
    "df['Developer'] = df['Developer'].astype(str)\n",
    "df['Operator'] = df['Operator'].astype(str)\n",
    "df['Owner'] = df['Owner'].astype(str)\n",
    "df['Commissioning date'] = df['Commissioning date'].astype(str)\n",
    "df['Status'] = df['Status'].astype(str)\n",
    "\n",
    "print(f\"number of wind turbines: {len(df['ID'])})\")\n",
    "\n",
    "# Speichere die gefilterten Daten im Parquet-Format (deutlich schneller zu lesen und schreiben, als Excel-Dateien, und auch platzsparender)\n",
    "df.to_parquet(\"data/WPPs/The_Wind_Power.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save production history example as parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lade die Excel-Datei nur einmal, filtere die relevanten Daten und speichere sie als Parquet-Datei\n",
    "example_file = \"data/production_history/Example/example_time_series.xlsx\" \n",
    "df = pd.read_excel(example_file)\n",
    "\n",
    "# Speichere die gefilterten Daten im Parquet-Format (deutlich schneller zu lesen und schreiben, als Excel-Dateien, und auch platzsparender)\n",
    "df.to_parquet(\"data/production_history/Example/example_time_series.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wind speeds (COSMO-REA6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "\n",
    "fn = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\weather history\\WS_100m.2D.199501.nc4\" # January 1995\n",
    "ds = nc.Dataset(fn)\n",
    "\n",
    "print(ds)\n",
    "\n",
    "time = ds.variables['time'][:]\n",
    "lon = ds.variables['RLON'][:]\n",
    "lat = ds.variables['RLAT'][:]\n",
    "wind_speed = ds.variables['wind_speed'][:]\n",
    "\n",
    "for i in range(len(lon)):\n",
    "    for j in range(len(lon[0])):\n",
    "        lon[i,j] = lon[i,j] - 360 if lon[i,j] > 180 else lon[i,j]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "three different visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "\n",
    "# Erstelle eine Karte mit cartopy\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Trage die Windgeschwindigkeit auf der Karte ein\n",
    "plt.contourf(lon, lat, wind_speed[0,:,:], transform=ccrs.PlateCarree(), cmap='viridis')\n",
    "\n",
    "# Füge Küstenlinien hinzu\n",
    "ax.coastlines()\n",
    "\n",
    "# Zeige die Karte\n",
    "plt.colorbar(label=\"Windgeschwindigkeit (m/s)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# Erstelle eine Karte mit curvilinearen Daten\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Zeichne die Windgeschwindigkeit auf dem curvilinearen Gitter\n",
    "plt.pcolormesh(lon, lat, wind_speed[0,:,:], transform=ccrs.PlateCarree(), cmap='viridis')\n",
    "\n",
    "# Füge Küstenlinien hinzu\n",
    "ax.coastlines()\n",
    "\n",
    "# Zeige die Karte\n",
    "plt.colorbar(label=\"Windgeschwindigkeit (m/s)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Verwende eine Lambert-Projektion\n",
    "ax = plt.axes(projection=ccrs.LambertConformal())\n",
    "\n",
    "# Zeichne die Windgeschwindigkeit auf dem curvilinearen Gitter\n",
    "plt.contourf(lon, lat, wind_speed[0,:,:], transform=ccrs.PlateCarree(), cmap='coolwarm')\n",
    "\n",
    "# Küstenlinien und Raster hinzufügen\n",
    "ax.coastlines()\n",
    "ax.gridlines()\n",
    "\n",
    "plt.colorbar(label=\"Windgeschwindigkeit (m/s)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "power curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definiere die Power Curve (Windgeschwindigkeit und Leistung)\n",
    "wind_speeds = np.arange(0, 25.5, 0.5)  # Windgeschwindigkeiten\n",
    "power_output = [0]*7 + [35, 80, 155, 238, 350, 474, 630, 802, 1018, 1234, 1504, 1773, 2076, 2379, 2664, 2948, 3141, 3334, 3425, 3515, 3546, 3577, 3586, 3594, 3598, 3599] + [3600]*18  # Leistung\n",
    "max_cap = 3600\n",
    "power_output_norm = [x / max_cap for x in power_output]\n",
    "wind_speed_max = 25\n",
    "\n",
    "# Erstelle die Interpolationsfunktion\n",
    "interpolation_function = interp1d(wind_speeds, power_output_norm, kind='cubic', fill_value=\"extrapolate\")\n",
    "\n",
    "# Beispiel für Interpolation: Bestimme Werte für feinere Windgeschwindigkeiten\n",
    "fine_wind_speeds = np.linspace(0, 25, 100)  # Feinere Windgeschwindigkeiten\n",
    "interpolated_power_output = interpolation_function(fine_wind_speeds)\n",
    "\n",
    "# Plot der diskreten und interpolierten Power Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(wind_speeds, power_output_norm, 'o', label='Diskrete Werte')  # Diskrete Punkte\n",
    "plt.plot(fine_wind_speeds, interpolated_power_output, '-', label='Interpolierte Werte')  # Interpolierte Werte\n",
    "plt.xlabel('Windgeschwindigkeit (m/s)')\n",
    "plt.ylabel('Leistung (kW)')\n",
    "plt.title('Interpolierte Power Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Datei laden (relativer Pfad)\n",
    "file_path = \"./Global-Wind-Power-Tracker-June-2024.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name='Data')\n",
    "\n",
    "# Bereich für Europa definieren\n",
    "lat_min, lat_max = 35, 72\n",
    "lon_min, lon_max = -25, 45\n",
    "\n",
    "# Filtere die Daten für den geografischen Bereich in Europa\n",
    "df_filtered = df[(df['Latitude'] >= lat_min) & (df['Latitude'] <= lat_max) & \n",
    "                 (df['Longitude'] >= lon_min) & (df['Longitude'] <= lon_max)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WPPs capacity distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Verteilung der Kapazität berechnen\n",
    "capacity_distribution = df_filtered['Capacity (MW)'].value_counts()\n",
    "\n",
    "# Diagramm erstellen, nur bis zum Maximalwert\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df_filtered['Capacity (MW)'].dropna(), bins=1000, range=(0, 2000), edgecolor='black')\n",
    "plt.title('Verteilung der Windkraftanlagenkapazitäten in Europa')\n",
    "plt.xlabel('Kapazität (MW)')\n",
    "plt.ylabel('Anzahl')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualisation of WPPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyleaflet import Map, Marker, MarkerCluster\n",
    "from ipywidgets import Layout\n",
    "\n",
    "# Erstelle die Karte\n",
    "m = Map(center=[(lat_min + lat_max) / 2, (lon_min + lon_max) / 2],\n",
    "        zoom=5,\n",
    "        layout=Layout(width='100%', height='500px')\n",
    "       )\n",
    "\n",
    "# Erstelle Marker-Objekte für jede Windkraftanlage\n",
    "markers = [Marker(location=(row['Latitude'], row['Longitude'])) for _, row in df_filtered.iterrows()]\n",
    "\n",
    "# Erstelle einen Marker Cluster\n",
    "marker_cluster = MarkerCluster(markers=markers, disable_clustering_at_zoom=18)\n",
    "\n",
    "# Füge den Marker Cluster zur Karte hinzu\n",
    "m.add_layer(marker_cluster)\n",
    "\n",
    "# Zeige die Karte an\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data generation with power curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Masked array in ein reguläres Array umwandeln\n",
    "wind_speed_array = np.ma.filled(wind_speed[0,:,:], np.nan)\n",
    "\n",
    "# Verteilung der Kapazitäten aus 'Capacity (MW)'\n",
    "capacity_distribution = df_filtered['Capacity (MW)'].dropna().values\n",
    "\n",
    "# Häufigkeiten der Kapazitäten berechnen\n",
    "unique_capacities, counts = np.unique(capacity_distribution, return_counts=True)\n",
    "\n",
    "# Wahrscheinlichkeiten für jede Kapazität (relative Häufigkeit)\n",
    "probabilities = counts / counts.sum()\n",
    "\n",
    "# Initialisiere die np.array Strukturen für die Daten\n",
    "data = np.zeros((3, wind_speed_array.size))  # 3 Reihen für wind_speed, capacity, production\n",
    "\n",
    "# Skaliere die Produktion basierend auf der Verteilung mit gewichteter Auswahl\n",
    "for i in range(wind_speed_array.shape[0]):\n",
    "    for j in range(wind_speed_array.shape[1]):\n",
    "        wind_speed_select = wind_speed_array[i, j] # Verwende alle Windgeschwindigkeiten von Januar 1995\n",
    "        capacity = np.random.choice(unique_capacities, p=probabilities) # Wähle eine Kapazität basierend auf ihrer Wahrscheinlichkeit\n",
    "        production = interpolation_function(wind_speed_select) * capacity\n",
    "        production = production if wind_speed_select < wind_speed_max else 0\n",
    "                \n",
    "        # Fülle die Werte in das np.array\n",
    "        data[0, i*wind_speed_array.shape[0]+j] = wind_speed_select  # Windgeschwindigkeit\n",
    "        data[1, i*wind_speed_array.shape[0]+j] = capacity  # Kapazität\n",
    "        data[2, i*wind_speed_array.shape[0]+j] = production # Produktion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of historical production data: discard data of unwanted power plants, retain monthly files - old version, because too many nan values or missing rows detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Basisverzeichnisse\n",
    "input_dir = r\"E:\\MA_data\\raw production history ENTSO-E\"\n",
    "output_dir = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_old\"\n",
    "\n",
    "# Funktion zum Auffüllen fehlender Stunden und Zählen der fehlenden Werte\n",
    "def fill_missing_hours(data, start_time, end_time):\n",
    "    # Erstelle eine vollständige Zeitreihe für den Monat\n",
    "    full_time_range = pd.date_range(start=start_time, end=end_time, freq='h')\n",
    "\n",
    "    # Umwandeln der 'DateTime (UTC)'-Spalte in datetime-Objekte\n",
    "    data.loc[:, 'DateTime (UTC)'] = pd.to_datetime(data['DateTime (UTC)'])\n",
    "\n",
    "    # Setze den Index auf die DateTime-Spalte\n",
    "    data.set_index('DateTime (UTC)', inplace=True)\n",
    "    \n",
    "    # Reindexiere die Daten, um fehlende Stunden mit NaN aufzufüllen\n",
    "    data = data.reindex(full_time_range)\n",
    "\n",
    "    # Zähle die Anzahl der fehlenden Werte\n",
    "    missing_count = data['ActualGenerationOutput(MW)'].isna().sum()\n",
    "    \n",
    "    # Fülle fehlende Werte mit 0 (sowohl gerade hinzugefügte Zeilen mit nan Werten, als auch bereits bestehende Zeilen, in denen schon vorher keine Werte für die Produktion waren)\n",
    "    data['ActualGenerationOutput(MW)'] = data['ActualGenerationOutput(MW)'].fillna(0)\n",
    "    \n",
    "    # Setze den Index zurück\n",
    "    data.reset_index(inplace=True)\n",
    "    \n",
    "    return data, missing_count\n",
    "\n",
    "number_missing_values = []\n",
    "\n",
    "# Liste der Monate von 2015-01 bis 2024-10 generieren\n",
    "months = pd.date_range(start=\"2015-01\", end=\"2024-10\", freq=\"MS\").strftime(\"%Y_%m\").tolist()\n",
    "\n",
    "# For-Schleife für jede Datei\n",
    "for i, month in enumerate(months):\n",
    "\n",
    "    # Dateipfad erstellen\n",
    "    input_file = os.path.join(input_dir, f\"{month}_ActualGenerationOutputPerGenerationUnit_16.1.A_r2.1.csv\")\n",
    "    output_file = os.path.join(output_dir, f\"production_summary_{month}.xlsx\")\n",
    "\n",
    "    # Datei einlesen\n",
    "    print(f\"Bearbeite Datei: {input_file}\")\n",
    "    data = pd.read_csv(input_file, sep='\\t')\n",
    "\n",
    "    # Filtere nach GenerationUnitType == 'Wind Onshore' oder 'Wind Offshore'\n",
    "    filtered_data = data[(data['GenerationUnitType'] == 'Wind Onshore ') | (data['GenerationUnitType'] == 'Wind Offshore ')]\n",
    "\n",
    "    # Wichtige Spalten identifizieren\n",
    "    unique_windfarms = filtered_data[['GenerationUnitName', 'GenerationUnitCode', 'GenerationUnitType', 'GenerationUnitInstalledCapacity(MW)', 'AreaCode']].drop_duplicates() # 'AreaCode', 'AreaDisplayName', 'AreaTypeCode' and 'MapCode' of identical WPPs may differ --> don't use as criterion to identify unique windfarms\n",
    "    \n",
    "    # Auffüllen fehlender Stunden und Zählen der fehlenden Werte\n",
    "    start_time = data['DateTime (UTC)'].min()\n",
    "    end_time = data['DateTime (UTC)'].max()\n",
    "\n",
    "    number_missing_values.append([])\n",
    "\n",
    "    # Listen für die Produktion zu jeder Stunde hinzufügen\n",
    "    production_data = []\n",
    "    for _, row in unique_windfarms.iterrows():\n",
    "        windfarm_data = filtered_data[(filtered_data['GenerationUnitName'] == row['GenerationUnitName']) & (filtered_data['AreaCode'] == row['AreaCode'])]\n",
    "        windfarm_data, missing_count = fill_missing_hours(windfarm_data, start_time, end_time)\n",
    "        production_values = windfarm_data['ActualGenerationOutput(MW)'].tolist() # where production is nan, the WPP has consumed and not generator power\n",
    "        row_data = {\n",
    "            'GenerationUnitName': row['GenerationUnitName'],\n",
    "            'GenerationUnitCode': row['GenerationUnitCode'],\n",
    "            'GenerationUnitType': row['GenerationUnitType'],\n",
    "            'GenerationUnitInstalledCapacity(MW)': row['GenerationUnitInstalledCapacity(MW)'],\n",
    "            'Production (MW)': production_values\n",
    "        }\n",
    "        production_data.append(row_data)\n",
    "\n",
    "        # Zähle die fehlenden Werte für diesen Monat\n",
    "        number_missing_values[i].append(missing_count)\n",
    "\n",
    "    # DataFrame erstellen und in Excel speichern\n",
    "    output_df = pd.DataFrame(production_data)\n",
    "    output_df.to_excel(output_file, index=False)\n",
    "\n",
    "    print(\"Anzahl der fehlenden Werte je Windkraftwerk für diesen Monat:\", number_missing_values[i])\n",
    "    print(f\"Excel-Datei wurde erfolgreich erstellt: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mache die Liste flach, um alle Elemente in einer einzigen Liste zu sammeln\n",
    "number_missing_values_flat = [item for sublist in number_missing_values for item in sublist]\n",
    "\n",
    "# Berechne den Durchschnitt\n",
    "average = sum(number_missing_values_flat) / len(number_missing_values_flat)\n",
    "print(f\"Der Durchschnitt ist: {average}\")\n",
    "\n",
    "# Überschrift\n",
    "column_name = \"number of missing elements per wind power plant for all investigated months\"\n",
    "\n",
    "# DataFrame erstellen\n",
    "df = pd.DataFrame({column_name: [str(sublist) for sublist in number_missing_values]})\n",
    "\n",
    "# Datei speichern\n",
    "output_file = r\"data\\number_missing_values.xlsx\"  # Pfad und Dateiname anpassen\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Die Excel-Datei wurde erfolgreich gespeichert: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge all monthly production data files to one combined file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Verzeichnisse\n",
    "input_dir = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed\"\n",
    "output_file = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\production_summary_all.xlsx\"\n",
    "\n",
    "# Liste der Monate von 2015_01 bis 2024_10\n",
    "months = pd.date_range(start=\"2015-01\", end=\"2024-10\", freq=\"MS\").strftime(\"%Y_%m\").tolist()\n",
    "\n",
    "columns_old = ['GenerationUnitName', 'GenerationUnitCode', 'GenerationUnitType', 'GenerationUnitInstalledCapacity(MW)', 'AreaDisplayName', 'MapCode', 'AreaTypeCode']\n",
    "# Leeres DataFrame für das Endergebnis\n",
    "columns = columns_old + months\n",
    "final_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Einlesen der einzelnen Dateien\n",
    "for month in months:\n",
    "    input_file = os.path.join(input_dir, f\"production_summary_{month}.xlsx\")\n",
    "\n",
    "    # Überprüfen, ob die Datei existiert\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Datei nicht gefunden: {input_file}\")\n",
    "        continue\n",
    "\n",
    "    # Datei einlesen\n",
    "    print(f\"Verarbeite Datei: {input_file}\")\n",
    "    df = pd.read_excel(input_file)\n",
    "\n",
    "    # Sicherstellen, dass die Spalte 'Production (MW)' existiert\n",
    "    if 'Production (MW)' not in df.columns:\n",
    "        print(f\"Spalte 'Production (MW)' fehlt in {input_file}\")\n",
    "        continue\n",
    "\n",
    "    df.rename(columns={'Production (MW)': month}, inplace=True)\n",
    "\n",
    "    # Zusammenführen der Daten\n",
    "    if final_df.empty:\n",
    "        final_df = df\n",
    "    else:\n",
    "        # Zusammenführen: Gleiche Windkraftanlagen zusammenführen, neue hinzufügen\n",
    "        final_df = pd.merge(final_df, df, how='outer', on=columns_old)\n",
    "\n",
    "# Excel-Tabelle speichern\n",
    "final_df.to_excel(output_file, index=False)\n",
    "print(f\"Zusammengeführte Excel-Tabelle wurde erfolgreich gespeichert unter: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise filling rate of production data file (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Datei laden\n",
    "file_path = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed\\production_summary_all.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Anzahl der Windkraftanlagen pro AreaDisplayName\n",
    "counts = df['AreaDisplayName'].value_counts()\n",
    "\n",
    "# Berechnung der prozentualen Ausfüllquote pro AreaDisplayName\n",
    "percentages = {}\n",
    "for area in counts.index:\n",
    "    subset = df[df['AreaDisplayName'] == area]\n",
    "    total_cells = len(subset) * (len(subset.columns) - 6)  # Exkludiere nicht-produktive Spalten\n",
    "    filled_cells = subset.iloc[:, 6:].notna().sum().sum()  # Nur Produktionsdaten berücksichtigen\n",
    "    percentages[area] = (filled_cells / total_cells) * 100\n",
    "\n",
    "# Balkendiagramm erstellen\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(counts.index, counts.values, color='skyblue')\n",
    "ax.set_title('Anzahl der Windkraftanlagen mit Produktionsdaten pro Land')\n",
    "ax.set_xlabel('Land (AreaDisplayName)')\n",
    "ax.set_ylabel('Anzahl der Windkraftanlagen')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Prozentsätze als Text hinzufügen\n",
    "for bar, area in zip(bars, counts.index):\n",
    "    height = bar.get_height()\n",
    "    percentage = percentages[area]\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, height, f'{percentage:.1f} %', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise filling rate of production data file (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Datei laden\n",
    "file_path = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed\\production_summary_all.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Liste der Monatskolumnen\n",
    "month_columns = [col for col in df.columns if col.startswith(\"20\")]\n",
    "\n",
    "# Anzahl der Windkraftanlagen pro AreaDisplayName (Land)\n",
    "windfarm_count = df.groupby(\"AreaDisplayName\").size()\n",
    "\n",
    "# Durchschnittliche Ausfüllquote pro Land\n",
    "fill_rates = df[month_columns].notna().mean(axis=1)  # Berechne pro Windkraftanlage\n",
    "average_fill_rate_per_country = df.groupby(\"AreaDisplayName\")[month_columns].apply(\n",
    "    lambda x: x.notna().mean(axis=1).mean()\n",
    ")\n",
    "\n",
    "# Plot erstellen\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Linke y-Achse: Anzahl der Windkraftanlagen\n",
    "ax1.bar(\n",
    "    windfarm_count.index,\n",
    "    windfarm_count.values,\n",
    "    label=\"Anzahl der Windkraftanlagen\",\n",
    "    alpha=0.7\n",
    ")\n",
    "ax1.set_ylabel(\"Anzahl der Windkraftanlagen\", fontsize=12)\n",
    "ax1.set_xlabel(\"AreaDisplayName (Land)\", fontsize=12)\n",
    "ax1.tick_params(axis=\"x\", rotation=45)\n",
    "ax1.legend(loc=\"upper left\")\n",
    "\n",
    "# Rechte y-Achse: Durchschnittliche Ausfüllquote\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(\n",
    "    average_fill_rate_per_country.index,\n",
    "    average_fill_rate_per_country.values * 100,  # Prozentualer Wert\n",
    "    label=\"Durchschnittliche Ausfüllquote (%)\",\n",
    "    alpha=0.5,\n",
    "    color=\"orange\"\n",
    ")\n",
    "ax2.set_ylabel(\"Durchschnittliche Ausfüllquote (%)\", fontsize=12)\n",
    "ax2.legend(loc=\"upper right\")\n",
    "\n",
    "# Titel und Layout\n",
    "plt.title(\"Windkraftanlagen und Ausfüllquote pro Land\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot anzeigen\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download table to find common name for UK power plants from website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL der Webseite\n",
    "url = \"https://osuked.github.io/Power-Station-Dictionary/dictionary.html\"\n",
    "\n",
    "# Abrufen der Webseite\n",
    "response = requests.get(url)\n",
    "if response.status_code != 200:\n",
    "    print(\"Fehler beim Abrufen der Webseite.\")\n",
    "    exit()\n",
    "\n",
    "# Parsing der Webseite mit BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Finden aller Tabellen auf der Seite\n",
    "tables = soup.find_all('table')\n",
    "if len(tables) < 3:\n",
    "    print(\"Weniger als 3 Tabellen auf der Seite gefunden.\")\n",
    "    exit()\n",
    "\n",
    "# Die dritte Tabelle auswählen (Index 2, da Python nullbasiert zählt)\n",
    "table = tables[2]\n",
    "\n",
    "# Spaltennamen extrahieren\n",
    "headers = [header.text.strip() for header in table.find_all('th')]\n",
    "\n",
    "# Zeilen extrahieren\n",
    "rows = []\n",
    "for row in table.find_all('tr')[1:]:  # Überspringt die Kopfzeile\n",
    "    cells = [cell.text.strip() for cell in row.find_all('td')]\n",
    "    rows.append(cells)\n",
    "\n",
    "# DataFrame erstellen\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "# Datei speichern\n",
    "output_path = \"data/WPPs/Power_Station_Dictionary.xlsx\"\n",
    "df.to_excel(output_path, index=False)\n",
    "print(f\"Die dritte Tabelle wurde erfolgreich als '{output_path}' gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add common names from downloaded to assignment file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two provided files\n",
    "file_1_path = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\WPPs\\Power_Station_Dictionary.xlsx\"\n",
    "file_2_path = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\production_summary_all.xlsx\"\n",
    "\n",
    "# read the data\n",
    "df1 = pd.read_excel(file_1_path)\n",
    "df2 = pd.read_excel(file_2_path)\n",
    "\n",
    "# introduce new columns at the end\n",
    "df2['Names_UK_Plants'] = None\n",
    "df2['ID_The-Wind-Power'] = None\n",
    "df2['Comment'] = None\n",
    "\n",
    "# move columns behind the column 'AreaTypeCode'\n",
    "area_type_code_index = df2.columns.get_loc('AreaTypeCode') + 1\n",
    "columns = list(df2.columns)\n",
    "columns.remove('Names_UK_Plants')\n",
    "columns.remove('ID_The-Wind-Power')\n",
    "columns.remove('Comment')\n",
    "columns = (\n",
    "    columns[:area_type_code_index] +\n",
    "    ['Names_UK_Plants', 'ID_The-Wind-Power', 'Comment'] +\n",
    "    columns[area_type_code_index:]\n",
    ")\n",
    "df2 = df2[columns]\n",
    "\n",
    "# Iterate over rows in df2 to match and update the \"Names_UK_Plants\" column\n",
    "for index, row in df2.iterrows():\n",
    "    generation_unit_name = row['GenerationUnitName']\n",
    "    \n",
    "    # Check if this name appears in the \"National Grid BMU ID\" of the first file\n",
    "    matching_rows = df1[df1['National Grid BMU ID'].str.contains(generation_unit_name, na=False, case=False)]\n",
    "    \n",
    "    if not matching_rows.empty:\n",
    "        # Get the \"Common Name\" value(s) and update the \"Names_UK_Plants\" column in df2\n",
    "        common_names = matching_rows['Common Name'].tolist()\n",
    "        df2.at[index, 'Names_UK_Plants'] = ', '.join(common_names)\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "output_path = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\Assignment.xlsx\"\n",
    "df2.to_excel(output_path, index=False)\n",
    "\n",
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load WPPs and assignment file (after manual assignment has been conducted) and combine the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Laden der Daten\n",
    "df_wind_power = pd.read_parquet(\"data/WPPs/The_Wind_Power.parquet\")\n",
    "df_assignment = pd.read_excel(\"data/Assignment.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtere nur Zeilen, bei denen \"ID_The-Wind-Power\" nicht \"not found\" ist\n",
    "df_assignment = df_assignment[df_assignment[\"ID_The-Wind-Power\"] != \"not found\"]\n",
    "\n",
    "# Extrahiere und entpacke alle gültigen IDs aus der Spalte \"ID_The-Wind-Power\"\n",
    "def extract_ids(value):\n",
    "    # Überprüfen, ob der Wert eine Liste ist, und ggf. in einzelne IDs zerlegen\n",
    "    if isinstance(value, str) and value.startswith(\"[\") and value.endswith(\"]\"):\n",
    "        return eval(value)  # Konvertiert die Zeichenkette in eine Liste\n",
    "    elif isinstance(value, (int, str)):\n",
    "        return [int(value)]  # Einzelne IDs werden in eine Liste gewandelt\n",
    "    return []\n",
    "\n",
    "valid_ids = set()\n",
    "df_assignment[\"ID_The-Wind-Power\"].apply(lambda x: valid_ids.update(extract_ids(x)))\n",
    "\n",
    "df_filtered = df_wind_power[df_wind_power['ID'].isin(valid_ids)].copy()\n",
    "actual_ids = set(df_filtered['ID'])\n",
    "suspended_ids = valid_ids - actual_ids\n",
    "\n",
    "print(\"number potential WPPs:\", len(valid_ids))\n",
    "print(\"number actual WPPs:\", len(actual_ids))\n",
    "print(\"number suspended WPPs (no name, location, capacity or status not in operation):\", len(suspended_ids))\n",
    "\n",
    "# Füge Spalten für Produktion von 2015_01 bis 2024_10 hinzu\n",
    "new_columns = {f\"{year}_{month:02d}\": [[] for _ in range(len(df_filtered))]\n",
    "            for year in range(2015, 2025) for month in range(1, 13)\n",
    "            if f\"{year}_{month:02d}\" in df_assignment.columns}\n",
    "\n",
    "# Füge die neuen Spalten zum DataFrame hinzu\n",
    "df_filtered = pd.concat([df_filtered, pd.DataFrame(new_columns, index=df_filtered.index)], axis=1)\n",
    "\n",
    "# Gehe durch jede Zeile der Assignment-Datei und füge Produktionsdaten hinzu\n",
    "for _, row in df_assignment.iterrows():\n",
    "    \n",
    "    ids_in_row = extract_ids(row[\"ID_The-Wind-Power\"])\n",
    "    first_id = ids_in_row[0]\n",
    "\n",
    "    if first_id in suspended_ids:\n",
    "        continue # jump to next iteration, because following line would fail for suspended_ids\n",
    "\n",
    "    current_index = df_filtered.loc[df_filtered['ID'] == first_id].index[0]\n",
    "\n",
    "    # add production values for each month, only requires the first ID\n",
    "    for year in range(2015, 2025):\n",
    "        for month in range(1, 13):\n",
    "            column_name = f\"{year}_{month:02d}\"\n",
    "            if column_name in df_assignment.columns: # neglect 2024_11 and 2024_12\n",
    "                production_month = row[column_name]\n",
    "                if isinstance(production_month, str): # type(value) = str, meaning value = production values\n",
    "                    production_month = production_month.replace(\"nan\", \"0\") # where production is nan, the WPP has consumed and not generator power\n",
    "                    production_month = eval(production_month) # [[]] <-- \"[[]]\"\n",
    "                    production_month = production_month[0] # [] <-- [[]]\n",
    "                    existing_production = df_filtered.at[current_index, column_name]\n",
    "                    \n",
    "                    if existing_production == []: # no production values in cell for this month\n",
    "                        df_filtered.at[current_index, column_name] = production_month\n",
    "                    else: # several production values to be added to one WPP\n",
    "                        combined_production = [a + b for a, b in zip(existing_production, production_month)]\n",
    "                        df_filtered.at[current_index, column_name] = combined_production\n",
    "\n",
    "    # add capacities of WPPs, if several are assigned to one row in the assignment table\n",
    "    if first_id in actual_ids and len(ids_in_row) > 1: # only treat every id once here, because rows are discarded\n",
    "        total_power = 0\n",
    "        for id in ids_in_row:\n",
    "            total_power += df_filtered.loc[df_filtered['ID'] == id, \"Total power\"].item() # add power\n",
    "            if id != first_id:\n",
    "                df_filtered = df_filtered[df_filtered['ID'] != id] # delete from dataframe\n",
    "        df_filtered.loc[df_filtered['ID'] == first_id, \"Total power\"] = total_power\n",
    "    \n",
    "    # keep track of treated IDs to not try to delete rows twice \n",
    "    for id in ids_in_row:\n",
    "        if id in actual_ids:\n",
    "            actual_ids.discard(id)\n",
    "\n",
    "actual_cluster_ids = set(df_filtered['ID'])\n",
    "print(\"number WPPs after clustering\", len(actual_cluster_ids))\n",
    "df_filtered.to_excel(\"data/WPPs+production.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assign wind speeds to WPPs and productions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Name of wind_speed_file must remain correct during execution of the next cell, because data are lazy loaded. Segmentation of files into years necessary, because datapoints at large indices of too large files can't be loaded into memory during lazy loading\n",
    "wind_speed_file = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\weather_history\\2024.grib\"\n",
    "# Chunkgröße für die Dimension \"time\" anpassen, sonst funktioniert die Berechnung wind_speeds = np.sqrt(wind_speeds_month['u100']**2 + wind_speeds_month['v100']**2) in der folgenden Zelle nicht\n",
    "wind_speed = xr.open_dataset(wind_speed_file, engine=\"cfgrib\", chunks={\"time\": 100})\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "times = pd.to_datetime(wind_speed['time'].values)\n",
    "latitudes = wind_speed['latitude'].values\n",
    "longitudes = wind_speed['longitude'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp2d\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "WPP_production = pd.read_excel(\"data/WPPs+production.xlsx\")\n",
    "df_assignment = pd.read_excel(\"data/Assignment.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp2d\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "WPP_production = pd.read_excel(\"data/WPPs+production.xlsx\")\n",
    "\n",
    "# all WPPs\n",
    "# ids = WPP_production['ID'].values\n",
    "# lats_plants = WPP_production['Latitude'].values\n",
    "# lons_plants = WPP_production['Longitude'].values\n",
    "\n",
    "# only two WPPs for test reasons\n",
    "ids = WPP_production['ID'].iloc[0:2]\n",
    "lats_plants = WPP_production['Latitude'].iloc[0:2]\n",
    "lons_plants = WPP_production['Longitude'].iloc[0:2]\n",
    "\n",
    "months = [f\"{year}_{month:02d}\" for year in range(2024, 2025) for month in range(10, 11) # range(2015, 2025) for month in range(1, 13)\n",
    "            if f\"{year}_{month:02d}\" in WPP_production.columns]\n",
    "\n",
    "for month in months:\n",
    "    print(f\"month {month}\")\n",
    "\n",
    "    month_data = times[times.strftime('%Y_%m') == month]\n",
    "    start = times.get_loc(month_data[0])\n",
    "    end = times.get_loc(month_data[-1])\n",
    "    wind_speeds_month = wind_speed.isel(time=slice(start, end+1))\n",
    "    # this operation requires chunking\n",
    "    wind_speeds = xr.apply_ufunc(\n",
    "        np.sqrt,\n",
    "        wind_speed['u100']**2 + wind_speed['v100']**2,\n",
    "        dask=\"parallelized\")\n",
    "    wind_speeds = wind_speeds.load()\n",
    "\n",
    "    for j in range(len(ids)):\n",
    "        print(f\"Wind Power Plant {j+1} / {len(ids)}\")\n",
    "        lon = lons_plants[j]\n",
    "        lat = lats_plants[j]\n",
    "        if WPP_production.at[j, month] != \"[]\":  # Check if there is production data\n",
    "            interpolated_wind_speeds = np.zeros(len(month_data))\n",
    "            for i, _ in enumerate(month_data):\n",
    "                wind_speeds_i = wind_speeds[i].values\n",
    "                spatial_interpolator = interp2d(longitudes, latitudes, wind_speeds_i, kind='cubic') # time in first dimension, i. e. wind_speeds[index] = wind_speeds[index, :, :]. Lazy evaluation of spatial_interpolator justifies creation of a new one not only for each time step, but also for each wind power plant\n",
    "                interpolated_value = spatial_interpolator(lon, lat)[0]\n",
    "                interpolated_value = round(interpolated_value, 3) # saves memory and computing resources\n",
    "                interpolated_wind_speeds[i] = interpolated_value\n",
    "\n",
    "            # Daten als NumPy-Array speichern (weniger speicherintensiv und stellt sicher, dass wind_speed und wind_power die gleiche Länge haben, die von wind_speed vorgegeben wird)\n",
    "            production_data = np.array(eval(WPP_production.at[j, month]))  # Vorherige Daten als Array\n",
    "            combined_data = np.stack((production_data, interpolated_wind_speeds), axis=0)\n",
    "            WPP_production.at[j, month] = combined_data  # Kombinierte Daten speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to save\n",
    "columns_to_save = [\n",
    "    'ID', 'Name', '2nd name', 'Latitude', 'Longitude', 'Manufacturer', 'Turbine',\n",
    "    'Hub height', 'Number of turbines', 'Total power', 'Developer', 'Operator',\n",
    "    'Owner', 'Commissioning date', 'Status', '2024_09', '2024_10'\n",
    "]\n",
    "\n",
    "# Filter the DataFrame to include only the specified columns and rows where ids correspond to WPP_production['ID']\n",
    "filtered_WPP_production = WPP_production[WPP_production['ID'].isin(ids)][columns_to_save]\n",
    "\n",
    "# Save the filtered DataFrame to an Excel file\n",
    "filtered_WPP_production.to_excel(\"data/WPPs+production+weather.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.6",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
