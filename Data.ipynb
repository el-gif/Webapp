{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate spearman correlation coefficient matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Beispiel-Daten erstellen\n",
    "data = {\n",
    "    'wind_speed': [1, 2, 3, 4, 5],\n",
    "    'temperature': [5, 6, 7, 8, 7],\n",
    "    'pressure': [10, 9, 2, 4, 3],\n",
    "    'production': []\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Spearman-Korrelationsmatrix berechnen\n",
    "spearman_corr_matrix = df.corr(method='spearman')\n",
    "\n",
    "print(\"Spearman-Korrelationsmatrix:\")\n",
    "print(spearman_corr_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load historical weather data (wind speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "wind_speed_file_1 = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\weather_history\\ERA\\data1.grib\"\n",
    "wind_speed_1 = xr.open_dataset(wind_speed_file_1, engine=\"cfgrib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_1 = wind_speed_1['time'].values\n",
    "latitudes = wind_speed_1['latitude'].values\n",
    "longitudes = wind_speed_1['longitude'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"data/WPPs/The_Wind_Power.parquet\")\n",
    "df['ID'] = list(range(1, len(df) + 1))\n",
    "ids = df['ID'].values\n",
    "lats_plants = df['Latitude'].values\n",
    "lons_plants = df['Longitude'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp2d\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Schleife über Zeitabschnitte\n",
    "chunk_size = 100  # Anzahl der Zeitschritte pro Chunk\n",
    "num_chunks = len(wind_speed_1['time']) // chunk_size + 1 # floor division + 1 = ceiling division\n",
    "\n",
    "wind_speeds_at_points = []\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    print(f\"chunk number {i}\")\n",
    "    for j in range(chunk_size):\n",
    "        start = i*chunk_size\n",
    "        index = start + j\n",
    "        print(f\"time step {i*chunk_size + index} / 43.800\")\n",
    "        end = min((i+1)*chunk_size, len(wind_speed_1['time']))\n",
    "\n",
    "        wind_speed_chunk = wind_speed_1.isel(time=slice(start, end))\n",
    "        wind_speeds_1 = np.sqrt(wind_speed_chunk['u100']**2 + wind_speed_chunk['v100']**2) # this operation requires chunking\n",
    "\n",
    "        spatial_interpolator = interp2d(longitudes, latitudes, wind_speeds_1[index], kind='cubic')\n",
    "        wind_speeds_at_points.append(np.array([spatial_interpolator(lon, lat)[0] for lon, lat in zip(lons_plants, lats_plants)]))\n",
    "\n",
    "        del wind_speeds_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "wind_speed_file_2 = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\weather_history\\ERA\\data2.grib\"\n",
    "wind_speed_2 = xr.open_dataset(wind_speed_file_2, engine=\"cfgrib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "times_2 = wind_speed_2['time'].values\n",
    "# Verarbeite die Geschwindigkeit in Chunks\n",
    "wind_speeds_2 = xr.apply_ufunc(\n",
    "    np.sqrt,\n",
    "    wind_speed_2['u100']**2 + wind_speed_2['v100']**2,\n",
    "    dask=\"parallelized\",\n",
    ")\n",
    "\n",
    "del wind_speed_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp2d\n",
    "\n",
    "for id in ids:\n",
    "    col = []\n",
    "    for time in times_1:\n",
    "        spatial_interpolator = interp2d(longitudes, latitudes, wind_speeds_1[time], kind='cubic')\n",
    "        wind_speeds_at_points.append(np.array([spatial_interpolator(lon, lat)[0] for lon, lat in zip(lons_plants, lats_plants)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load historical weather data (wind speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "from netCDF4 import num2date\n",
    "\n",
    "wind_speed_file = \"data/weather_history/COSMO_REA6/WS_100m.2D.201501.nc4\"\n",
    "\n",
    "wind_speed = nc.Dataset(wind_speed_file)\n",
    "\n",
    "number_hours = 48\n",
    "\n",
    "lons = wind_speed['RLON'][:].filled(np.nan)\n",
    "lons = np.where(lons > 180, lons - 360, lons)\n",
    "lats = wind_speed['RLAT'][:].filled(np.nan)\n",
    "time_unit = wind_speed['time'].units\n",
    "times = num2date(wind_speed['time'][:number_hours], time_unit)\n",
    "times = np.array([np.datetime64(t.strftime('%Y-%m-%dT%H:%M:%S')) for t in times])\n",
    "wind_speeds = wind_speed['wind_speed'][:number_hours,:,:].filled(np.nan) # convert masked array to regular array by converting masked values to nan values\n",
    "\n",
    "np.save(\"data/weather_history/COSMO_REA6/lons.npy\", lons)\n",
    "np.save(\"data/weather_history/COSMO_REA6/lats.npy\", lats)\n",
    "np.save(\"data/weather_history/COSMO_REA6/times.npy\", times)\n",
    "np.save(\"data/weather_history/COSMO_REA6/wind_speeds.npy\", wind_speeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save WPPs in parquet file (The Wind Power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23527\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lade die Excel-Datei nur einmal, filtere die relevanten Daten und speichere sie als Parquet-Datei\n",
    "WPP_file = \"data/WPPs/Windfarms_Europe_20241123.xlsx\"\n",
    "df = pd.read_excel(WPP_file, sheet_name='Windfarms', na_values=[\"#ND\"])\n",
    "df = df.drop(0) # remove first row (after headlines)\n",
    "\n",
    "# Wähle nur die benötigten Spalten aus\n",
    "df = df[['ID', 'Name', '2nd name', 'Latitude', 'Longitude', 'Manufacturer', 'Turbine', 'Hub height', 'Number of turbines', 'Total power', 'Developer', 'Operator', 'Owner', 'Commissioning date', 'Status']]\n",
    "\n",
    "# Entferne Zeilen, bei denen Name, Total power, Latitude oder Longitude NaN ist\n",
    "df = df.dropna(subset=['Name', 'Total power', 'Latitude', 'Longitude'])\n",
    "\n",
    "# Behalte nur Zeilen, bei denen Status == \"Production\"\n",
    "df = df.loc[df['Status'] == 'Production']\n",
    "\n",
    "# Konvertiere Spalten explizit in ihre entsprechenden Datentypen\n",
    "df['ID'] = df['ID'].astype(int)\n",
    "df['Name'] = df['Name'].astype(str)\n",
    "df['2nd name'] = df['2nd name'].astype(str)\n",
    "df['Latitude'] = df['Latitude'].astype(float)\n",
    "df['Longitude'] = df['Longitude'].astype(float)\n",
    "df['Manufacturer'] = df['Manufacturer'].astype(str)\n",
    "df['Turbine'] = df['Turbine'].astype(str)\n",
    "df['Hub height'] = df['Hub height'].astype(float)\n",
    "df['Number of turbines'] = df['Number of turbines'].fillna(0).astype(int)\n",
    "df['Total power'] = df['Total power'].astype(float)\n",
    "df['Developer'] = df['Developer'].astype(str)\n",
    "df['Operator'] = df['Operator'].astype(str)\n",
    "df['Owner'] = df['Owner'].astype(str)\n",
    "df['Commissioning date'] = df['Commissioning date'].astype(str)\n",
    "df['Status'] = df['Status'].astype(str)\n",
    "\n",
    "print(f\"number of wind turbines: {len(df['ID'])})\")\n",
    "\n",
    "# Speichere die gefilterten Daten im Parquet-Format (deutlich schneller zu lesen und schreiben, als Excel-Dateien, und auch platzsparender)\n",
    "df.to_parquet(\"data/WPPs/The_Wind_Power.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save production history example as parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lade die Excel-Datei nur einmal, filtere die relevanten Daten und speichere sie als Parquet-Datei\n",
    "example_file = \"data/production_history/Example/example_time_series.xlsx\" \n",
    "df = pd.read_excel(example_file)\n",
    "\n",
    "# Speichere die gefilterten Daten im Parquet-Format (deutlich schneller zu lesen und schreiben, als Excel-Dateien, und auch platzsparender)\n",
    "df.to_parquet(\"data/production_history/Example/example_time_series.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wind speeds (COSMO-REA6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "\n",
    "fn = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\weather history\\WS_100m.2D.199501.nc4\" # January 1995\n",
    "ds = nc.Dataset(fn)\n",
    "\n",
    "print(ds)\n",
    "\n",
    "time = ds.variables['time'][:]\n",
    "lon = ds.variables['RLON'][:]\n",
    "lat = ds.variables['RLAT'][:]\n",
    "wind_speed = ds.variables['wind_speed'][:]\n",
    "\n",
    "for i in range(len(lon)):\n",
    "    for j in range(len(lon[0])):\n",
    "        lon[i,j] = lon[i,j] - 360 if lon[i,j] > 180 else lon[i,j]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "three different visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "\n",
    "# Erstelle eine Karte mit cartopy\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Trage die Windgeschwindigkeit auf der Karte ein\n",
    "plt.contourf(lon, lat, wind_speed[0,:,:], transform=ccrs.PlateCarree(), cmap='viridis')\n",
    "\n",
    "# Füge Küstenlinien hinzu\n",
    "ax.coastlines()\n",
    "\n",
    "# Zeige die Karte\n",
    "plt.colorbar(label=\"Windgeschwindigkeit (m/s)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# Erstelle eine Karte mit curvilinearen Daten\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Zeichne die Windgeschwindigkeit auf dem curvilinearen Gitter\n",
    "plt.pcolormesh(lon, lat, wind_speed[0,:,:], transform=ccrs.PlateCarree(), cmap='viridis')\n",
    "\n",
    "# Füge Küstenlinien hinzu\n",
    "ax.coastlines()\n",
    "\n",
    "# Zeige die Karte\n",
    "plt.colorbar(label=\"Windgeschwindigkeit (m/s)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Verwende eine Lambert-Projektion\n",
    "ax = plt.axes(projection=ccrs.LambertConformal())\n",
    "\n",
    "# Zeichne die Windgeschwindigkeit auf dem curvilinearen Gitter\n",
    "plt.contourf(lon, lat, wind_speed[0,:,:], transform=ccrs.PlateCarree(), cmap='coolwarm')\n",
    "\n",
    "# Küstenlinien und Raster hinzufügen\n",
    "ax.coastlines()\n",
    "ax.gridlines()\n",
    "\n",
    "plt.colorbar(label=\"Windgeschwindigkeit (m/s)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "power curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definiere die Power Curve (Windgeschwindigkeit und Leistung)\n",
    "wind_speeds = np.arange(0, 25.5, 0.5)  # Windgeschwindigkeiten\n",
    "power_output = [0]*7 + [35, 80, 155, 238, 350, 474, 630, 802, 1018, 1234, 1504, 1773, 2076, 2379, 2664, 2948, 3141, 3334, 3425, 3515, 3546, 3577, 3586, 3594, 3598, 3599] + [3600]*18  # Leistung\n",
    "max_cap = 3600\n",
    "power_output_norm = [x / max_cap for x in power_output]\n",
    "wind_speed_max = 25\n",
    "\n",
    "# Erstelle die Interpolationsfunktion\n",
    "interpolation_function = interp1d(wind_speeds, power_output_norm, kind='cubic', fill_value=\"extrapolate\")\n",
    "\n",
    "# Beispiel für Interpolation: Bestimme Werte für feinere Windgeschwindigkeiten\n",
    "fine_wind_speeds = np.linspace(0, 25, 100)  # Feinere Windgeschwindigkeiten\n",
    "interpolated_power_output = interpolation_function(fine_wind_speeds)\n",
    "\n",
    "# Plot der diskreten und interpolierten Power Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(wind_speeds, power_output_norm, 'o', label='Diskrete Werte')  # Diskrete Punkte\n",
    "plt.plot(fine_wind_speeds, interpolated_power_output, '-', label='Interpolierte Werte')  # Interpolierte Werte\n",
    "plt.xlabel('Windgeschwindigkeit (m/s)')\n",
    "plt.ylabel('Leistung (kW)')\n",
    "plt.title('Interpolierte Power Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Datei laden (relativer Pfad)\n",
    "file_path = \"./Global-Wind-Power-Tracker-June-2024.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name='Data')\n",
    "\n",
    "# Bereich für Europa definieren\n",
    "lat_min, lat_max = 35, 72\n",
    "lon_min, lon_max = -25, 45\n",
    "\n",
    "# Filtere die Daten für den geografischen Bereich in Europa\n",
    "df_filtered = df[(df['Latitude'] >= lat_min) & (df['Latitude'] <= lat_max) & \n",
    "                 (df['Longitude'] >= lon_min) & (df['Longitude'] <= lon_max)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WPPs capacity distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Verteilung der Kapazität berechnen\n",
    "capacity_distribution = df_filtered['Capacity (MW)'].value_counts()\n",
    "\n",
    "# Diagramm erstellen, nur bis zum Maximalwert\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df_filtered['Capacity (MW)'].dropna(), bins=1000, range=(0, 2000), edgecolor='black')\n",
    "plt.title('Verteilung der Windkraftanlagenkapazitäten in Europa')\n",
    "plt.xlabel('Kapazität (MW)')\n",
    "plt.ylabel('Anzahl')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualisation of WPPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyleaflet import Map, Marker, MarkerCluster\n",
    "from ipywidgets import Layout\n",
    "\n",
    "# Erstelle die Karte\n",
    "m = Map(center=[(lat_min + lat_max) / 2, (lon_min + lon_max) / 2],\n",
    "        zoom=5,\n",
    "        layout=Layout(width='100%', height='500px')\n",
    "       )\n",
    "\n",
    "# Erstelle Marker-Objekte für jede Windkraftanlage\n",
    "markers = [Marker(location=(row['Latitude'], row['Longitude'])) for _, row in df_filtered.iterrows()]\n",
    "\n",
    "# Erstelle einen Marker Cluster\n",
    "marker_cluster = MarkerCluster(markers=markers, disable_clustering_at_zoom=18)\n",
    "\n",
    "# Füge den Marker Cluster zur Karte hinzu\n",
    "m.add_layer(marker_cluster)\n",
    "\n",
    "# Zeige die Karte an\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data generation with power curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Masked array in ein reguläres Array umwandeln\n",
    "wind_speed_array = np.ma.filled(wind_speed[0,:,:], np.nan)\n",
    "\n",
    "# Verteilung der Kapazitäten aus 'Capacity (MW)'\n",
    "capacity_distribution = df_filtered['Capacity (MW)'].dropna().values\n",
    "\n",
    "# Häufigkeiten der Kapazitäten berechnen\n",
    "unique_capacities, counts = np.unique(capacity_distribution, return_counts=True)\n",
    "\n",
    "# Wahrscheinlichkeiten für jede Kapazität (relative Häufigkeit)\n",
    "probabilities = counts / counts.sum()\n",
    "\n",
    "# Initialisiere die np.array Strukturen für die Daten\n",
    "data = np.zeros((3, wind_speed_array.size))  # 3 Reihen für wind_speed, capacity, production\n",
    "\n",
    "# Skaliere die Produktion basierend auf der Verteilung mit gewichteter Auswahl\n",
    "for i in range(wind_speed_array.shape[0]):\n",
    "    for j in range(wind_speed_array.shape[1]):\n",
    "        wind_speed_select = wind_speed_array[i, j] # Verwende alle Windgeschwindigkeiten von Januar 1995\n",
    "        capacity = np.random.choice(unique_capacities, p=probabilities) # Wähle eine Kapazität basierend auf ihrer Wahrscheinlichkeit\n",
    "        production = interpolation_function(wind_speed_select) * capacity\n",
    "        production = production if wind_speed_select < wind_speed_max else 0\n",
    "                \n",
    "        # Fülle die Werte in das np.array\n",
    "        data[0, i*wind_speed_array.shape[0]+j] = wind_speed_select  # Windgeschwindigkeit\n",
    "        data[1, i*wind_speed_array.shape[0]+j] = capacity  # Kapazität\n",
    "        data[2, i*wind_speed_array.shape[0]+j] = production # Produktion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Verwende dein generiertes numpy Array mit Wind Speed, Capacity und Scaled Production\n",
    "# data[0, :] -> Windgeschwindigkeit\n",
    "# data[1, :] -> Kapazität\n",
    "# data[2, :] -> Skalierte Produktion\n",
    "\n",
    "X = data[:2, :].T  # Features: Erste zwei Reihen, transponiert zu (Anzahl der Datenpunkte, 2)\n",
    "y = data[2, :]  # Target: Dritte Reihe (scaled production)\n",
    "\n",
    "# Normalisierung der Features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)  # Normiere Windgeschwindigkeit und Kapazität auf Standardnormalverteilung\n",
    "\n",
    "# Train/Test Split (80% Training, 20% Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Konvertiere die Numpy Arrays in PyTorch Tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Output Tensor (2D, mit shape [N, 1])\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Erstellen des ANN-Modells\n",
    "class ANN_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANN_Model, self).__init__()\n",
    "        # Input Layer: 2 Features (Wind Speed und Capacity)\n",
    "        # Hidden Layer 1: 64 Neuronen\n",
    "        # Hidden Layer 2: 32 Neuronen\n",
    "        # Output Layer: 1 (Scaled Production)\n",
    "        self.fc1 = nn.Linear(2, 64)  # Eingabe: 2 Features\n",
    "        self.fc2 = nn.Linear(64, 32)  # Erste versteckte Schicht\n",
    "        self.fc3 = nn.Linear(32, 1)  # Ausgabe: 1 Wert (Scaled Production)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialisiere das Modell, den Optimizer und die Loss-Funktion\n",
    "model = ANN_Model()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error für Regression\n",
    "\n",
    "# Training des Modells\n",
    "epochs = 500\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Vorwärtsdurchlauf\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    \n",
    "    # Rückwärtsdurchlauf und Optimierung\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Speichern des Verlusts\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Plotte den Trainingsverlust\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.show()\n",
    "\n",
    "# Teste das Modell\n",
    "model.eval()\n",
    "with torch.no_grad(): # don't calculate gradient: model remains tel quel during testing\n",
    "    predictions = model(X_test_tensor)\n",
    "    test_loss = criterion(predictions, y_test_tensor)\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "# Forecast auf Basis hypothetischer Werte (z.B. hypothetische Wind Speed und Capacity)\n",
    "hypothetical_data = np.array([[12, 1500], [8, 3000], [20, 500]])  # Beispielwerte: [Wind Speed, Capacity]\n",
    "hypothetical_tensor = torch.tensor(hypothetical_data, dtype=torch.float32)\n",
    "\n",
    "# Vorhersagen machen\n",
    "with torch.no_grad(): # don't calculate gradient: model remains tel quel during forecasting\n",
    "    forecasted_production = model(hypothetical_tensor)\n",
    "    print(f'Hypothetische Vorhersagen (Wind Speed, Capacity -> Scaled Production): {forecasted_production}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Beispiel-Daten (Windgeschwindigkeit, Kapazität, Produktion)\n",
    "# Nehmen wir an, 'data' ist ein Vektor mit drei Listen:\n",
    "wind_speed = data[0]  # Erste Zeile: Windgeschwindigkeiten\n",
    "capacity_mw = data[1]  # Zweite Zeile: Kapazität (MW)\n",
    "scaled_production = data[2]  # Dritte Zeile: Produktion (Scaled Production)\n",
    "\n",
    "# Eingabedaten als Feature-Array\n",
    "X = np.column_stack((wind_speed, capacity_mw))\n",
    "\n",
    "# Zielwert (Scaled Production)\n",
    "y = scaled_production\n",
    "\n",
    "# Daten aufteilen in Trainings- und Testdatensatz (80% Training, 20% Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Daten normalisieren (wichtig für ein stabiles Training)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Erstelle das Modell (ANN)\n",
    "model = Sequential()\n",
    "\n",
    "# Eingabeschicht und erste verborgene Schicht (mit 64 Neuronen und ReLU-Aktivierungsfunktion)\n",
    "model.add(Dense(64, input_dim=2, activation='relu'))\n",
    "\n",
    "# Zweite verborgene Schicht\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Ausgabeschicht (1 Neuron für die Vorhersage der Produktion)\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Kompiliere das Modell mit Adam Optimizer und Mean Squared Error als Verlustfunktion\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Trainiere das Modell auf den Trainingsdaten\n",
    "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=1, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluierung des Modells auf den Testdaten\n",
    "loss = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f'Test Loss (Mean Squared Error): {loss}')\n",
    "\n",
    "# Vorhersage auf neuen hypothetischen Daten (z.B. Kapazität=4.5 MW, Windgeschwindigkeit=15 m/s)\n",
    "hypothetical_data = np.array([[15, 4.5]])\n",
    "hypothetical_data_scaled = scaler.transform(hypothetical_data)\n",
    "predicted_production = model.predict(hypothetical_data_scaled)\n",
    "\n",
    "print(f'Hypothetische Vorhersage der Produktion: {predicted_production[0][0]}')\n",
    "\n",
    "# Vorhersage auf den Testdaten (zum Vergleich)\n",
    "predictions = model.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of historical production data: discard data from unwanted power plants, retain monthly files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Basisverzeichnisse\n",
    "input_dir = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\raw\"\n",
    "output_dir = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed\"\n",
    "\n",
    "# Liste der Monate von 2015_06 bis 2024_10 generieren\n",
    "months = pd.date_range(start=\"2015-05\", end=\"2024-10\", freq=\"MS\").strftime(\"%Y_%m\").tolist()\n",
    "\n",
    "# For-Schleife für jede Datei\n",
    "for month in months:\n",
    "    # Dateipfad erstellen\n",
    "    input_file = os.path.join(input_dir, f\"{month}_ActualGenerationOutputPerGenerationUnit_16.1.A_r2.1.csv\")\n",
    "    output_file = os.path.join(output_dir, f\"production_summary_{month}.xlsx\")\n",
    "\n",
    "    # Überprüfen, ob die Datei existiert\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Datei nicht gefunden: {input_file}\")\n",
    "        continue  # Überspringt diese Iteration, wenn die Datei nicht existiert\n",
    "\n",
    "    # Datei einlesen\n",
    "    print(f\"Bearbeite Datei: {input_file}\")\n",
    "    data = pd.read_csv(input_file, sep='\\t')\n",
    "\n",
    "    # Filtere nach GenerationUnitType == 'Wind Onshore' oder 'Wind Offshore'\n",
    "    filtered_data = data[(data['GenerationUnitType'] == 'Wind Onshore ') | (data['GenerationUnitType'] == 'Wind Offshore ')]\n",
    "\n",
    "    # Wichtige Spalten identifizieren\n",
    "    unique_windfarms = filtered_data[['GenerationUnitName', 'GenerationUnitCode', 'GenerationUnitType', 'AreaDisplayName', 'MapCode', 'AreaTypeCode', 'GenerationUnitInstalledCapacity(MW)']].drop_duplicates()\n",
    "\n",
    "    # Listen für die Produktion zu jeder Stunde hinzufügen\n",
    "    production_data = []\n",
    "    for _, row in unique_windfarms.iterrows():\n",
    "        windfarm_data = filtered_data[filtered_data['GenerationUnitName'] == row['GenerationUnitName']]\n",
    "        production_values = windfarm_data['ActualGenerationOutput(MW)'].tolist()\n",
    "        row_data = {\n",
    "            'GenerationUnitName': row['GenerationUnitName'],\n",
    "            'GenerationUnitCode': row['GenerationUnitCode'],\n",
    "            'GenerationUnitType': row['GenerationUnitType'],\n",
    "            'GenerationUnitInstalledCapacity(MW)': row['GenerationUnitInstalledCapacity(MW)'],\n",
    "            'AreaDisplayName': row['AreaDisplayName'],\n",
    "            'MapCode': row['MapCode'],\n",
    "            'AreaTypeCode': row['AreaTypeCode'],\n",
    "            'Production (MW)': [production_values]  # Hier die Produktion als Liste speichern\n",
    "        }\n",
    "        production_data.append(row_data)\n",
    "\n",
    "    # DataFrame erstellen und in Excel speichern\n",
    "    output_df = pd.DataFrame(production_data)\n",
    "    output_df.to_excel(output_file, index=False)\n",
    "\n",
    "    print(f\"Excel-Datei wurde erfolgreich erstellt: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of historical production data: merge all monthly files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Verzeichnisse\n",
    "input_dir = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed\"\n",
    "output_file = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\production_summary_all.xlsx\"\n",
    "\n",
    "# Liste der Monate von 2015_01 bis 2024_10\n",
    "months = pd.date_range(start=\"2015-01\", end=\"2024-10\", freq=\"MS\").strftime(\"%Y_%m\").tolist()\n",
    "\n",
    "columns_old = ['GenerationUnitName', 'GenerationUnitCode', 'GenerationUnitType', 'GenerationUnitInstalledCapacity(MW)', 'AreaDisplayName', 'MapCode', 'AreaTypeCode']\n",
    "# Leeres DataFrame für das Endergebnis\n",
    "columns = columns_old + months\n",
    "final_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Einlesen der einzelnen Dateien\n",
    "for month in months:\n",
    "    input_file = os.path.join(input_dir, f\"production_summary_{month}.xlsx\")\n",
    "\n",
    "    # Überprüfen, ob die Datei existiert\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Datei nicht gefunden: {input_file}\")\n",
    "        continue\n",
    "\n",
    "    # Datei einlesen\n",
    "    print(f\"Verarbeite Datei: {input_file}\")\n",
    "    df = pd.read_excel(input_file)\n",
    "\n",
    "    # Sicherstellen, dass die Spalte 'Production (MW)' existiert\n",
    "    if 'Production (MW)' not in df.columns:\n",
    "        print(f\"Spalte 'Production (MW)' fehlt in {input_file}\")\n",
    "        continue\n",
    "\n",
    "    df.rename(columns={'Production (MW)': month}, inplace=True)\n",
    "\n",
    "    # Zusammenführen der Daten\n",
    "    if final_df.empty:\n",
    "        final_df = df\n",
    "    else:\n",
    "        # Zusammenführen: Gleiche Windkraftanlagen zusammenführen, neue hinzufügen\n",
    "        final_df = pd.merge(final_df, df, how='outer', on=columns_old)\n",
    "\n",
    "# Excel-Tabelle speichern\n",
    "final_df.to_excel(output_file, index=False)\n",
    "print(f\"Zusammengeführte Excel-Tabelle wurde erfolgreich gespeichert unter: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise filling rate of production data file (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Datei laden\n",
    "file_path = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed\\production_summary_all.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Anzahl der Windkraftanlagen pro AreaDisplayName\n",
    "counts = df['AreaDisplayName'].value_counts()\n",
    "\n",
    "# Berechnung der prozentualen Ausfüllquote pro AreaDisplayName\n",
    "percentages = {}\n",
    "for area in counts.index:\n",
    "    subset = df[df['AreaDisplayName'] == area]\n",
    "    total_cells = len(subset) * (len(subset.columns) - 6)  # Exkludiere nicht-produktive Spalten\n",
    "    filled_cells = subset.iloc[:, 6:].notna().sum().sum()  # Nur Produktionsdaten berücksichtigen\n",
    "    percentages[area] = (filled_cells / total_cells) * 100\n",
    "\n",
    "# Balkendiagramm erstellen\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(counts.index, counts.values, color='skyblue')\n",
    "ax.set_title('Anzahl der Windkraftanlagen mit Produktionsdaten pro Land')\n",
    "ax.set_xlabel('Land (AreaDisplayName)')\n",
    "ax.set_ylabel('Anzahl der Windkraftanlagen')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Prozentsätze als Text hinzufügen\n",
    "for bar, area in zip(bars, counts.index):\n",
    "    height = bar.get_height()\n",
    "    percentage = percentages[area]\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, height, f'{percentage:.1f} %', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise filling rate of production data file (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Datei laden\n",
    "file_path = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed\\production_summary_all.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Liste der Monatskolumnen\n",
    "month_columns = [col for col in df.columns if col.startswith(\"20\")]\n",
    "\n",
    "# Anzahl der Windkraftanlagen pro AreaDisplayName (Land)\n",
    "windfarm_count = df.groupby(\"AreaDisplayName\").size()\n",
    "\n",
    "# Durchschnittliche Ausfüllquote pro Land\n",
    "fill_rates = df[month_columns].notna().mean(axis=1)  # Berechne pro Windkraftanlage\n",
    "average_fill_rate_per_country = df.groupby(\"AreaDisplayName\")[month_columns].apply(\n",
    "    lambda x: x.notna().mean(axis=1).mean()\n",
    ")\n",
    "\n",
    "# Plot erstellen\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Linke y-Achse: Anzahl der Windkraftanlagen\n",
    "ax1.bar(\n",
    "    windfarm_count.index,\n",
    "    windfarm_count.values,\n",
    "    label=\"Anzahl der Windkraftanlagen\",\n",
    "    alpha=0.7\n",
    ")\n",
    "ax1.set_ylabel(\"Anzahl der Windkraftanlagen\", fontsize=12)\n",
    "ax1.set_xlabel(\"AreaDisplayName (Land)\", fontsize=12)\n",
    "ax1.tick_params(axis=\"x\", rotation=45)\n",
    "ax1.legend(loc=\"upper left\")\n",
    "\n",
    "# Rechte y-Achse: Durchschnittliche Ausfüllquote\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(\n",
    "    average_fill_rate_per_country.index,\n",
    "    average_fill_rate_per_country.values * 100,  # Prozentualer Wert\n",
    "    label=\"Durchschnittliche Ausfüllquote (%)\",\n",
    "    alpha=0.5,\n",
    "    color=\"orange\"\n",
    ")\n",
    "ax2.set_ylabel(\"Durchschnittliche Ausfüllquote (%)\", fontsize=12)\n",
    "ax2.legend(loc=\"upper right\")\n",
    "\n",
    "# Titel und Layout\n",
    "plt.title(\"Windkraftanlagen und Ausfüllquote pro Land\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot anzeigen\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download table to find common name for UK power plants from website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL der Webseite\n",
    "url = \"https://osuked.github.io/Power-Station-Dictionary/dictionary.html\"\n",
    "\n",
    "# Abrufen der Webseite\n",
    "response = requests.get(url)\n",
    "if response.status_code != 200:\n",
    "    print(\"Fehler beim Abrufen der Webseite.\")\n",
    "    exit()\n",
    "\n",
    "# Parsing der Webseite mit BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Finden aller Tabellen auf der Seite\n",
    "tables = soup.find_all('table')\n",
    "if len(tables) < 3:\n",
    "    print(\"Weniger als 3 Tabellen auf der Seite gefunden.\")\n",
    "    exit()\n",
    "\n",
    "# Die dritte Tabelle auswählen (Index 2, da Python nullbasiert zählt)\n",
    "table = tables[2]\n",
    "\n",
    "# Spaltennamen extrahieren\n",
    "headers = [header.text.strip() for header in table.find_all('th')]\n",
    "\n",
    "# Zeilen extrahieren\n",
    "rows = []\n",
    "for row in table.find_all('tr')[1:]:  # Überspringt die Kopfzeile\n",
    "    cells = [cell.text.strip() for cell in row.find_all('td')]\n",
    "    rows.append(cells)\n",
    "\n",
    "# DataFrame erstellen\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "# Datei speichern\n",
    "output_path = \"data/WPPs/Power_Station_Dictionary.xlsx\"\n",
    "df.to_excel(output_path, index=False)\n",
    "print(f\"Die dritte Tabelle wurde erfolgreich als '{output_path}' gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add common names from downloaded to assignment file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two provided files\n",
    "file_1_path = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\WPPs\\Power_Station_Dictionary.xlsx\"\n",
    "file_2_path = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\production_summary_all.xlsx\"\n",
    "\n",
    "# Read the first file (Power Station Dictionary)\n",
    "df1 = pd.read_excel(file_1_path)\n",
    "\n",
    "# Read the second file (Assignment data, Sheet2)\n",
    "df2 = pd.read_excel(file_2_path, sheet_name=\"Sheet2\")\n",
    "\n",
    "# Iterate over rows in df2 to match and update the \"Names_UK_Plants\" column\n",
    "for index, row in df2.iterrows():\n",
    "    generation_unit_name = row['GenerationUnitName']\n",
    "    \n",
    "    # Check if this name appears in the \"National Grid BMU ID\" of the first file\n",
    "    matching_rows = df1[df1['National Grid BMU ID'].str.contains(generation_unit_name, na=False, case=False)]\n",
    "    \n",
    "    if not matching_rows.empty:\n",
    "        # Get the \"Common Name\" value(s) and update the \"Names_UK_Plants\" column in df2\n",
    "        common_names = matching_rows['Common Name'].tolist()\n",
    "        df2.at[index, 'Names_UK_Plants'] = ', '.join(common_names)\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "output_path = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\Assignment.xlsx\"\n",
    "df2.to_excel(output_path, index=False)\n",
    "\n",
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load WPPs and assignment file (after manual assignment has been conducted) and combine the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Laden der Daten\n",
    "df_wind_power = pd.read_parquet(\"data/WPPs/The_Wind_Power.parquet\")\n",
    "df_assignment = pd.read_excel(\"data/Assignment.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number potential WPPs: 115\n",
      "number actual WPPs: 114\n",
      "suspended WPPs (no name, location, capacity or not in operating status): {38854}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_33800\\1812135338.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['Production'] = None\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_33800\\1812135338.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered[\"Production\"] = df_filtered[\"Production\"].astype(object)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"str\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 61\u001b[0m\n\u001b[0;32m     57\u001b[0m     df_filtered\u001b[38;5;241m.\u001b[39mat[current_index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProduction\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m production_data\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# Kombiniere die vorhandenen und neuen Produktionsdaten\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     combined_production \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 61\u001b[0m         \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(existing_production, production_data)\n\u001b[0;32m     62\u001b[0m     ]\n\u001b[0;32m     63\u001b[0m     df_filtered\u001b[38;5;241m.\u001b[39mat[current_index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProduction\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m combined_production\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate list (not \"str\") to list"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Filtere nur Zeilen, bei denen \"ID_The-Wind-Power\" nicht \"not found\" ist\n",
    "df_assignment = df_assignment[df_assignment[\"ID_The-Wind-Power\"] != \"not found\"]\n",
    "\n",
    "# Extrahiere und entpacke alle gültigen IDs aus der Spalte \"ID_The-Wind-Power\"\n",
    "def extract_ids(value):\n",
    "    # Überprüfen, ob der Wert eine Liste ist, und ggf. in einzelne IDs zerlegen\n",
    "    if isinstance(value, str) and value.startswith(\"[\") and value.endswith(\"]\"):\n",
    "        return eval(value)  # Konvertiert die Zeichenkette in eine Liste\n",
    "    elif isinstance(value, (int, str)):\n",
    "        return [int(value)]  # Einzelne IDs werden in eine Liste gewandelt\n",
    "    return []\n",
    "\n",
    "valid_ids = set()\n",
    "df_assignment[\"ID_The-Wind-Power\"].apply(lambda x: valid_ids.update(extract_ids(x)))\n",
    "\n",
    "df_filtered = df_wind_power[df_wind_power['ID'].isin(valid_ids)]\n",
    "\n",
    "print(\"number potential WPPs:\", len(valid_ids))\n",
    "print(\"number actual WPPs:\", len(df_filtered))\n",
    "print(\"suspended WPPs (no name, location, capacity or not in operating status):\", valid_ids - set(df_filtered['ID']))\n",
    "\n",
    "# Füge Attribut für Produktion hinzu\n",
    "df_filtered['Production'] = None\n",
    "df_filtered[\"Production\"] = df_filtered[\"Production\"].astype(object)\n",
    "\n",
    "# Gehe durch jede Zeile der Assignment-Datei und füge Produktionsdaten hinzu\n",
    "for index, row in df_assignment.iterrows():\n",
    "    \n",
    "    ids_in_row = extract_ids(row[\"ID_The-Wind-Power\"])\n",
    "    first_id = ids_in_row[0]\n",
    "    \n",
    "    production_data = []\n",
    "\n",
    "    for year in range(2015, 2025):\n",
    "        for month in range(1, 13):\n",
    "            column_name = f\"{year}_{month:02d}\"\n",
    "            if column_name in df_assignment.columns: # neglect 2024_11 and 2024_12\n",
    "                value = row[column_name] if pd.notna(row[column_name]) else []\n",
    "                production_data.append(value)\n",
    "    \n",
    "    if len(ids_in_row) > 1:\n",
    "        total_power = 0\n",
    "        for id in ids_in_row:\n",
    "            total_power += df_filtered.loc[df_filtered['ID'] == id, \"Total power\"].item() # add power\n",
    "            if id != first_id:\n",
    "                df_filtered = df_filtered[df_filtered['ID'] != id] # delete from dataframe\n",
    "        df_filtered.loc[df_filtered['ID'] == first_id, \"Total power\"] = total_power\n",
    "\n",
    "    # Kombiniere die Produktionsdaten\n",
    "    current_index = df_filtered.loc[df_filtered['ID'] == first_id].index[0]\n",
    "    existing_production = df_filtered.at[current_index, \"Production\"]\n",
    "\n",
    "    # Falls noch keine Produktionsdaten vorhanden sind, initialisiere sie\n",
    "    if existing_production is None:\n",
    "        df_filtered.at[current_index, \"Production\"] = production_data\n",
    "    else:\n",
    "        # Kombiniere die vorhandenen und neuen Produktionsdaten\n",
    "        combined_production = [\n",
    "            a + b for a, b in zip(existing_production, production_data)\n",
    "        ]\n",
    "        df_filtered.at[current_index, \"Production\"] = combined_production\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webapp_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
