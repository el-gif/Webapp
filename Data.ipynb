{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate spearman correlation coefficient matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Beispiel-Daten erstellen\n",
    "data = {\n",
    "    'wind_speed': [1, 2, 3, 4, 5],\n",
    "    'temperature': [5, 6, 7, 8, 7],\n",
    "    'pressure': [10, 9, 2, 4, 3],\n",
    "    'production': []\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Spearman-Korrelationsmatrix berechnen\n",
    "spearman_corr_matrix = df.corr(method='spearman')\n",
    "\n",
    "print(\"Spearman-Korrelationsmatrix:\")\n",
    "print(spearman_corr_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load historical weather data (pressure, temperautre, wind speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "from netCDF4 import num2date\n",
    "\n",
    "# File paths\n",
    "pressure_file = \"data/weather_history/COSMO_REA6/PS.2D.201501.grb\"\n",
    "temperature_file = \"data/weather_history/COSMO_REA6/T_2M.2D.201501.grb\"\n",
    "wind_speed_file = \"data/weather_history/COSMO_REA6/WS_100m.2D.201501.nc4\"\n",
    "\n",
    "# Load datasets\n",
    "pressure = xr.open_dataset(pressure_file, engine=\"cfgrib\")\n",
    "temperature = xr.open_dataset(temperature_file, engine=\"cfgrib\")\n",
    "wind_speed = nc.Dataset(wind_speed_file)\n",
    "\n",
    "number_hours = 1\n",
    "\n",
    "pressure_lons = pressure.longitude.values\n",
    "pressure_lats = pressure.latitude.values\n",
    "pressure_times = pressure.time.values[:number_hours]\n",
    "pressure_values = pressure['sp'].values[:number_hours,:,:]\n",
    "\n",
    "temperature_lons = temperature.longitude.values\n",
    "temperature_lats = temperature.latitude.values\n",
    "temperature_times = temperature.time.values[:number_hours]\n",
    "temperature_values = temperature['t2m'].values[:number_hours,:,:]\n",
    "\n",
    "wind_speed_lons = wind_speed['RLON'][:]\n",
    "wind_speed_lons = np.where(wind_speed_lons > 180, wind_speed_lons - 360, wind_speed_lons)\n",
    "wind_speed_lats = wind_speed['RLAT'][:]\n",
    "wind_speed_time_unit = wind_speed['time'].units\n",
    "wind_speed_times = num2date(wind_speed['time'][:number_hours], wind_speed_time_unit)\n",
    "wind_speed_times = np.array([np.datetime64(t.strftime('%Y-%m-%dT%H:%M:%S')) for t in wind_speed_times])\n",
    "wind_speed_values = wind_speed['wind_speed'][:number_hours,:,:].filled(np.nan) # convert masked array to regular array by converting masked values to nan values\n",
    "\n",
    "def is_equal(array1, array2):\n",
    "    abs_difference = np.abs(array1-array2)\n",
    "    average_difference = np.mean(abs_difference)\n",
    "    if np.array_equal(array1, array2):\n",
    "        print('arrays equal')\n",
    "    elif average_difference < 0.1:\n",
    "        print('difference negligible')\n",
    "    else:\n",
    "        print('arrays not equal')\n",
    "        print(\"average difference:\", np.mean(abs_difference))\n",
    "        print(\"median difference:\", np.median(abs_difference))\n",
    "        print(\"minimal difference:\", np.max(abs_difference))\n",
    "        print(\"maximal difference:\", np.min(abs_difference))\n",
    "\n",
    "is_equal(pressure_lons, temperature_lons)\n",
    "is_equal(pressure_lons, wind_speed_lons)\n",
    "is_equal(pressure_lats, temperature_lats)\n",
    "is_equal(pressure_lats, wind_speed_lats)\n",
    "is_equal(pressure_times, temperature_times)\n",
    "is_equal(pressure_times, wind_speed_times)\n",
    "\n",
    "# # scatter plot\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(pressure_lon, pressure_lat, c='blue', marker='o')\n",
    "# plt.title('Scatter-Diagramm der Längen- und Breitengradwerte')\n",
    "# plt.xlabel('Längengrad')\n",
    "# plt.ylabel('Breitengrad')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "np.save(\"data/weather_history/COSMO_REA6/lons.npy\", pressure_lons)\n",
    "np.save(\"data/weather_history/COSMO_REA6/lats.npy\", pressure_lats)\n",
    "np.save(\"data/weather_history/COSMO_REA6/times.npy\", pressure_times)\n",
    "np.save(\"data/weather_history/COSMO_REA6/pressure_values.npy\", pressure_values)\n",
    "np.save(\"data/weather_history/COSMO_REA6/temperature_values.npy\", temperature_values)\n",
    "np.save(\"data/weather_history/COSMO_REA6/wind_speed_values.npy\", wind_speed_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load historical weather data (wind speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "from netCDF4 import num2date\n",
    "\n",
    "wind_speed_file = \"data/weather_history/COSMO_REA6/WS_100m.2D.201501.nc4\"\n",
    "\n",
    "wind_speed = nc.Dataset(wind_speed_file)\n",
    "\n",
    "number_hours = 48\n",
    "\n",
    "lons = wind_speed['RLON'][:].filled(np.nan)\n",
    "lons = np.where(lons > 180, lons - 360, lons)\n",
    "lats = wind_speed['RLAT'][:].filled(np.nan)\n",
    "time_unit = wind_speed['time'].units\n",
    "times = num2date(wind_speed['time'][:number_hours], time_unit)\n",
    "times = np.array([np.datetime64(t.strftime('%Y-%m-%dT%H:%M:%S')) for t in times])\n",
    "wind_speeds = wind_speed['wind_speed'][:number_hours,:,:].filled(np.nan) # convert masked array to regular array by converting masked values to nan values\n",
    "\n",
    "np.save(\"data/weather_history/COSMO_REA6/lons.npy\", lons)\n",
    "np.save(\"data/weather_history/COSMO_REA6/lats.npy\", lats)\n",
    "np.save(\"data/weather_history/COSMO_REA6/times.npy\", times)\n",
    "np.save(\"data/weather_history/COSMO_REA6/wind_speeds.npy\", wind_speeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save WPPs in parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lat_min, lat_max = 35, 72\n",
    "lon_min, lon_max = -25, 45\n",
    "\n",
    "# Lade die Excel-Datei nur einmal, filtere die relevanten Daten und speichere sie als Parquet-Datei\n",
    "WPP_file = \"data/WPPs/Global-Wind-Power-Tracker-June-2024.xlsx\"\n",
    "df = pd.read_excel(WPP_file, sheet_name='Data')\n",
    "\n",
    "# Filtere die Daten für Europa\n",
    "df_filtered = df[(df['Latitude'] >= lat_min) & (df['Latitude'] <= lat_max) & (df['Longitude'] >= lon_min) & (df['Longitude'] <= lon_max)]\n",
    "\n",
    "# Wähle nur die benötigten Spalten aus\n",
    "df_filtered = df_filtered[['Latitude', 'Longitude', 'Capacity (MW)', 'Project Name', 'Status', 'Operator', 'Owner', 'Start year']]\n",
    "\n",
    "# Speichere die gefilterten Daten im Parquet-Format (deutlich schneller zu lesen und schreiben, als Excel-Dateien, und auch platzsparender)\n",
    "df_filtered.to_parquet(\"data/WPPs/Global-Wind-Power-Tracker-Europe.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save production history example as parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lade die Excel-Datei nur einmal, filtere die relevanten Daten und speichere sie als Parquet-Datei\n",
    "example_file = \"data/production_history/Example/example_time_series.xlsx\" \n",
    "df = pd.read_excel(example_file)\n",
    "\n",
    "# Speichere die gefilterten Daten im Parquet-Format (deutlich schneller zu lesen und schreiben, als Excel-Dateien, und auch platzsparender)\n",
    "df.to_parquet(\"data/production_history/Example/example_time_series.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wind speeds (COSMO-REA6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "\n",
    "fn = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\weather history\\WS_100m.2D.199501.nc4\" # January 1995\n",
    "ds = nc.Dataset(fn)\n",
    "\n",
    "print(ds)\n",
    "\n",
    "time = ds.variables['time'][:]\n",
    "lon = ds.variables['RLON'][:]\n",
    "lat = ds.variables['RLAT'][:]\n",
    "wind_speed = ds.variables['wind_speed'][:]\n",
    "\n",
    "for i in range(len(lon)):\n",
    "    for j in range(len(lon[0])):\n",
    "        lon[i,j] = lon[i,j] - 360 if lon[i,j] > 180 else lon[i,j]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "three different visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "\n",
    "# Erstelle eine Karte mit cartopy\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Trage die Windgeschwindigkeit auf der Karte ein\n",
    "plt.contourf(lon, lat, wind_speed[0,:,:], transform=ccrs.PlateCarree(), cmap='viridis')\n",
    "\n",
    "# Füge Küstenlinien hinzu\n",
    "ax.coastlines()\n",
    "\n",
    "# Zeige die Karte\n",
    "plt.colorbar(label=\"Windgeschwindigkeit (m/s)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# Erstelle eine Karte mit curvilinearen Daten\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Zeichne die Windgeschwindigkeit auf dem curvilinearen Gitter\n",
    "plt.pcolormesh(lon, lat, wind_speed[0,:,:], transform=ccrs.PlateCarree(), cmap='viridis')\n",
    "\n",
    "# Füge Küstenlinien hinzu\n",
    "ax.coastlines()\n",
    "\n",
    "# Zeige die Karte\n",
    "plt.colorbar(label=\"Windgeschwindigkeit (m/s)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Verwende eine Lambert-Projektion\n",
    "ax = plt.axes(projection=ccrs.LambertConformal())\n",
    "\n",
    "# Zeichne die Windgeschwindigkeit auf dem curvilinearen Gitter\n",
    "plt.contourf(lon, lat, wind_speed[0,:,:], transform=ccrs.PlateCarree(), cmap='coolwarm')\n",
    "\n",
    "# Küstenlinien und Raster hinzufügen\n",
    "ax.coastlines()\n",
    "ax.gridlines()\n",
    "\n",
    "plt.colorbar(label=\"Windgeschwindigkeit (m/s)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "power curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definiere die Power Curve (Windgeschwindigkeit und Leistung)\n",
    "wind_speeds = np.arange(0, 25.5, 0.5)  # Windgeschwindigkeiten\n",
    "power_output = [0]*7 + [35, 80, 155, 238, 350, 474, 630, 802, 1018, 1234, 1504, 1773, 2076, 2379, 2664, 2948, 3141, 3334, 3425, 3515, 3546, 3577, 3586, 3594, 3598, 3599] + [3600]*18  # Leistung\n",
    "max_cap = 3600\n",
    "power_output_norm = [x / max_cap for x in power_output]\n",
    "wind_speed_max = 25\n",
    "\n",
    "# Erstelle die Interpolationsfunktion\n",
    "interpolation_function = interp1d(wind_speeds, power_output_norm, kind='cubic', fill_value=\"extrapolate\")\n",
    "\n",
    "# Beispiel für Interpolation: Bestimme Werte für feinere Windgeschwindigkeiten\n",
    "fine_wind_speeds = np.linspace(0, 25, 100)  # Feinere Windgeschwindigkeiten\n",
    "interpolated_power_output = interpolation_function(fine_wind_speeds)\n",
    "\n",
    "# Plot der diskreten und interpolierten Power Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(wind_speeds, power_output_norm, 'o', label='Diskrete Werte')  # Diskrete Punkte\n",
    "plt.plot(fine_wind_speeds, interpolated_power_output, '-', label='Interpolierte Werte')  # Interpolierte Werte\n",
    "plt.xlabel('Windgeschwindigkeit (m/s)')\n",
    "plt.ylabel('Leistung (kW)')\n",
    "plt.title('Interpolierte Power Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Datei laden (relativer Pfad)\n",
    "file_path = \"./Global-Wind-Power-Tracker-June-2024.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name='Data')\n",
    "\n",
    "# Bereich für Europa definieren\n",
    "lat_min, lat_max = 35, 72\n",
    "lon_min, lon_max = -25, 45\n",
    "\n",
    "# Filtere die Daten für den geografischen Bereich in Europa\n",
    "df_filtered = df[(df['Latitude'] >= lat_min) & (df['Latitude'] <= lat_max) & \n",
    "                 (df['Longitude'] >= lon_min) & (df['Longitude'] <= lon_max)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WPPs capacity distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Verteilung der Kapazität berechnen\n",
    "capacity_distribution = df_filtered['Capacity (MW)'].value_counts()\n",
    "\n",
    "# Diagramm erstellen, nur bis zum Maximalwert\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df_filtered['Capacity (MW)'].dropna(), bins=1000, range=(0, 2000), edgecolor='black')\n",
    "plt.title('Verteilung der Windkraftanlagenkapazitäten in Europa')\n",
    "plt.xlabel('Kapazität (MW)')\n",
    "plt.ylabel('Anzahl')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualisation of WPPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyleaflet import Map, Marker, MarkerCluster\n",
    "from ipywidgets import Layout\n",
    "\n",
    "# Erstelle die Karte\n",
    "m = Map(center=[(lat_min + lat_max) / 2, (lon_min + lon_max) / 2],\n",
    "        zoom=5,\n",
    "        layout=Layout(width='100%', height='500px')\n",
    "       )\n",
    "\n",
    "# Erstelle Marker-Objekte für jede Windkraftanlage\n",
    "markers = [Marker(location=(row['Latitude'], row['Longitude'])) for _, row in df_filtered.iterrows()]\n",
    "\n",
    "# Erstelle einen Marker Cluster\n",
    "marker_cluster = MarkerCluster(markers=markers, disable_clustering_at_zoom=18)\n",
    "\n",
    "# Füge den Marker Cluster zur Karte hinzu\n",
    "m.add_layer(marker_cluster)\n",
    "\n",
    "# Zeige die Karte an\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data generation with power curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Masked array in ein reguläres Array umwandeln\n",
    "wind_speed_array = np.ma.filled(wind_speed[0,:,:], np.nan)\n",
    "\n",
    "# Verteilung der Kapazitäten aus 'Capacity (MW)'\n",
    "capacity_distribution = df_filtered['Capacity (MW)'].dropna().values\n",
    "\n",
    "# Häufigkeiten der Kapazitäten berechnen\n",
    "unique_capacities, counts = np.unique(capacity_distribution, return_counts=True)\n",
    "\n",
    "# Wahrscheinlichkeiten für jede Kapazität (relative Häufigkeit)\n",
    "probabilities = counts / counts.sum()\n",
    "\n",
    "# Initialisiere die np.array Strukturen für die Daten\n",
    "data = np.zeros((3, wind_speed_array.size))  # 3 Reihen für wind_speed, capacity, production\n",
    "\n",
    "# Skaliere die Produktion basierend auf der Verteilung mit gewichteter Auswahl\n",
    "for i in range(wind_speed_array.shape[0]):\n",
    "    for j in range(wind_speed_array.shape[1]):\n",
    "        wind_speed_select = wind_speed_array[i, j] # Verwende alle Windgeschwindigkeiten von Januar 1995\n",
    "        capacity = np.random.choice(unique_capacities, p=probabilities) # Wähle eine Kapazität basierend auf ihrer Wahrscheinlichkeit\n",
    "        production = interpolation_function(wind_speed_select) * capacity\n",
    "        production = production if wind_speed_select < wind_speed_max else 0\n",
    "                \n",
    "        # Fülle die Werte in das np.array\n",
    "        data[0, i*wind_speed_array.shape[0]+j] = wind_speed_select  # Windgeschwindigkeit\n",
    "        data[1, i*wind_speed_array.shape[0]+j] = capacity  # Kapazität\n",
    "        data[2, i*wind_speed_array.shape[0]+j] = production # Produktion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Verwende dein generiertes numpy Array mit Wind Speed, Capacity und Scaled Production\n",
    "# data[0, :] -> Windgeschwindigkeit\n",
    "# data[1, :] -> Kapazität\n",
    "# data[2, :] -> Skalierte Produktion\n",
    "\n",
    "X = data[:2, :].T  # Features: Erste zwei Reihen, transponiert zu (Anzahl der Datenpunkte, 2)\n",
    "y = data[2, :]  # Target: Dritte Reihe (scaled production)\n",
    "\n",
    "# Normalisierung der Features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)  # Normiere Windgeschwindigkeit und Kapazität auf Standardnormalverteilung\n",
    "\n",
    "# Train/Test Split (80% Training, 20% Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Konvertiere die Numpy Arrays in PyTorch Tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Output Tensor (2D, mit shape [N, 1])\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Erstellen des ANN-Modells\n",
    "class ANN_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANN_Model, self).__init__()\n",
    "        # Input Layer: 2 Features (Wind Speed und Capacity)\n",
    "        # Hidden Layer 1: 64 Neuronen\n",
    "        # Hidden Layer 2: 32 Neuronen\n",
    "        # Output Layer: 1 (Scaled Production)\n",
    "        self.fc1 = nn.Linear(2, 64)  # Eingabe: 2 Features\n",
    "        self.fc2 = nn.Linear(64, 32)  # Erste versteckte Schicht\n",
    "        self.fc3 = nn.Linear(32, 1)  # Ausgabe: 1 Wert (Scaled Production)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialisiere das Modell, den Optimizer und die Loss-Funktion\n",
    "model = ANN_Model()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error für Regression\n",
    "\n",
    "# Training des Modells\n",
    "epochs = 500\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Vorwärtsdurchlauf\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    \n",
    "    # Rückwärtsdurchlauf und Optimierung\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Speichern des Verlusts\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Plotte den Trainingsverlust\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.show()\n",
    "\n",
    "# Teste das Modell\n",
    "model.eval()\n",
    "with torch.no_grad(): # don't calculate gradient: model remains tel quel during testing\n",
    "    predictions = model(X_test_tensor)\n",
    "    test_loss = criterion(predictions, y_test_tensor)\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "# Forecast auf Basis hypothetischer Werte (z.B. hypothetische Wind Speed und Capacity)\n",
    "hypothetical_data = np.array([[12, 1500], [8, 3000], [20, 500]])  # Beispielwerte: [Wind Speed, Capacity]\n",
    "hypothetical_tensor = torch.tensor(hypothetical_data, dtype=torch.float32)\n",
    "\n",
    "# Vorhersagen machen\n",
    "with torch.no_grad(): # don't calculate gradient: model remains tel quel during forecasting\n",
    "    forecasted_production = model(hypothetical_tensor)\n",
    "    print(f'Hypothetische Vorhersagen (Wind Speed, Capacity -> Scaled Production): {forecasted_production}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Beispiel-Daten (Windgeschwindigkeit, Kapazität, Produktion)\n",
    "# Nehmen wir an, 'data' ist ein Vektor mit drei Listen:\n",
    "wind_speed = data[0]  # Erste Zeile: Windgeschwindigkeiten\n",
    "capacity_mw = data[1]  # Zweite Zeile: Kapazität (MW)\n",
    "scaled_production = data[2]  # Dritte Zeile: Produktion (Scaled Production)\n",
    "\n",
    "# Eingabedaten als Feature-Array\n",
    "X = np.column_stack((wind_speed, capacity_mw))\n",
    "\n",
    "# Zielwert (Scaled Production)\n",
    "y = scaled_production\n",
    "\n",
    "# Daten aufteilen in Trainings- und Testdatensatz (80% Training, 20% Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Daten normalisieren (wichtig für ein stabiles Training)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Erstelle das Modell (ANN)\n",
    "model = Sequential()\n",
    "\n",
    "# Eingabeschicht und erste verborgene Schicht (mit 64 Neuronen und ReLU-Aktivierungsfunktion)\n",
    "model.add(Dense(64, input_dim=2, activation='relu'))\n",
    "\n",
    "# Zweite verborgene Schicht\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Ausgabeschicht (1 Neuron für die Vorhersage der Produktion)\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Kompiliere das Modell mit Adam Optimizer und Mean Squared Error als Verlustfunktion\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Trainiere das Modell auf den Trainingsdaten\n",
    "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=1, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluierung des Modells auf den Testdaten\n",
    "loss = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f'Test Loss (Mean Squared Error): {loss}')\n",
    "\n",
    "# Vorhersage auf neuen hypothetischen Daten (z.B. Kapazität=4.5 MW, Windgeschwindigkeit=15 m/s)\n",
    "hypothetical_data = np.array([[15, 4.5]])\n",
    "hypothetical_data_scaled = scaler.transform(hypothetical_data)\n",
    "predicted_production = model.predict(hypothetical_data_scaled)\n",
    "\n",
    "print(f'Hypothetische Vorhersage der Produktion: {predicted_production[0][0]}')\n",
    "\n",
    "# Vorhersage auf den Testdaten (zum Vergleich)\n",
    "predictions = model.predict(X_test_scaled)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webapp_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
