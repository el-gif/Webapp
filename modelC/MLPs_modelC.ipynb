{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lead time: 0\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 102\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 105\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 108\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 111\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 114\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 117\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 12\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 120\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 123\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 126\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 129\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 132\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 135\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 138\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 141\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 144\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 15\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 18\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 21\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 24\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 27\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 3\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 30\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 33\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 36\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 39\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 42\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 45\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 48\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 51\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 54\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 57\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 6\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 60\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 63\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 66\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 69\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 72\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 75\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 78\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 81\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 84\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 87\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 9\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 90\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 93\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 96\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 99\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "All parameters saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.nn import HuberLoss, MSELoss, L1Loss\n",
    "\n",
    "# Lists to store models and scalers\n",
    "models = {}\n",
    "scalers = {}\n",
    "encoders = {}\n",
    "input_sizes = {}\n",
    "metrics = {} # for model performance analysis\n",
    "test_indices = {} # for testing based on category\n",
    "\n",
    "# Define MLP class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3366)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# PyTorch Dataset Class\n",
    "class WindPowerDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.targets[index]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "input_dir = r\"E:\\MA_data\\WPPs+production+reforecast\"\n",
    "\n",
    "for file in os.listdir(input_dir):\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    if os.path.isfile(file_path):  # Ensure it's a file (not a folder)\n",
    "        lead_time = int(file.split(\"_\")[-1].replace(\".json\", \"\"))\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            forecast_data = json.load(file)\n",
    "\n",
    "    print(f\"Processing lead time: {lead_time}\")\n",
    "\n",
    "    print(f\"    Data preparation\")\n",
    "\n",
    "    all_turbine_types = []\n",
    "    all_hub_heights = []\n",
    "    all_capacities = []\n",
    "    all_commissioning_dates = []\n",
    "    all_production_data = []\n",
    "\n",
    "    for unique_key, wpp in forecast_data.items():\n",
    "        all_turbine_types.append(str(wpp[\"Turbine\"]))\n",
    "        all_hub_heights.append(wpp[\"Hub_height\"])\n",
    "        all_capacities.append(wpp[\"Capacity\"])\n",
    "        all_commissioning_dates.append(f\"{wpp['Commissioning_date']}/06\" if isinstance(wpp[\"Commissioning_date\"], str) and \"/\" not in wpp[\"Commissioning_date\"] else wpp[\"Commissioning_date\"])\n",
    "        all_production_data.append(wpp[\"Time Series\"])\n",
    "\n",
    "    # One-Hot-Encoding for turbine types\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    turbine_types_onehot = encoder.fit_transform(np.array(all_turbine_types).reshape(-1, 1))\n",
    "\n",
    "    # convert to datetime\n",
    "    standardised_dates = pd.to_datetime(all_commissioning_dates, format='%Y/%m')\n",
    "\n",
    "    # calculate age\n",
    "    ref_date = pd.Timestamp(\"2024-12-01\")\n",
    "    ages = ref_date.year * 12 + ref_date.month - (standardised_dates.year * 12 + standardised_dates.month)\n",
    "\n",
    "    # create combined features and output lists\n",
    "    combined_features_raw = []\n",
    "    output_raw = []\n",
    "    \n",
    "    # convert data in feature arrays\n",
    "    for idx, production_data in enumerate(all_production_data):\n",
    "        num_rows = len(production_data)\n",
    "\n",
    "        # Repetitions for common features\n",
    "        turbine_type_repeated = np.tile(turbine_types_onehot[idx], (num_rows, 1))\n",
    "        hub_height_repeated = np.full((num_rows, 1), float(all_hub_heights[idx]))\n",
    "        age_repeated = np.full((num_rows, 1), ages[idx])\n",
    "\n",
    "        # Extract production values and wind speeds\n",
    "        production_values = np.array([entry[1] for entry in production_data]).reshape(-1, 1) / all_capacities[idx]\n",
    "        wind_speeds = np.array([entry[2] for entry in production_data]).reshape(-1, 1)\n",
    "\n",
    "        # combine all features\n",
    "        combined_chunk = np.hstack((\n",
    "            turbine_type_repeated,\n",
    "            hub_height_repeated,\n",
    "            age_repeated,\n",
    "            wind_speeds\n",
    "        ))\n",
    "\n",
    "        # add the data\n",
    "        combined_features_raw.append(combined_chunk)\n",
    "        output_raw.append(production_values)\n",
    "\n",
    "    # combine all data chunks to one array\n",
    "    combined_features = np.vstack(combined_features_raw)\n",
    "    output = np.vstack(output_raw)\n",
    "\n",
    "    # Interpolate missing values (linear interpolation) in pandas\n",
    "    wind_speed_series = pd.Series(combined_features[:, -1])\n",
    "    wind_speed_series.interpolate(method='linear', inplace=True)\n",
    "    combined_features[:, -1] = wind_speed_series.to_numpy()\n",
    "\n",
    "    # round all values to four decimal places\n",
    "    combined_features = np.round(combined_features, decimals=4)\n",
    "    output = np.round(output, decimals=4)\n",
    "        \n",
    "    # Normalise numerical features\n",
    "    scaler_wind = StandardScaler()\n",
    "    scaler_ages = StandardScaler()\n",
    "    scaler_hub_heights = StandardScaler()\n",
    "\n",
    "    # Skalieren der einzelnen Features\n",
    "    combined_features[:, -1] = scaler_wind.fit_transform(combined_features[:, -1].reshape(-1, 1)).flatten() # scale wind speeds\n",
    "    combined_features[:, -2] = scaler_ages.fit_transform(combined_features[:, -2].reshape(-1, 1)).flatten()  # scale ages\n",
    "    combined_features[:, -3] = scaler_hub_heights.fit_transform(combined_features[:, -3].reshape(-1, 1)).flatten()  # scale hub heights\n",
    "    \n",
    "    # Convert to PyTorch Dataset\n",
    "    dataset = WindPowerDataset(combined_features, output)\n",
    "    \n",
    "    params = {\"batch_size\": 128,\n",
    "              \"lr\": 0.00010155,\n",
    "              \"number_epochs\": 10}\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Train-test split\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    torch.manual_seed(0)\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    # shuffling doesn't matter here, has already taken place during random_split\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "    # Model setup\n",
    "    input_size = combined_features.shape[1]\n",
    "\n",
    "    # use static instead of dynamic computational graphs\n",
    "    model = torch.jit.script(MLP(input_size=input_size)).to(device)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Trainings-Konfiguration\n",
    "    mae_criterion = L1Loss()\n",
    "    mse_criterion = MSELoss()\n",
    "    huber_criterion = HuberLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "\n",
    "    # Training\n",
    "    print(f\"    Training\")\n",
    "    for epoch in range(params[\"number_epochs\"]):\n",
    "        print(f\"        Epoch {epoch + 1}/{params['number_epochs']}\")\n",
    "        model.train()\n",
    "        train_loss_mae, train_loss_mse, train_loss_huber = 0, 0, 0\n",
    "\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            \n",
    "            # Calculate metrics for each criterion\n",
    "            loss_mae = mae_criterion(outputs, batch_y)\n",
    "            loss_mse = mse_criterion(outputs, batch_y)\n",
    "            loss_huber = huber_criterion(outputs, batch_y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_huber.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate metrics for logging\n",
    "            train_loss_mae += loss_mae.item()\n",
    "            train_loss_mse += loss_mse.item()\n",
    "            train_loss_huber += loss_huber.item()\n",
    "\n",
    "        train_loss_mae /= len(train_loader)\n",
    "        train_loss_mse /= len(train_loader)\n",
    "        train_loss_huber /= len(train_loader)\n",
    "\n",
    "    # Testen\n",
    "    print(f\"    Testing\")\n",
    "    model.eval()\n",
    "\n",
    "    test_loss_mae, test_loss_mse, test_loss_huber = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            preds = model(batch_x)\n",
    "            \n",
    "            test_loss_mae += mae_criterion(preds, batch_y).item()\n",
    "            test_loss_mse += mse_criterion(preds, batch_y).item()\n",
    "            test_loss_huber += huber_criterion(preds, batch_y).item()\n",
    "\n",
    "    test_loss_mae /= len(test_loader)\n",
    "    test_loss_mse /= len(test_loader)\n",
    "    test_loss_huber /= len(test_loader)\n",
    "    \n",
    "    models[lead_time] = model.state_dict()\n",
    "    \n",
    "    scalers[lead_time] = {\n",
    "        \"winds\": scaler_wind,\n",
    "        \"ages\": scaler_ages,\n",
    "        \"hub_heights\": scaler_hub_heights\n",
    "    }\n",
    "\n",
    "    encoders[lead_time] = encoder\n",
    "\n",
    "    input_sizes[lead_time] = input_size\n",
    "\n",
    "    metrics[lead_time] = {\n",
    "        \"Training\": {\n",
    "            \"Huber\": train_loss_huber,\n",
    "            \"MAE\": train_loss_mae,\n",
    "            \"MSE\":train_loss_mse,\n",
    "            \"RMSE\": np.sqrt(train_loss_mse)\n",
    "        },\n",
    "        \"Testing\": {\n",
    "            \"Huber\": test_loss_huber,\n",
    "            \"MAE\": test_loss_mae,\n",
    "            \"MSE\": test_loss_mse,\n",
    "            \"RMSE\": np.sqrt(test_loss_mse)\n",
    "        },\n",
    "    }\n",
    "\n",
    "    test_indices[lead_time] = test_dataset.indices\n",
    "\n",
    "# Save all parameters\n",
    "torch.save(models, \"parameters/models.pth\")\n",
    "joblib.dump(scalers, \"parameters/scalers.pkl\")\n",
    "joblib.dump(encoders, \"parameters/encoders.pkl\")\n",
    "joblib.dump(input_sizes, \"parameters/input_sizes.pkl\")\n",
    "joblib.dump(metrics, \"parameters/metrics.pkl\")\n",
    "joblib.dump(test_indices, \"parameters/test_indices.pkl\")\n",
    "print(\"All parameters saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model training for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lead time: 0\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 102\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 105\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 108\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 111\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 114\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 117\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 12\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 120\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 123\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 126\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 129\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 132\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 135\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 138\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 141\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 144\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 15\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 18\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 21\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 24\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 27\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 3\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 30\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 33\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 36\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 39\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 42\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 45\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 48\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 51\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 54\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 57\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 6\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 60\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 63\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 66\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 69\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 72\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 75\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 78\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 81\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 84\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 87\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 9\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 90\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 93\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 96\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 99\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "All parameters saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.nn import HuberLoss, MSELoss, L1Loss\n",
    "\n",
    "# Lists to store models and scalers\n",
    "models = {}\n",
    "scalers = {}\n",
    "encoders = {}\n",
    "input_sizes = {}\n",
    "metrics = {}\n",
    "\n",
    "# Define MLP class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3366)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# PyTorch Dataset Class\n",
    "class WindPowerDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.targets[index]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "input_dir = r\"E:\\MA_data\\WPPs+production+reforecast\"\n",
    "\n",
    "for file in os.listdir(input_dir):\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    if os.path.isfile(file_path):  # Ensure it's a file (not a folder)\n",
    "        lead_time = int(file.split(\"_\")[-1].replace(\".json\", \"\"))\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            forecast_data = json.load(file)\n",
    "\n",
    "    print(f\"Processing lead time: {lead_time}\")\n",
    "\n",
    "    print(f\"    Data preparation\")\n",
    "\n",
    "    all_turbine_types = []\n",
    "    all_hub_heights = []\n",
    "    all_capacities = []\n",
    "    all_commissioning_dates = []\n",
    "    all_production_data = []\n",
    "\n",
    "    for unique_key, wpp in forecast_data.items():\n",
    "        all_turbine_types.append(str(wpp[\"Turbine\"]))\n",
    "        all_hub_heights.append(wpp[\"Hub_height\"])\n",
    "        all_capacities.append(wpp[\"Capacity\"])\n",
    "        all_commissioning_dates.append(f\"{wpp['Commissioning_date']}/06\" if isinstance(wpp[\"Commissioning_date\"], str) and \"/\" not in wpp[\"Commissioning_date\"] else wpp[\"Commissioning_date\"])\n",
    "        all_production_data.append(wpp[\"Time Series\"])\n",
    "\n",
    "    # One-Hot-Encoding for turbine types\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    turbine_types_onehot = encoder.fit_transform(np.array(all_turbine_types).reshape(-1, 1))\n",
    "\n",
    "    # convert to datetime\n",
    "    standardised_dates = pd.to_datetime(all_commissioning_dates, format='%Y/%m')\n",
    "\n",
    "    # calculate age\n",
    "    ref_date = pd.Timestamp(\"2024-12-01\")\n",
    "    ages = ref_date.year * 12 + ref_date.month - (standardised_dates.year * 12 + standardised_dates.month)\n",
    "\n",
    "    # create combined features and output lists\n",
    "    combined_features_raw = []\n",
    "    output_raw = []\n",
    "    \n",
    "    # convert data in feature arrays\n",
    "    for idx, production_data in enumerate(all_production_data):\n",
    "        num_rows = len(production_data)\n",
    "\n",
    "        # Repetitions for common features\n",
    "        turbine_type_repeated = np.tile(turbine_types_onehot[idx], (num_rows, 1))\n",
    "        hub_height_repeated = np.full((num_rows, 1), float(all_hub_heights[idx]))\n",
    "        age_repeated = np.full((num_rows, 1), ages[idx])\n",
    "\n",
    "        # Extract production values and wind speeds\n",
    "        production_values = np.array([entry[1] for entry in production_data]).reshape(-1, 1) / all_capacities[idx]\n",
    "        wind_speeds = np.array([entry[2] for entry in production_data]).reshape(-1, 1)\n",
    "\n",
    "        # combine all features\n",
    "        combined_chunk = np.hstack((\n",
    "            turbine_type_repeated,\n",
    "            hub_height_repeated,\n",
    "            age_repeated,\n",
    "            wind_speeds\n",
    "        ))\n",
    "\n",
    "        # add the data\n",
    "        combined_features_raw.append(combined_chunk)\n",
    "        output_raw.append(production_values)\n",
    "\n",
    "    # combine all data chunks to one array\n",
    "    combined_features = np.vstack(combined_features_raw)\n",
    "    output = np.vstack(output_raw)\n",
    "\n",
    "    # Interpolate missing values (linear interpolation) in pandas\n",
    "    wind_speed_series = pd.Series(combined_features[:, -1])\n",
    "    wind_speed_series.interpolate(method='linear', inplace=True)\n",
    "    combined_features[:, -1] = wind_speed_series.to_numpy()\n",
    "\n",
    "    # round all values to four decimal places\n",
    "    combined_features = np.round(combined_features, decimals=4)\n",
    "    output = np.round(output, decimals=4)\n",
    "        \n",
    "    # Normalise numerical features\n",
    "    scaler_wind = StandardScaler()\n",
    "    scaler_ages = StandardScaler()\n",
    "    scaler_hub_heights = StandardScaler()\n",
    "\n",
    "    # Skalieren der einzelnen Features\n",
    "    combined_features[:, -1] = scaler_wind.fit_transform(combined_features[:, -1].reshape(-1, 1)).flatten() # scale wind speeds\n",
    "    combined_features[:, -2] = scaler_ages.fit_transform(combined_features[:, -2].reshape(-1, 1)).flatten()  # scale ages\n",
    "    combined_features[:, -3] = scaler_hub_heights.fit_transform(combined_features[:, -3].reshape(-1, 1)).flatten()  # scale hub heights\n",
    "    \n",
    "    # Convert to PyTorch Dataset\n",
    "    dataset = WindPowerDataset(combined_features, output)\n",
    "    \n",
    "    params = {\"batch_size\": 128,\n",
    "              \"lr\": 0.00010155,\n",
    "              \"number_epochs\": 10}\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # shuffling matters here\n",
    "    data_loader = DataLoader(dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    \n",
    "    # Model setup\n",
    "    input_size = combined_features.shape[1]\n",
    "\n",
    "    # use static instead of dynamic computational graphs\n",
    "    model = torch.jit.script(MLP(input_size=input_size)).to(device)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Trainings-Konfiguration\n",
    "    mae_criterion = L1Loss()\n",
    "    mse_criterion = MSELoss()\n",
    "    huber_criterion = HuberLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "\n",
    "    # Training\n",
    "    print(f\"    Training\")\n",
    "    for epoch in range(params[\"number_epochs\"]):\n",
    "        print(f\"        Epoch {epoch + 1}/{params['number_epochs']}\")\n",
    "        model.train()\n",
    "        train_loss_mae, train_loss_mse, train_loss_huber = 0, 0, 0\n",
    "\n",
    "        for batch_x, batch_y in data_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            \n",
    "            # Calculate metrics for each criterion\n",
    "            loss_mae = mae_criterion(outputs, batch_y)\n",
    "            loss_mse = mse_criterion(outputs, batch_y)\n",
    "            loss_huber = huber_criterion(outputs, batch_y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_huber.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate metrics for logging\n",
    "            train_loss_mae += loss_mae.item()\n",
    "            train_loss_mse += loss_mse.item()\n",
    "            train_loss_huber += loss_huber.item()\n",
    "\n",
    "        train_loss_mae /= len(data_loader)\n",
    "        train_loss_mse /= len(data_loader)\n",
    "        train_loss_huber /= len(data_loader)\n",
    "    \n",
    "    models[lead_time] = model.state_dict()\n",
    "    \n",
    "    scalers[lead_time] = {\n",
    "        \"winds\": scaler_wind,\n",
    "        \"ages\": scaler_ages,\n",
    "        \"hub_heights\": scaler_hub_heights\n",
    "    }\n",
    "\n",
    "    encoders[lead_time] = encoder\n",
    "\n",
    "    input_sizes[lead_time] = input_size\n",
    "\n",
    "    metrics[lead_time] = {\n",
    "        \"Huber\": train_loss_huber,\n",
    "        \"MAE\": train_loss_mae,\n",
    "        \"MSE\":train_loss_mse,\n",
    "        \"RMSE\": np.sqrt(train_loss_mse)\n",
    "    }\n",
    "\n",
    "# Save all parameters\n",
    "torch.save(models, \"parameters_deployment/models.pth\")\n",
    "joblib.dump(scalers, \"parameters_deployment/scalers.pkl\")\n",
    "joblib.dump(encoders, \"parameters_deployment/encoders.pkl\")\n",
    "joblib.dump(input_sizes, \"parameters_deployment/input_sizes.pkl\")\n",
    "joblib.dump(metrics, \"parameters_deployment/metrics.pkl\")\n",
    "print(\"All parameters saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Check if input sizes and encoders are the same for all lead times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model C for testing\n",
      "✅ All lead times have the same input size: 56\n",
      "✅ All encoders have identical categories across lead times.\n",
      "Model C for deployment\n",
      "✅ All lead times have the same input size: 56\n",
      "✅ All encoders have identical categories across lead times.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "print(\"Model C for testing\")\n",
    "\n",
    "input_sizes = joblib.load(\"parameters/input_sizes.pkl\")\n",
    "encoders = joblib.load(\"parameters/encoders.pkl\")\n",
    "\n",
    "# Check if all input sizes are the same for all lead times\n",
    "unique_input_sizes = set(input_sizes.values())\n",
    "\n",
    "if len(unique_input_sizes) == 1:\n",
    "    print(f\"✅ All lead times have the same input size: {unique_input_sizes.pop()}\")\n",
    "else:\n",
    "    print(f\"⚠️ Different input sizes detected: {unique_input_sizes}\")\n",
    "\n",
    "# Check if all encoders have the same categories\n",
    "encoder_refs = list(encoders.values())\n",
    "first_encoder_categories = encoder_refs[0].categories_\n",
    "\n",
    "same_encoders = all(\n",
    "    all((cat1 == cat2).all() for cat1, cat2 in zip(enc.categories_, first_encoder_categories))\n",
    "    for enc in encoder_refs\n",
    ")\n",
    "\n",
    "if same_encoders:\n",
    "    print(\"✅ All encoders have identical categories across lead times.\")\n",
    "else:\n",
    "    print(\"⚠️ Encoders have different categories across lead times!\")\n",
    "\n",
    "\n",
    "print(\"Model C for deployment\")\n",
    "\n",
    "input_sizes = joblib.load(\"parameters_deployment/input_sizes.pkl\")\n",
    "encoders = joblib.load(\"parameters_deployment/encoders.pkl\")\n",
    "\n",
    "# Check if all input sizes are the same for all lead times\n",
    "unique_input_sizes = set(input_sizes.values())\n",
    "\n",
    "if len(unique_input_sizes) == 1:\n",
    "    print(f\"✅ All lead times have the same input size: {unique_input_sizes.pop()}\")\n",
    "else:\n",
    "    print(f\"⚠️ Different input sizes detected: {unique_input_sizes}\")\n",
    "\n",
    "# Check if all encoders have the same categories\n",
    "encoder_refs = list(encoders.values())\n",
    "first_encoder_categories = encoder_refs[0].categories_\n",
    "\n",
    "same_encoders = all(\n",
    "    all((cat1 == cat2).all() for cat1, cat2 in zip(enc.categories_, first_encoder_categories))\n",
    "    for enc in encoder_refs\n",
    ")\n",
    "\n",
    "if same_encoders:\n",
    "    print(\"✅ All encoders have identical categories across lead times.\")\n",
    "else:\n",
    "    print(\"⚠️ Encoders have different categories across lead times!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Further investigation</h1>\n",
    "1. Model testing by groups in certain categories<br>\n",
    "2. Determination of standard deviation and bias<br>\n",
    "3. Creation of vector with itemised production and outputs for histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lead time: 0\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 102\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 105\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 108\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 111\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 114\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 117\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 12\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 120\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 123\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 126\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 129\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 132\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 135\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 138\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 141\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 144\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 15\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 18\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 21\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 24\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 27\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 3\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 30\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 33\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 36\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 39\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 42\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 45\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 48\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 51\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 54\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 57\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 6\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 60\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 63\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 66\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 69\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 72\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 75\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 78\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 81\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 84\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 87\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 9\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 90\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 93\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 96\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 99\n",
      "    Data preparation\n",
      "    Testing\n",
      "Metrics saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.nn import HuberLoss, MSELoss, L1Loss\n",
    "\n",
    "encoders = joblib.load(\"parameters/encoders.pkl\")\n",
    "input_sizes = joblib.load(\"parameters/input_sizes.pkl\")\n",
    "scalers = joblib.load(\"parameters/scalers.pkl\")\n",
    "test_indices = joblib.load(\"parameters/test_indices.pkl\") # same random_split as during training of the model, so that testing now only is done on unseen data \n",
    "model_state_dicts = torch.load(\"parameters/models.pth\", weights_only=True)\n",
    "\n",
    "metrics = {}\n",
    "metrics_detailed = {}\n",
    "all_values_itemised = {} # for histograms with error distributions\n",
    "\n",
    "# Define MLP class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3366)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# PyTorch Dataset Class\n",
    "class WindPowerDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.targets[index]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "params = {\"batch_size\": 128,\n",
    "        \"lr\": 0.00010155,\n",
    "        \"number_epochs\": 10}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "mae_criterion = L1Loss()\n",
    "mse_criterion = MSELoss()\n",
    "huber_criterion = HuberLoss()\n",
    "\n",
    "input_dir = r\"E:\\MA_data\\WPPs+production+reforecast\"\n",
    "\n",
    "for file in os.listdir(input_dir):\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    if os.path.isfile(file_path):  # Ensure it's a file (not a folder)\n",
    "        lead_time = int(file.split(\"_\")[-1].replace(\".json\", \"\"))\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            forecast_data = json.load(file)\n",
    "\n",
    "    print(f\"Processing lead time: {lead_time}\")\n",
    "\n",
    "    print(f\"    Data preparation\")\n",
    "\n",
    "    metrics[lead_time] = {}\n",
    "\n",
    "    encoder = encoders[lead_time]\n",
    "    input_size = input_sizes[lead_time]\n",
    "    scalers_ = scalers[lead_time]\n",
    "    test_indices_ = test_indices[lead_time]\n",
    "    model_state_dict = model_state_dicts[lead_time]\n",
    "\n",
    "    all_turbine_types = []\n",
    "    all_hub_heights = []\n",
    "    all_capacities = []\n",
    "    all_commissioning_dates = []\n",
    "    all_production_data = []\n",
    "    all_iso_codes = []\n",
    "    all_types = []\n",
    "\n",
    "    for unique_key, wpp in forecast_data.items():\n",
    "        all_turbine_types.append(str(wpp[\"Turbine\"]))\n",
    "        all_hub_heights.append(wpp[\"Hub_height\"])\n",
    "        all_capacities.append(wpp[\"Capacity\"])\n",
    "        all_commissioning_dates.append(f\"{wpp['Commissioning_date']}/06\" if isinstance(wpp[\"Commissioning_date\"], str) and \"/\" not in wpp[\"Commissioning_date\"] else wpp[\"Commissioning_date\"])\n",
    "        all_production_data.append(wpp[\"Time Series\"])\n",
    "        all_iso_codes.append(wpp[\"ISO_code\"])\n",
    "        all_types.append(wpp[\"Type\"])\n",
    "\n",
    "    # One-Hot-Encoding for turbine types\n",
    "    turbine_types_onehot = encoder.transform(np.array(all_turbine_types).reshape(-1, 1))\n",
    "\n",
    "    # convert to datetime\n",
    "    standardised_dates = pd.to_datetime(all_commissioning_dates, format='%Y/%m')\n",
    "\n",
    "    # calculate age\n",
    "    ref_date = pd.Timestamp(\"2024-12-01\")\n",
    "    ages = ref_date.year * 12 + ref_date.month - (standardised_dates.year * 12 + standardised_dates.month)\n",
    "\n",
    "    # create combined features and output lists\n",
    "    combined_features_all = []\n",
    "    output_raw = []\n",
    "    \n",
    "    # convert data in feature arrays\n",
    "    for idx, production_data in enumerate(all_production_data):\n",
    "        num_rows = len(production_data)\n",
    "\n",
    "        # Repetitions for common features\n",
    "        turbine_type_repeated = np.tile(turbine_types_onehot[idx], (num_rows, 1))\n",
    "        hub_height_repeated = np.full((num_rows, 1), all_hub_heights[idx])\n",
    "        capacity_repeated = np.full((num_rows, 1), all_capacities[idx])\n",
    "        age_repeated = np.full((num_rows, 1), ages[idx])\n",
    "        iso_codes_repeated = np.full((num_rows, 1), all_iso_codes[idx])\n",
    "        types_repeated = np.full((num_rows, 1), all_types[idx])\n",
    "\n",
    "        # Extract production values and wind speeds\n",
    "        production_values = np.array([entry[1] for entry in production_data]).reshape(-1, 1) / all_capacities[idx]\n",
    "        wind_speeds = np.array([entry[2] for entry in production_data]).reshape(-1, 1)\n",
    "\n",
    "        # combine all features\n",
    "        combined_chunk = np.hstack((\n",
    "            turbine_type_repeated,\n",
    "            hub_height_repeated,\n",
    "            capacity_repeated,\n",
    "            age_repeated,\n",
    "            iso_codes_repeated,\n",
    "            types_repeated,\n",
    "            wind_speeds\n",
    "        ))\n",
    "\n",
    "        # add the data\n",
    "        combined_features_all.append(combined_chunk)\n",
    "        output_raw.append(production_values)\n",
    "\n",
    "    # combine all data chunks to one array\n",
    "    combined_features_all = np.vstack(combined_features_all)\n",
    "    output = np.vstack(output_raw)\n",
    "\n",
    "    # Interpolate missing values (linear interpolation) in pandas\n",
    "    wind_speed_series = pd.Series(combined_features_all[:, -1]).astype(float)\n",
    "    wind_speed_series = wind_speed_series.infer_objects(copy=False)\n",
    "    wind_speed_series.interpolate(method='linear', inplace=True)\n",
    "    combined_features_all[:, -1] = wind_speed_series.to_numpy()\n",
    "        \n",
    "    # Skalieren der einzelnen Features\n",
    "    combined_features = np.hstack((\n",
    "        combined_features_all[:, 0:len(encoder.categories_[0])].astype(float),\n",
    "        scalers_[\"hub_heights\"].transform(combined_features_all[:, -6].astype(float).reshape(-1, 1)),\n",
    "        scalers_[\"ages\"].transform(combined_features_all[:, -4].astype(float).reshape(-1, 1)),\n",
    "        scalers_[\"winds\"].transform(combined_features_all[:, -1].astype(float).reshape(-1, 1))\n",
    "    ))\n",
    "\n",
    "    # round all values to four decimal places\n",
    "    combined_features = np.round(combined_features, decimals=4)\n",
    "    output = np.round(output, decimals=4)\n",
    "    \n",
    "    # use static instead of dynamic computational graphs\n",
    "    model = torch.jit.script(MLP(input_size=input_size))\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Testen\n",
    "    print(f\"    Testing\")\n",
    "    model.eval()\n",
    "\n",
    "    # Extract features from test_dataset_all\n",
    "    test_features_all = combined_features_all[test_indices_]\n",
    "    test_features = combined_features[test_indices_]\n",
    "    test_output = output[test_indices_]\n",
    "    \n",
    "    ##################### 1. Model testing by groups in certain categories ##################\n",
    "\n",
    "    # Indices for categorical features\n",
    "    group_indices = {\n",
    "        \"turbine type\": list(range(len(encoder.categories_[0]))),  # Multiple indices for one-hot encoding\n",
    "        \"hub height\": len(encoder.categories_[0]),\n",
    "        \"capacity\": len(encoder.categories_[0]) + 1,\n",
    "        \"age\": len(encoder.categories_[0]) + 2,\n",
    "        \"iso code\": len(encoder.categories_[0]) + 3,\n",
    "        \"type\": len(encoder.categories_[0]) + 4,\n",
    "        \"wind speed\": len(encoder.categories_[0]) + 5\n",
    "    }\n",
    "    \n",
    "    interval_length = 1.0  # Binning for continuous wind speeds\n",
    "\n",
    "    for testing_group, index in group_indices.items():\n",
    "\n",
    "        if testing_group == \"turbine type\":\n",
    "            unique_values = encoder.categories_[0]  # Get actual turbine type names\n",
    "        elif testing_group == \"wind speed\":\n",
    "            unique_values = np.unique(np.floor(test_features_all[:, index].astype(float) / interval_length) * interval_length) # as an exception, search for unique values in test_features_all and not combined_features_all in this category of \"wind speeds\", because for high extremely wind speeds, it may be that a value only occurs in the latter and not the first\n",
    "        else:\n",
    "            unique_values = np.unique(combined_features_all[:, index])  # Normal numerical features\n",
    "\n",
    "        group_results = {}\n",
    "\n",
    "        for i, value in enumerate(unique_values):\n",
    "\n",
    "            if testing_group == \"turbine type\":\n",
    "                mask = test_features_all[:, index[i]].astype(float) == 1 # convert <U32 type of column of test_features_all to float, because \"value\" (0 and 1) has same data type\n",
    "            elif testing_group == \"wind speed\":\n",
    "                mask = (np.floor(test_features_all[:, index].astype(float) / interval_length) * interval_length) == value  # convert <U32 type of column of test_features_all to float, because <U32 data type doesn't allow division\n",
    "            else:\n",
    "                mask = test_features_all[:, index] == value # keep <U32 type of column of test_features_all, because \"value\" has same data type\n",
    "\n",
    "            test_features_filtered = test_features[mask]\n",
    "            test_output_filtered = test_output[mask]\n",
    "\n",
    "            # Convert to PyTorch Dataset\n",
    "            filtered_test_dataset = WindPowerDataset(test_features_filtered, test_output_filtered)\n",
    "\n",
    "            # shuffling doesn't matter here, has already taken place during random_split, in the same fashion as during training of the model\n",
    "            filtered_test_loader = DataLoader(filtered_test_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "            test_loss_mae, test_loss_mse, test_loss_huber = 0, 0, 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in filtered_test_loader:\n",
    "                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                    preds = model(batch_x)\n",
    "\n",
    "                    test_loss_mae += mae_criterion(preds, batch_y).item() # average loss of a single batch\n",
    "                    test_loss_mse += mse_criterion(preds, batch_y).item()\n",
    "                    test_loss_huber += huber_criterion(preds, batch_y).item()\n",
    "\n",
    "            # Compute final test losses\n",
    "            test_loss_mae /= len(filtered_test_loader)\n",
    "            test_loss_mse /= len(filtered_test_loader)\n",
    "            test_loss_huber /= len(filtered_test_loader)\n",
    "\n",
    "            group_results[value] = {\n",
    "                \"Huber\": test_loss_huber,\n",
    "                \"MAE\": test_loss_mae,\n",
    "                \"MSE\": test_loss_mse,\n",
    "                \"RMSE\": np.sqrt(test_loss_mse),\n",
    "                \"Share\": len(filtered_test_dataset) / len(test_dataset)\n",
    "            }\n",
    "\n",
    "        # Store per group results\n",
    "        metrics[lead_time][testing_group] = group_results\n",
    "\n",
    "    ######################## 2. Determination of standard deviation and bias ####################\n",
    "    ######################## 3. Creation of vector with itemised production and outputs for histograms ####################\n",
    "\n",
    "    test_dataset = WindPowerDataset(test_features, test_output)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    test_loss_mae, test_loss_mse, test_loss_huber = 0, 0, 0\n",
    "    outputs_itemised = []\n",
    "    predictions_itemised = []\n",
    "    all_residuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            preds = model(batch_x)\n",
    "            \n",
    "            test_loss_mae += mae_criterion(preds, batch_y).item()\n",
    "            test_loss_mse += mse_criterion(preds, batch_y).item()\n",
    "            test_loss_huber += huber_criterion(preds, batch_y).item()\n",
    "\n",
    "            outputs_itemised.append(batch_y.detach().numpy())\n",
    "            predictions_itemised.append(preds.detach().numpy())\n",
    "            all_residuals.append(batch_y.cpu().numpy() - preds.cpu().numpy())\n",
    "\n",
    "    # Compute final test losses\n",
    "    test_loss_mae /= len(test_loader)\n",
    "    test_loss_mse /= len(test_loader)\n",
    "    test_loss_huber /= len(test_loader)\n",
    "\n",
    "    # Flatten residuals into a single array\n",
    "    all_residuals = np.concatenate(all_residuals)\n",
    "\n",
    "    # Compute bias (mean error)\n",
    "    bias = np.mean(all_residuals)\n",
    "\n",
    "    # Compute standard deviation of residuals\n",
    "    std_ddof0 = np.std(all_residuals, ddof=0)\n",
    "    std_ddof1 = np.std(all_residuals, ddof=1)\n",
    "\n",
    "    metrics_detailed[lead_time] = {\n",
    "        \"Huber\": test_loss_huber,\n",
    "        \"MAE\": test_loss_mae,\n",
    "        \"MSE\": test_loss_mse,\n",
    "        \"RMSE_1\": np.sqrt(test_loss_mse),\n",
    "        \"RMSE_2\": np.sqrt(np.mean(all_residuals**2)),\n",
    "        \"MBE\": bias,\n",
    "        \"STD_0\": std_ddof0,\n",
    "        \"STD_1\": std_ddof1\n",
    "    }\n",
    "\n",
    "    all_values_itemised[lead_time] = {\n",
    "        \"outputs\": np.vstack(outputs_itemised),\n",
    "        \"predictions\": np.vstack(predictions_itemised)\n",
    "    }\n",
    "\n",
    "\n",
    "#joblib.dump(metrics, \"metrics_per_attribute/metrics.pkl\")\n",
    "joblib.dump(metrics_detailed, \"metrics_per_attribute/metrics_detailed.pkl\")\n",
    "joblib.dump(all_values_itemised, \"metrics_per_attribute/all_values_itemised.pkl\")\n",
    "print(\"Metrics saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Lead Time     Huber       MAE       MSE      RMSE\n",
      "0           0  0.014760  0.118425  0.029519  0.171812\n",
      "22          3  0.015321  0.120778  0.030652  0.175078\n",
      "33          6  0.014424  0.117802  0.028848  0.169848\n",
      "44          9  0.014559  0.118628  0.029119  0.170643\n",
      "7          12  0.015325  0.121273  0.030650  0.175072\n",
      "17         15  0.016178  0.125622  0.032363  0.179897\n",
      "18         18  0.015578  0.124113  0.031156  0.176510\n",
      "19         21  0.015612  0.123889  0.031225  0.176707\n",
      "20         24  0.016360  0.126647  0.032721  0.180889\n",
      "21         27  0.017199  0.130842  0.034410  0.185500\n",
      "23         30  0.016690  0.129421  0.033380  0.182703\n",
      "24         33  0.016514  0.128883  0.033028  0.181735\n",
      "25         36  0.017870  0.134037  0.035740  0.189051\n",
      "26         39  0.018630  0.138081  0.037274  0.193064\n",
      "27         42  0.018375  0.137211  0.036750  0.191703\n",
      "28         45  0.018390  0.137886  0.036779  0.191779\n",
      "29         48  0.019119  0.140323  0.038237  0.195544\n",
      "30         51  0.020311  0.146229  0.040639  0.201591\n",
      "31         54  0.019812  0.143980  0.039624  0.199058\n",
      "32         57  0.020298  0.146772  0.040596  0.201485\n",
      "34         60  0.021367  0.149863  0.042735  0.206724\n",
      "35         63  0.022399  0.155440  0.044807  0.211676\n",
      "36         66  0.021877  0.153815  0.043753  0.209173\n",
      "37         69  0.022930  0.157758  0.045859  0.214148\n",
      "38         72  0.024071  0.162488  0.048142  0.219414\n",
      "39         75  0.024890  0.166915  0.049796  0.223150\n",
      "40         78  0.024862  0.167042  0.049724  0.222989\n",
      "41         81  0.025761  0.170264  0.051522  0.226985\n",
      "42         84  0.026621  0.173444  0.053242  0.230743\n",
      "43         87  0.027207  0.176632  0.054429  0.233301\n",
      "45         90  0.026752  0.175646  0.053505  0.231311\n",
      "46         93  0.027947  0.179845  0.055894  0.236419\n",
      "47         96  0.028388  0.180967  0.056776  0.238277\n",
      "48         99  0.028548  0.183038  0.057118  0.238993\n",
      "1         102  0.028694  0.184051  0.057387  0.239557\n",
      "2         105  0.029790  0.187942  0.059580  0.244090\n",
      "3         108  0.030629  0.190844  0.061259  0.247506\n",
      "4         111  0.031033  0.193274  0.062081  0.249161\n",
      "5         114  0.031395  0.194833  0.062790  0.250580\n",
      "6         117  0.032431  0.198331  0.064862  0.254680\n",
      "8         120  0.034645  0.206866  0.069289  0.263228\n",
      "9         123  0.035897  0.212407  0.071804  0.267963\n",
      "10        126  0.035306  0.210465  0.070612  0.265729\n",
      "11        129  0.035991  0.212679  0.071983  0.268296\n",
      "12        132  0.036855  0.215583  0.073710  0.271496\n",
      "13        135  0.037576  0.218572  0.075167  0.274167\n",
      "14        138  0.036955  0.216483  0.073911  0.271865\n",
      "15        141  0.037840  0.220521  0.075679  0.275099\n",
      "16        144  0.038554  0.223727  0.077107  0.277682\n",
      "Updated HTML file 'documentation.html' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the metrics from the file\n",
    "metrics = joblib.load(\"parameters/metrics.pkl\")\n",
    "\n",
    "# Convert the dictionary into a structured DataFrame with lead time as a column\n",
    "training_metrics = []\n",
    "\n",
    "for lead_time, value in metrics.items():\n",
    "    row = {\"Lead Time\": lead_time}  # Store lead time as a column\n",
    "    row.update(value[\"Training\"])  # Merge the metric values into the row\n",
    "    training_metrics.append(row)\n",
    "\n",
    "# Convert to a Pandas DataFrame\n",
    "df = pd.DataFrame(training_metrics)\n",
    "\n",
    "# Sort by Lead Time (ascending)\n",
    "df = df.sort_values(by=\"Lead Time\")\n",
    "\n",
    "# Print the DataFrame to verify\n",
    "print(df)\n",
    "\n",
    "# Generate the table HTML\n",
    "table_html = df.to_html(index=False, classes=\"styled-table\")\n",
    "\n",
    "# Read the existing HTML file\n",
    "with open(\"documentation.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "    existing_html = f.read()\n",
    "\n",
    "# Find the placeholder div in the existing HTML\n",
    "placeholder = '<div id=\"results-table\"></div>'\n",
    "\n",
    "# Replace the placeholder with the actual table\n",
    "new_html = existing_html.replace(placeholder, table_html)\n",
    "\n",
    "# Save the updated HTML file\n",
    "with open(\"documentation.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(new_html)\n",
    "\n",
    "print(\"Updated HTML file 'documentation.html' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ..., -0.5575, -0.0858,  0.0059],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.8165,  2.5522, -1.1907],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.9100,  0.1068, -0.5667],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.2410, -0.1821, -1.1701],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0108,  1.4354, -0.6591],\n",
       "        [ 1.0000,  0.0000,  0.0000,  ..., -0.9891,  0.2223, -0.6566]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webapp_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
