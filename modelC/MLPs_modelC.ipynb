{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lead time: 0\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 102\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 105\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 108\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 111\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 114\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 117\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 12\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 120\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 123\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 126\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 129\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 132\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 135\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 138\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 141\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 144\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 15\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 18\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 21\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 24\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 27\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 3\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 30\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 33\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 36\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 39\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 42\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 45\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 48\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 51\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 54\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 57\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 6\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 60\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 63\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 66\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 69\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 72\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 75\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 78\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 81\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 84\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 87\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 9\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 90\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 93\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 96\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "Processing lead time: 99\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "    Testing\n",
      "All parameters saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.nn import HuberLoss, MSELoss, L1Loss\n",
    "\n",
    "# Lists to store models and scalers\n",
    "models = {}\n",
    "scalers = {}\n",
    "encoders = {}\n",
    "input_sizes = {}\n",
    "metrics = {}\n",
    "test_indices = {}\n",
    "\n",
    "# Define MLP class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3366)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# PyTorch Dataset Class\n",
    "class WindPowerDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.targets[index]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "input_dir = r\"E:\\MA_data\\WPPs+production+reforecast\"\n",
    "\n",
    "for file in os.listdir(input_dir):\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    if os.path.isfile(file_path):  # Ensure it's a file (not a folder)\n",
    "        lead_time = int(file.split(\"_\")[-1].replace(\".json\", \"\"))\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            forecast_data = json.load(file)\n",
    "\n",
    "    print(f\"Processing lead time: {lead_time}\")\n",
    "\n",
    "    print(f\"    Data preparation\")\n",
    "\n",
    "    all_turbine_types = []\n",
    "    all_hub_heights = []\n",
    "    all_capacities = []\n",
    "    all_commissioning_dates = []\n",
    "    all_production_data = []\n",
    "\n",
    "    for unique_key, wpp in forecast_data.items():\n",
    "        all_turbine_types.append(str(wpp[\"Turbine\"]))\n",
    "        all_hub_heights.append(wpp[\"Hub_height\"])\n",
    "        all_capacities.append(wpp[\"Capacity\"])\n",
    "        all_commissioning_dates.append(f\"{wpp['Commissioning_date']}/06\" if isinstance(wpp[\"Commissioning_date\"], str) and \"/\" not in wpp[\"Commissioning_date\"] else wpp[\"Commissioning_date\"])\n",
    "        all_production_data.append(wpp[\"Time Series\"])\n",
    "\n",
    "    # One-Hot-Encoding for turbine types\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    turbine_types_onehot = encoder.fit_transform(np.array(all_turbine_types).reshape(-1, 1))\n",
    "\n",
    "    # convert to datetime\n",
    "    standardised_dates = pd.to_datetime(all_commissioning_dates, format='%Y/%m')\n",
    "\n",
    "    # calculate age\n",
    "    ref_date = pd.Timestamp(\"2024-12-01\")\n",
    "    ages = ref_date.year * 12 + ref_date.month - (standardised_dates.year * 12 + standardised_dates.month)\n",
    "\n",
    "    # create combined features and output lists\n",
    "    combined_features_raw = []\n",
    "    output_raw = []\n",
    "    \n",
    "    # convert data in feature arrays\n",
    "    for idx, production_data in enumerate(all_production_data):\n",
    "        num_rows = len(production_data)\n",
    "\n",
    "        # Repetitions for common features\n",
    "        turbine_type_repeated = np.tile(turbine_types_onehot[idx], (num_rows, 1))\n",
    "        hub_height_repeated = np.full((num_rows, 1), float(all_hub_heights[idx]))\n",
    "        age_repeated = np.full((num_rows, 1), ages[idx])\n",
    "\n",
    "        # Extract production values and wind speeds\n",
    "        production_values = np.array([entry[1] for entry in production_data]).reshape(-1, 1) / all_capacities[idx]\n",
    "        wind_speeds = np.array([entry[2] for entry in production_data]).reshape(-1, 1)\n",
    "\n",
    "        # combine all features\n",
    "        combined_chunk = np.hstack((\n",
    "            turbine_type_repeated,\n",
    "            hub_height_repeated,\n",
    "            age_repeated,\n",
    "            wind_speeds\n",
    "        ))\n",
    "\n",
    "        # add the data\n",
    "        combined_features_raw.append(combined_chunk)\n",
    "        output_raw.append(production_values)\n",
    "\n",
    "    # combine all data chunks to one array\n",
    "    combined_features = np.vstack(combined_features_raw)\n",
    "    output = np.vstack(output_raw)\n",
    "\n",
    "    # Interpolate missing values (linear interpolation) in pandas\n",
    "    wind_speed_series = pd.Series(combined_features[:, -1])\n",
    "    wind_speed_series.interpolate(method='linear', inplace=True)\n",
    "    combined_features[:, -1] = wind_speed_series.to_numpy()\n",
    "\n",
    "    # round all values to four decimal places\n",
    "    combined_features = np.round(combined_features, decimals=4)\n",
    "    output = np.round(output, decimals=4)\n",
    "        \n",
    "    # Normalise numerical features\n",
    "    scaler_wind = StandardScaler()\n",
    "    scaler_ages = StandardScaler()\n",
    "    scaler_hub_heights = StandardScaler()\n",
    "\n",
    "    # Skalieren der einzelnen Features\n",
    "    combined_features[:, -1] = scaler_wind.fit_transform(combined_features[:, -1].reshape(-1, 1)).flatten() # scale wind speeds\n",
    "    combined_features[:, -2] = scaler_ages.fit_transform(combined_features[:, -2].reshape(-1, 1)).flatten()  # scale ages\n",
    "    combined_features[:, -3] = scaler_hub_heights.fit_transform(combined_features[:, -3].reshape(-1, 1)).flatten()  # scale hub heights\n",
    "    \n",
    "    # Convert to PyTorch Dataset\n",
    "    dataset = WindPowerDataset(combined_features, output)\n",
    "    \n",
    "    params = {\"batch_size\": 128,\n",
    "              \"lr\": 0.00010155,\n",
    "              \"number_epochs\": 10}\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Train-test split\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    torch.manual_seed(0)\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    # shuffling doesn't matter here, has already taken place during random_split\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "    # Model setup\n",
    "    input_size = combined_features.shape[1]\n",
    "\n",
    "    # use static instead of dynamic computational graphs\n",
    "    model = torch.jit.script(MLP(input_size=input_size)).to(device)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Trainings-Konfiguration\n",
    "    mae_criterion = L1Loss()\n",
    "    mse_criterion = MSELoss()\n",
    "    huber_criterion = HuberLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "\n",
    "    # Training\n",
    "    print(f\"    Training\")\n",
    "    for epoch in range(params[\"number_epochs\"]):\n",
    "        print(f\"        Epoch {epoch + 1}/{params['number_epochs']}\")\n",
    "        model.train()\n",
    "        train_loss_mae, train_loss_mse, train_loss_huber = 0, 0, 0\n",
    "\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            \n",
    "            # Calculate metrics for each criterion\n",
    "            loss_mae = mae_criterion(outputs, batch_y)\n",
    "            loss_mse = mse_criterion(outputs, batch_y)\n",
    "            loss_huber = huber_criterion(outputs, batch_y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_huber.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate metrics for logging\n",
    "            train_loss_mae += loss_mae.item()\n",
    "            train_loss_mse += loss_mse.item()\n",
    "            train_loss_huber += loss_huber.item()\n",
    "\n",
    "        train_loss_mae /= len(train_loader)\n",
    "        train_loss_mse /= len(train_loader)\n",
    "        train_loss_huber /= len(train_loader)\n",
    "\n",
    "    # Testen\n",
    "    print(f\"    Testing\")\n",
    "    model.eval()\n",
    "\n",
    "    test_loss_mae, test_loss_mse, test_loss_huber = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            preds = model(batch_x)\n",
    "            \n",
    "            test_loss_mae += mae_criterion(preds, batch_y).item()\n",
    "            test_loss_mse += mse_criterion(preds, batch_y).item()\n",
    "            test_loss_huber += huber_criterion(preds, batch_y).item()\n",
    "\n",
    "    test_loss_mae /= len(test_loader)\n",
    "    test_loss_mse /= len(test_loader)\n",
    "    test_loss_huber /= len(test_loader)\n",
    "    \n",
    "    models[lead_time] = model.state_dict()\n",
    "    \n",
    "    scalers[lead_time] = {\n",
    "        \"winds\": scaler_wind,\n",
    "        \"ages\": scaler_ages,\n",
    "        \"hub_heights\": scaler_hub_heights\n",
    "    }\n",
    "\n",
    "    encoders[lead_time] = encoder\n",
    "\n",
    "    input_sizes[lead_time] = input_size\n",
    "\n",
    "    metrics[lead_time] = {\n",
    "        \"Training\": {\n",
    "            \"Huber\": train_loss_huber,\n",
    "            \"MAE\": train_loss_mae,\n",
    "            \"MSE\":train_loss_mse,\n",
    "            \"RMSE\": np.sqrt(train_loss_mse)\n",
    "        },\n",
    "        \"Testing\": {\n",
    "            \"Huber\": test_loss_huber,\n",
    "            \"MAE\": test_loss_mae,\n",
    "            \"MSE\": test_loss_mse,\n",
    "            \"RMSE\": np.sqrt(test_loss_mse)\n",
    "        },\n",
    "    }\n",
    "\n",
    "    test_indices[lead_time] = test_dataset.indices\n",
    "\n",
    "# Save all parameters\n",
    "torch.save(models, \"parameters/models.pth\")\n",
    "joblib.dump(scalers, \"parameters/scalers.pkl\")\n",
    "joblib.dump(encoders, \"parameters/encoders.pkl\")\n",
    "joblib.dump(input_sizes, \"parameters/input_sizes.pkl\")\n",
    "joblib.dump(metrics, \"parameters/metrics.pkl\")\n",
    "joblib.dump(test_indices, \"parameters/test_indices.pkl\")\n",
    "print(\"All parameters saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model training for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lead time: 0\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 102\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 105\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 108\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 111\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 114\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 117\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 12\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 120\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 123\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 126\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 129\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 132\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 135\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 138\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 141\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 144\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 15\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 18\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 21\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 24\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 27\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 3\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 30\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 33\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 36\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 39\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 42\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 45\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 48\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 51\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 54\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 57\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 6\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 60\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 63\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 66\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 69\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 72\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 75\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 78\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 81\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 84\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 87\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 9\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 90\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 93\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 96\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "Processing lead time: 99\n",
      "    Data preparation\n",
      "    Training\n",
      "        Epoch 1/10\n",
      "        Epoch 2/10\n",
      "        Epoch 3/10\n",
      "        Epoch 4/10\n",
      "        Epoch 5/10\n",
      "        Epoch 6/10\n",
      "        Epoch 7/10\n",
      "        Epoch 8/10\n",
      "        Epoch 9/10\n",
      "        Epoch 10/10\n",
      "All parameters saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.nn import HuberLoss, MSELoss, L1Loss\n",
    "\n",
    "# Lists to store models and scalers\n",
    "models = {}\n",
    "scalers = {}\n",
    "encoders = {}\n",
    "input_sizes = {}\n",
    "metrics = {}\n",
    "\n",
    "# Define MLP class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3366)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# PyTorch Dataset Class\n",
    "class WindPowerDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.targets[index]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "input_dir = r\"E:\\MA_data\\WPPs+production+reforecast\"\n",
    "\n",
    "for file in os.listdir(input_dir):\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    if os.path.isfile(file_path):  # Ensure it's a file (not a folder)\n",
    "        lead_time = int(file.split(\"_\")[-1].replace(\".json\", \"\"))\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            forecast_data = json.load(file)\n",
    "\n",
    "    print(f\"Processing lead time: {lead_time}\")\n",
    "\n",
    "    print(f\"    Data preparation\")\n",
    "\n",
    "    all_turbine_types = []\n",
    "    all_hub_heights = []\n",
    "    all_capacities = []\n",
    "    all_commissioning_dates = []\n",
    "    all_production_data = []\n",
    "\n",
    "    for unique_key, wpp in forecast_data.items():\n",
    "        all_turbine_types.append(str(wpp[\"Turbine\"]))\n",
    "        all_hub_heights.append(wpp[\"Hub_height\"])\n",
    "        all_capacities.append(wpp[\"Capacity\"])\n",
    "        all_commissioning_dates.append(f\"{wpp['Commissioning_date']}/06\" if isinstance(wpp[\"Commissioning_date\"], str) and \"/\" not in wpp[\"Commissioning_date\"] else wpp[\"Commissioning_date\"])\n",
    "        all_production_data.append(wpp[\"Time Series\"])\n",
    "\n",
    "    # One-Hot-Encoding for turbine types\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    turbine_types_onehot = encoder.fit_transform(np.array(all_turbine_types).reshape(-1, 1))\n",
    "\n",
    "    # convert to datetime\n",
    "    standardised_dates = pd.to_datetime(all_commissioning_dates, format='%Y/%m')\n",
    "\n",
    "    # calculate age\n",
    "    ref_date = pd.Timestamp(\"2024-12-01\")\n",
    "    ages = ref_date.year * 12 + ref_date.month - (standardised_dates.year * 12 + standardised_dates.month)\n",
    "\n",
    "    # create combined features and output lists\n",
    "    combined_features_raw = []\n",
    "    output_raw = []\n",
    "    \n",
    "    # convert data in feature arrays\n",
    "    for idx, production_data in enumerate(all_production_data):\n",
    "        num_rows = len(production_data)\n",
    "\n",
    "        # Repetitions for common features\n",
    "        turbine_type_repeated = np.tile(turbine_types_onehot[idx], (num_rows, 1))\n",
    "        hub_height_repeated = np.full((num_rows, 1), float(all_hub_heights[idx]))\n",
    "        age_repeated = np.full((num_rows, 1), ages[idx])\n",
    "\n",
    "        # Extract production values and wind speeds\n",
    "        production_values = np.array([entry[1] for entry in production_data]).reshape(-1, 1) / all_capacities[idx]\n",
    "        wind_speeds = np.array([entry[2] for entry in production_data]).reshape(-1, 1)\n",
    "\n",
    "        # combine all features\n",
    "        combined_chunk = np.hstack((\n",
    "            turbine_type_repeated,\n",
    "            hub_height_repeated,\n",
    "            age_repeated,\n",
    "            wind_speeds\n",
    "        ))\n",
    "\n",
    "        # add the data\n",
    "        combined_features_raw.append(combined_chunk)\n",
    "        output_raw.append(production_values)\n",
    "\n",
    "    # combine all data chunks to one array\n",
    "    combined_features = np.vstack(combined_features_raw)\n",
    "    output = np.vstack(output_raw)\n",
    "\n",
    "    # Interpolate missing values (linear interpolation) in pandas\n",
    "    wind_speed_series = pd.Series(combined_features[:, -1])\n",
    "    wind_speed_series.interpolate(method='linear', inplace=True)\n",
    "    combined_features[:, -1] = wind_speed_series.to_numpy()\n",
    "\n",
    "    # round all values to four decimal places\n",
    "    combined_features = np.round(combined_features, decimals=4)\n",
    "    output = np.round(output, decimals=4)\n",
    "        \n",
    "    # Normalise numerical features\n",
    "    scaler_wind = StandardScaler()\n",
    "    scaler_ages = StandardScaler()\n",
    "    scaler_hub_heights = StandardScaler()\n",
    "\n",
    "    # Skalieren der einzelnen Features\n",
    "    combined_features[:, -1] = scaler_wind.fit_transform(combined_features[:, -1].reshape(-1, 1)).flatten() # scale wind speeds\n",
    "    combined_features[:, -2] = scaler_ages.fit_transform(combined_features[:, -2].reshape(-1, 1)).flatten()  # scale ages\n",
    "    combined_features[:, -3] = scaler_hub_heights.fit_transform(combined_features[:, -3].reshape(-1, 1)).flatten()  # scale hub heights\n",
    "    \n",
    "    # Convert to PyTorch Dataset\n",
    "    dataset = WindPowerDataset(combined_features, output)\n",
    "    \n",
    "    params = {\"batch_size\": 128,\n",
    "              \"lr\": 0.00010155,\n",
    "              \"number_epochs\": 10}\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # shuffling matters here\n",
    "    data_loader = DataLoader(dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    \n",
    "    # Model setup\n",
    "    input_size = combined_features.shape[1]\n",
    "\n",
    "    # use static instead of dynamic computational graphs\n",
    "    model = torch.jit.script(MLP(input_size=input_size)).to(device)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Trainings-Konfiguration\n",
    "    mae_criterion = L1Loss()\n",
    "    mse_criterion = MSELoss()\n",
    "    huber_criterion = HuberLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "\n",
    "    # Training\n",
    "    print(f\"    Training\")\n",
    "    for epoch in range(params[\"number_epochs\"]):\n",
    "        print(f\"        Epoch {epoch + 1}/{params['number_epochs']}\")\n",
    "        model.train()\n",
    "        train_loss_mae, train_loss_mse, train_loss_huber = 0, 0, 0\n",
    "\n",
    "        for batch_x, batch_y in data_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            \n",
    "            # Calculate metrics for each criterion\n",
    "            loss_mae = mae_criterion(outputs, batch_y)\n",
    "            loss_mse = mse_criterion(outputs, batch_y)\n",
    "            loss_huber = huber_criterion(outputs, batch_y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_huber.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate metrics for logging\n",
    "            train_loss_mae += loss_mae.item()\n",
    "            train_loss_mse += loss_mse.item()\n",
    "            train_loss_huber += loss_huber.item()\n",
    "\n",
    "        train_loss_mae /= len(data_loader)\n",
    "        train_loss_mse /= len(data_loader)\n",
    "        train_loss_huber /= len(data_loader)\n",
    "    \n",
    "    models[lead_time] = model.state_dict()\n",
    "    \n",
    "    scalers[lead_time] = {\n",
    "        \"winds\": scaler_wind,\n",
    "        \"ages\": scaler_ages,\n",
    "        \"hub_heights\": scaler_hub_heights\n",
    "    }\n",
    "\n",
    "    encoders[lead_time] = encoder\n",
    "\n",
    "    input_sizes[lead_time] = input_size\n",
    "\n",
    "    metrics[lead_time] = {\n",
    "        \"Huber\": train_loss_huber,\n",
    "        \"MAE\": train_loss_mae,\n",
    "        \"MSE\":train_loss_mse,\n",
    "        \"RMSE\": np.sqrt(train_loss_mse)\n",
    "    }\n",
    "\n",
    "# Save all parameters\n",
    "torch.save(models, \"parameters_deployment/models.pth\")\n",
    "joblib.dump(scalers, \"parameters_deployment/scalers.pkl\")\n",
    "joblib.dump(encoders, \"parameters_deployment/encoders.pkl\")\n",
    "joblib.dump(input_sizes, \"parameters_deployment/input_sizes.pkl\")\n",
    "joblib.dump(metrics, \"parameters_deployment/metrics.pkl\")\n",
    "print(\"All parameters saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Check if input sizes and encoders are the same for all lead times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model C for testing\n",
      " All lead times have the same input size: 56\n",
      " All encoders have identical categories across lead times.\n",
      "Model C for deployment\n",
      " All lead times have the same input size: 56\n",
      " All encoders have identical categories across lead times.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "print(\"Model C for testing\")\n",
    "\n",
    "input_sizes = joblib.load(\"parameters/input_sizes.pkl\")\n",
    "encoders = joblib.load(\"parameters/encoders.pkl\")\n",
    "\n",
    "# Check if all input sizes are the same for all lead times\n",
    "unique_input_sizes = set(input_sizes.values())\n",
    "\n",
    "if len(unique_input_sizes) == 1:\n",
    "    print(f\" All lead times have the same input size: {unique_input_sizes.pop()}\")\n",
    "else:\n",
    "    print(f\" Different input sizes detected: {unique_input_sizes}\")\n",
    "\n",
    "# Check if all encoders have the same categories\n",
    "encoder_refs = list(encoders.values())\n",
    "first_encoder_categories = encoder_refs[0].categories_\n",
    "\n",
    "same_encoders = all(\n",
    "    all((cat1 == cat2).all() for cat1, cat2 in zip(enc.categories_, first_encoder_categories))\n",
    "    for enc in encoder_refs\n",
    ")\n",
    "\n",
    "if same_encoders:\n",
    "    print(\" All encoders have identical categories across lead times.\")\n",
    "else:\n",
    "    print(\" Encoders have different categories across lead times!\")\n",
    "\n",
    "\n",
    "print(\"Model C for deployment\")\n",
    "\n",
    "input_sizes = joblib.load(\"parameters_deployment/input_sizes.pkl\")\n",
    "encoders = joblib.load(\"parameters_deployment/encoders.pkl\")\n",
    "\n",
    "# Check if all input sizes are the same for all lead times\n",
    "unique_input_sizes = set(input_sizes.values())\n",
    "\n",
    "if len(unique_input_sizes) == 1:\n",
    "    print(f\" All lead times have the same input size: {unique_input_sizes.pop()}\")\n",
    "else:\n",
    "    print(f\" Different input sizes detected: {unique_input_sizes}\")\n",
    "\n",
    "# Check if all encoders have the same categories\n",
    "encoder_refs = list(encoders.values())\n",
    "first_encoder_categories = encoder_refs[0].categories_\n",
    "\n",
    "same_encoders = all(\n",
    "    all((cat1 == cat2).all() for cat1, cat2 in zip(enc.categories_, first_encoder_categories))\n",
    "    for enc in encoder_refs\n",
    ")\n",
    "\n",
    "if same_encoders:\n",
    "    print(\" All encoders have identical categories across lead times.\")\n",
    "else:\n",
    "    print(\" Encoders have different categories across lead times!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model testing by attributes and determination of standard deviation and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lead time: 0\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 102\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 105\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 108\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 111\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 114\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 117\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 12\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 120\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 123\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 126\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 129\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 132\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 135\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 138\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 141\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 144\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 15\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 18\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 21\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 24\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 27\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 3\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 30\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 33\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 36\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 39\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 42\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 45\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 48\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 51\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 54\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 57\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 6\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 60\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 63\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 66\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 69\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 72\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 75\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 78\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 81\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 84\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 87\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 9\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 90\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 93\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 96\n",
      "    Data preparation\n",
      "    Testing\n",
      "Processing lead time: 99\n",
      "    Data preparation\n",
      "    Testing\n",
      "Metrics saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.nn import HuberLoss, MSELoss, L1Loss\n",
    "\n",
    "encoders = joblib.load(\"parameters/encoders.pkl\")\n",
    "input_sizes = joblib.load(\"parameters/input_sizes.pkl\")\n",
    "scalers = joblib.load(\"parameters/scalers.pkl\")\n",
    "test_indices = joblib.load(\"parameters/test_indices.pkl\") # same random_split as during training of the model, so that testing now only is done on unseen data \n",
    "model_state_dicts = torch.load(\"parameters/models.pth\", weights_only=True)\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "# Define MLP class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3366)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# PyTorch Dataset Class\n",
    "class WindPowerDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.targets[index]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "params = {\"batch_size\": 128,\n",
    "        \"lr\": 0.00010155,\n",
    "        \"number_epochs\": 10}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "mae_criterion = L1Loss()\n",
    "mse_criterion = MSELoss()\n",
    "huber_criterion = HuberLoss()\n",
    "\n",
    "input_dir = r\"E:\\MA_data\\WPPs+production+reforecast\"\n",
    "\n",
    "for file in os.listdir(input_dir):\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    if os.path.isfile(file_path):  # Ensure it's a file (not a folder)\n",
    "        lead_time = int(file.split(\"_\")[-1].replace(\".json\", \"\"))\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            forecast_data = json.load(file)\n",
    "\n",
    "    print(f\"Processing lead time: {lead_time}\")\n",
    "\n",
    "    print(f\"    Data preparation\")\n",
    "\n",
    "    metrics[lead_time] = {}\n",
    "\n",
    "    encoder = encoders[lead_time]\n",
    "    input_size = input_sizes[lead_time]\n",
    "    scalers_ = scalers[lead_time]\n",
    "    test_indices_ = test_indices[lead_time]\n",
    "    model_state_dict = model_state_dicts[lead_time]\n",
    "\n",
    "    all_turbine_types = []\n",
    "    all_hub_heights = []\n",
    "    all_capacities = []\n",
    "    all_commissioning_dates = []\n",
    "    all_production_data = []\n",
    "    all_iso_codes = []\n",
    "    all_types = []\n",
    "\n",
    "    for unique_key, wpp in forecast_data.items():\n",
    "        all_turbine_types.append(str(wpp[\"Turbine\"]))\n",
    "        all_hub_heights.append(wpp[\"Hub_height\"])\n",
    "        all_capacities.append(wpp[\"Capacity\"])\n",
    "        all_commissioning_dates.append(f\"{wpp['Commissioning_date']}/06\" if isinstance(wpp[\"Commissioning_date\"], str) and \"/\" not in wpp[\"Commissioning_date\"] else wpp[\"Commissioning_date\"])\n",
    "        all_production_data.append(wpp[\"Time Series\"])\n",
    "        all_iso_codes.append(wpp[\"ISO_code\"])\n",
    "        all_types.append(wpp[\"Type\"])\n",
    "\n",
    "    # One-Hot-Encoding for turbine types\n",
    "    turbine_types_onehot = encoder.transform(np.array(all_turbine_types).reshape(-1, 1))\n",
    "\n",
    "    # convert to datetime\n",
    "    standardised_dates = pd.to_datetime(all_commissioning_dates, format='%Y/%m')\n",
    "\n",
    "    # calculate age\n",
    "    ref_date = pd.Timestamp(\"2024-12-01\")\n",
    "    ages = ref_date.year * 12 + ref_date.month - (standardised_dates.year * 12 + standardised_dates.month)\n",
    "\n",
    "    # create combined features and output lists\n",
    "    combined_features_all = []\n",
    "    output_raw = []\n",
    "    \n",
    "    # convert data in feature arrays\n",
    "    for idx, production_data in enumerate(all_production_data):\n",
    "        num_rows = len(production_data)\n",
    "\n",
    "        # Repetitions for common features\n",
    "        turbine_type_repeated = np.tile(turbine_types_onehot[idx], (num_rows, 1))\n",
    "        hub_height_repeated = np.full((num_rows, 1), all_hub_heights[idx])\n",
    "        capacity_repeated = np.full((num_rows, 1), all_capacities[idx])\n",
    "        age_repeated = np.full((num_rows, 1), ages[idx])\n",
    "        iso_codes_repeated = np.full((num_rows, 1), all_iso_codes[idx])\n",
    "        types_repeated = np.full((num_rows, 1), all_types[idx])\n",
    "\n",
    "        # Extract production values and wind speeds\n",
    "        production_values = np.array([entry[1] for entry in production_data]).reshape(-1, 1) / all_capacities[idx]\n",
    "        wind_speeds = np.array([entry[2] for entry in production_data]).reshape(-1, 1)\n",
    "\n",
    "        # combine all features\n",
    "        combined_chunk = np.hstack((\n",
    "            turbine_type_repeated,\n",
    "            hub_height_repeated,\n",
    "            capacity_repeated,\n",
    "            age_repeated,\n",
    "            iso_codes_repeated,\n",
    "            types_repeated,\n",
    "            wind_speeds\n",
    "        ))\n",
    "\n",
    "        # add the data\n",
    "        combined_features_all.append(combined_chunk)\n",
    "        output_raw.append(production_values)\n",
    "\n",
    "    # combine all data chunks to one array\n",
    "    combined_features_all = np.vstack(combined_features_all)\n",
    "    output = np.vstack(output_raw)\n",
    "\n",
    "    # Interpolate missing values (linear interpolation) in pandas\n",
    "    wind_speed_series = pd.Series(combined_features_all[:, -1]).astype(float)\n",
    "    wind_speed_series = wind_speed_series.infer_objects(copy=False)\n",
    "    wind_speed_series.interpolate(method='linear', inplace=True)\n",
    "    combined_features_all[:, -1] = wind_speed_series.to_numpy()\n",
    "        \n",
    "    # Skalieren der einzelnen Features\n",
    "    combined_features = np.hstack((\n",
    "        combined_features_all[:, 0:len(encoder.categories_[0])].astype(float),\n",
    "        scalers_[\"hub_heights\"].transform(combined_features_all[:, -6].astype(float).reshape(-1, 1)),\n",
    "        scalers_[\"ages\"].transform(combined_features_all[:, -4].astype(float).reshape(-1, 1)),\n",
    "        scalers_[\"winds\"].transform(combined_features_all[:, -1].astype(float).reshape(-1, 1))\n",
    "    ))\n",
    "\n",
    "    # round all values to four decimal places\n",
    "    combined_features = np.round(combined_features, decimals=4)\n",
    "    output = np.round(output, decimals=4)\n",
    "\n",
    "    # Extract features from test_dataset_all\n",
    "    test_features_all = combined_features_all[test_indices_]\n",
    "    test_features = combined_features[test_indices_]\n",
    "    test_output = output[test_indices_]\n",
    "    \n",
    "    # use static instead of dynamic computational graphs\n",
    "    model = torch.jit.script(MLP(input_size=input_size))\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Testen\n",
    "    print(f\"    Testing\")\n",
    "    model.eval()\n",
    "\n",
    "    # Indices for categorical features\n",
    "    group_indices = {\n",
    "        \"turbine type\": list(range(len(encoder.categories_[0]))),  # Multiple indices for one-hot encoding\n",
    "        \"hub height\": len(encoder.categories_[0]),\n",
    "        \"capacity\": len(encoder.categories_[0]) + 1,\n",
    "        \"age\": len(encoder.categories_[0]) + 2,\n",
    "        \"iso code\": len(encoder.categories_[0]) + 3,\n",
    "        \"type\": len(encoder.categories_[0]) + 4,\n",
    "        \"wind speed\": len(encoder.categories_[0]) + 5\n",
    "    }\n",
    "\n",
    "    interval_length = 1.0  # Binning for continuous wind speeds\n",
    "\n",
    "    for testing_group, index in group_indices.items():\n",
    "\n",
    "        if testing_group == \"turbine type\":\n",
    "            unique_values = encoder.categories_[0]  # Get actual turbine type names\n",
    "        elif testing_group == \"wind speed\":\n",
    "            unique_values = np.unique(np.floor(test_features_all[:, index].astype(float) / interval_length) * interval_length) # as an exception, search for unique values in test_features_all and not combined_features_all in this category of \"wind speeds\", because for high extremely wind speeds, it may be that a value only occurs in the latter and not the first\n",
    "        else:\n",
    "            unique_values = np.unique(combined_features_all[:, index])  # Normal numerical features\n",
    "\n",
    "        group_results = {}\n",
    "\n",
    "        for i, value in enumerate(unique_values):\n",
    "\n",
    "            if testing_group == \"turbine type\":\n",
    "                mask = test_features_all[:, index[i]].astype(float) == 1 # convert <U32 type of column of test_features_all to float, because \"value\" (0 and 1) has same data type\n",
    "            elif testing_group == \"wind speed\":\n",
    "                mask = (np.floor(test_features_all[:, index].astype(float) / interval_length) * interval_length) == value  # convert <U32 type of column of test_features_all to float, because <U32 data type doesn't allow division\n",
    "            else:\n",
    "                mask = test_features_all[:, index] == value # keep <U32 type of column of test_features_all, because \"value\" has same data type\n",
    "\n",
    "            test_features_filtered = test_features[mask]\n",
    "            test_output_filtered = test_output[mask]\n",
    "\n",
    "            # Convert to PyTorch Dataset\n",
    "            filtered_test_dataset = WindPowerDataset(test_features_filtered, test_output_filtered)\n",
    "\n",
    "            # shuffling doesn't matter here, has already taken place during random_split, in the same fashion as during training of the model\n",
    "            filtered_test_loader = DataLoader(filtered_test_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "            test_loss_mae, test_loss_mse, test_loss_huber = 0, 0, 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in filtered_test_loader:\n",
    "                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                    preds = model(batch_x)\n",
    "\n",
    "                    test_loss_mae += mae_criterion(preds, batch_y).item() # average loss of a single batch\n",
    "                    test_loss_mse += mse_criterion(preds, batch_y).item()\n",
    "                    test_loss_huber += huber_criterion(preds, batch_y).item()\n",
    "\n",
    "            # Compute final test losses\n",
    "            test_loss_mae /= len(filtered_test_loader)\n",
    "            test_loss_mse /= len(filtered_test_loader)\n",
    "            test_loss_huber /= len(filtered_test_loader)\n",
    "\n",
    "            group_results[value] = {\n",
    "                \"Huber\": test_loss_huber,\n",
    "                \"MAE\": test_loss_mae,\n",
    "                \"MSE\": test_loss_mse,\n",
    "                \"RMSE\": np.sqrt(test_loss_mse),\n",
    "                \"Share\": len(filtered_test_dataset) / len(test_dataset)\n",
    "            }\n",
    "\n",
    "        # Store per group results\n",
    "        metrics[lead_time][testing_group] = group_results\n",
    "\n",
    "    # on all data: determine standard deviation and bias\n",
    "    test_dataset = WindPowerDataset(test_features, test_output)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    test_loss_mae, test_loss_mse, test_loss_huber = 0, 0, 0\n",
    "    all_residuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            preds = model(batch_x)\n",
    "\n",
    "            test_loss_mae += mae_criterion(preds, batch_y).item()\n",
    "            test_loss_mse += mse_criterion(preds, batch_y).item()\n",
    "            test_loss_huber += huber_criterion(preds, batch_y).item()\n",
    "\n",
    "            residuals = batch_y.cpu().numpy() - preds.cpu().numpy()\n",
    "            all_residuals.append(residuals)\n",
    "\n",
    "    # Compute final test losses\n",
    "    test_loss_mae /= len(test_loader)\n",
    "    test_loss_mse /= len(test_loader)\n",
    "    test_loss_huber /= len(test_loader)\n",
    "\n",
    "    # Flatten residuals into a single array\n",
    "    all_residuals = np.concatenate(all_residuals)\n",
    "\n",
    "    # Compute bias (mean error)\n",
    "    bias = np.mean(all_residuals)\n",
    "\n",
    "    # Compute standard deviation of residuals\n",
    "    std_ddof0 = np.std(all_residuals, ddof=0)\n",
    "    std_ddof1 = np.std(all_residuals, ddof=1)\n",
    "\n",
    "    metrics[lead_time][\"detailed\"] = {\n",
    "        \"Huber\": test_loss_huber,\n",
    "        \"MAE\": test_loss_mae,\n",
    "        \"MSE\": test_loss_mse,\n",
    "        \"RMSE_1\": np.sqrt(test_loss_mse),\n",
    "        \"RMSE_2\": np.sqrt(np.mean(all_residuals**2)),\n",
    "        \"MBE\": bias,\n",
    "        \"STD_0\": std_ddof0,\n",
    "        \"STD_1\": std_ddof1\n",
    "    }\n",
    "\n",
    "\n",
    "joblib.dump(metrics, \"metrics_per_attribute/metrics.pkl\")\n",
    "print(\"Metrics saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webapp_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
