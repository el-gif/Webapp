{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Build feature and output arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# lists for all data\n",
    "all_turbine_types = []\n",
    "all_hub_heights = []\n",
    "all_capacities = []\n",
    "all_commissioning_dates = []\n",
    "all_production_data = []\n",
    "\n",
    "with open(f\"data/WPPs+production+wind.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    WPP_production_wind = json.load(file)\n",
    "\n",
    "# collect data\n",
    "for wpp in WPP_production_wind:\n",
    "    all_turbine_types.append([wpp[\"Turbine\"] if pd.notna(wpp[\"Turbine\"]) else \"nan\" for wpp in WPP_production_wind])\n",
    "    all_hub_heights.append(wpp[\"Hub_height\"] if not pd.isna(wpp[\"Hub_height\"]) else 100)\n",
    "    all_capacities.append(wpp[\"Capacity\"])\n",
    "    all_commissioning_dates.append(\"2015/06\" if wpp[\"Commission_date\"] == \"nan\" else f\"{wpp['Commission_date']}/06\" if isinstance(wpp[\"Commission_date\"], str) and \"/\" not in wpp[\"Commission_date\"] else wpp[\"Commission_date\"])\n",
    "    all_production_data.append(wpp[\"Production\"])\n",
    "\n",
    "# One-Hot-Encoding for turbine types\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "turbine_types_onehot = encoder.fit_transform(np.array(all_turbine_types).reshape(-1, 1))\n",
    "\n",
    "# convert to datetime\n",
    "standardised_dates = pd.to_datetime(all_commissioning_dates, format='%Y/%m')\n",
    "\n",
    "# calculate age\n",
    "current_date = pd.Timestamp(\"2024-12-01\")\n",
    "ages = current_date.year * 12 + current_date.month - (standardised_dates.year * 12 + standardised_dates.month)\n",
    "\n",
    "# create combined features and output lists\n",
    "combined_features_raw = []\n",
    "output_raw = []\n",
    "\n",
    "# convert data in feature arrays\n",
    "for idx, production_data in enumerate(all_production_data):\n",
    "    num_rows = len(production_data)\n",
    "\n",
    "    # repetitions for common features\n",
    "    turbine_type_repeated = np.tile(turbine_types_onehot[idx], (num_rows, 1))\n",
    "    hub_height_repeated = np.full((num_rows, 1), all_hub_heights[idx])\n",
    "    age_repeated = np.full((num_rows, 1), ages[idx])\n",
    "\n",
    "    # extract production values and wind speeds\n",
    "    production_values = np.array([entry[1] for entry in production_data]).reshape(-1, 1) / all_capacities[idx]\n",
    "    wind_speeds = np.array([entry[2] for entry in production_data]).reshape(-1, 1)\n",
    "\n",
    "    # combine all features\n",
    "    combined_chunk = np.hstack((\n",
    "        turbine_type_repeated,\n",
    "        hub_height_repeated,\n",
    "        age_repeated,\n",
    "        wind_speeds\n",
    "    ))\n",
    "\n",
    "    # add the data\n",
    "    combined_features_raw.append(combined_chunk)\n",
    "    output_raw.append(production_values)\n",
    "\n",
    "np.save(\"turbine_types_order.npy\", encoder.categories_[0])\n",
    "\n",
    "# combine all data chunks to one array\n",
    "combined_features_raw = np.vstack(combined_features_raw)\n",
    "output_raw = np.vstack(output_raw)\n",
    "\n",
    "# round all values to two decimal places\n",
    "combined_features_raw = np.round(combined_features_raw, decimals=4)\n",
    "output_raw = np.round(output_raw, decimals=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = output_raw.flatten() > 0.01\n",
    "\n",
    "# Apply the mask to filter the subsets\n",
    "combined_features_filtered = combined_features_raw[mask]\n",
    "output_filtered = output_raw[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Scale feature vector and define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import joblib\n",
    "\n",
    "combined_features = combined_features_filtered.copy()\n",
    "output = output_filtered.copy()\n",
    "\n",
    "# Separate Scaler für jedes Feature\n",
    "scaler_wind = StandardScaler()\n",
    "scaler_ages = StandardScaler()\n",
    "scaler_hub_heights = StandardScaler()\n",
    "\n",
    "# Skalieren der einzelnen Features\n",
    "combined_features[:, -1] = scaler_wind.fit_transform(combined_features[:, -1].reshape(-1, 1)).flatten() # scale wind speeds\n",
    "combined_features[:, -2] = scaler_ages.fit_transform(combined_features[:, -2].reshape(-1, 1)).flatten()  # scale ages\n",
    "combined_features[:, -3] = scaler_hub_heights.fit_transform(combined_features[:, -3].reshape(-1, 1)).flatten()  # scale hub heights\n",
    "\n",
    "# Speichere alle Scaler in einem Dictionary\n",
    "scalers = {\n",
    "    \"winds\": scaler_wind,\n",
    "    \"ages\": scaler_ages,\n",
    "    \"hub_heights\": scaler_hub_heights,\n",
    "}\n",
    "\n",
    "# Speichere das Dictionary mit Joblib\n",
    "joblib.dump(scalers, \"scalers.pkl\")\n",
    "\n",
    "# Dataset-Klasse für PyTorch\n",
    "class WindPowerDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.targets[index]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Erstellung der PyTorch-Datasets\n",
    "dataset = WindPowerDataset(combined_features, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, use_dropout=False, dropout_rate=0.3, \n",
    "                 use_batch_norm=False, activation_fn=nn.ReLU):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # Erste Schicht\n",
    "        layers.append(nn.Linear(input_size, 256))\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(256))\n",
    "        layers.append(activation_fn())\n",
    "\n",
    "        # Zweite Schicht\n",
    "        layers.append(nn.Linear(256, 128))\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(128))\n",
    "        layers.append(activation_fn())\n",
    "\n",
    "        # Dritte Schicht\n",
    "        layers.append(nn.Linear(128, 64))\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(64))\n",
    "        layers.append(activation_fn())\n",
    "\n",
    "        # Dropout nach der letzten versteckten Schicht (optional)\n",
    "        if use_dropout:\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        # Ausgabeschicht\n",
    "        layers.append(nn.Linear(64, 1))\n",
    "\n",
    "        # Modell zusammenstellen\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1. Hyperparameter search: Training, Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 09:22:06,436] A new study created in memory with name: no-name-b97103b7-ef9f-4179-978e-933285641b5e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cpu\n",
      "CUDA Available: False\n",
      "Device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b823e4190ca43c5ad4fa322d702c513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: batch_size=64, lr=0.00030, number_epochs=51, use_dropout=False, dropout_rate=0.4403619936961537, use_batch_norm=True\n",
      "  Fold 1/4\n",
      "[W 2025-01-13 09:22:23,057] Trial 0 failed with parameters: {'batch_size': 51, 'lr': 0.00029627547762384323, 'number_epochs': 51, 'use_dropout': False, 'dropout_rate': 0.4403619936961537, 'use_batch_norm': True} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_42312\\903217237.py\", line 84, in objective\n",
      "    optimizer.step()\n",
      "  File \"c:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\optim\\optimizer.py\", line 487, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\optim\\optimizer.py\", line 91, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\optim\\adam.py\", line 223, in step\n",
      "    adam(\n",
      "  File \"c:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\optim\\optimizer.py\", line 154, in maybe_fallback\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\optim\\adam.py\", line 784, in adam\n",
      "    func(\n",
      "  File \"c:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\optim\\adam.py\", line 432, in _single_tensor_adam\n",
      "    param.addcdiv_(exp_avg, denom, value=-step_size)\n",
      "KeyboardInterrupt\n",
      "[W 2025-01-13 09:22:23,065] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 121\u001b[0m\n\u001b[0;32m    118\u001b[0m best_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    120\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m, pruner\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39mpruners\u001b[38;5;241m.\u001b[39mMedianPruner())\n\u001b[1;32m--> 121\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBeste Parameterkombination:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[5], line 84\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     82\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     83\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 84\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     87\u001b[0m     memory_info \u001b[38;5;241m=\u001b[39m pynvml\u001b[38;5;241m.\u001b[39mnvmlDeviceGetMemoryInfo(gpu_handle)\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\optim\\adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    214\u001b[0m         group,\n\u001b[0;32m    215\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m         state_steps,\n\u001b[0;32m    221\u001b[0m     )\n\u001b[1;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\optim\\adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\optim\\adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m--> 432\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "import time\n",
    "import pynvml\n",
    "import psutil\n",
    "\n",
    "# Ressourcenüberwachung initialisieren\n",
    "gpu_used = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == torch.device(\"cuda\"):\n",
    "    pynvml.nvmlInit()\n",
    "    gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # GPU 0 verwenden\n",
    "\n",
    "n_splits = 2  # Anzahl der Folds für Kreuzvalidierung\n",
    "\n",
    "def system_info():\n",
    "    print(\"PyTorch Version:\", torch.__version__)\n",
    "    print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "    print(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"\\nCUDA Details:\")\n",
    "        print(\"CUDA Version:\", torch.version.cuda)\n",
    "        print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            print(f\"  - Compute Capability: {torch.cuda.get_device_capability(i)}\")\n",
    "            print(f\"  - Memory Allocated: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MB\")\n",
    "            print(f\"  - Memory Cached: {torch.cuda.memory_reserved(i) / 1024**2:.2f} MB\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    system_info()\n",
    "\n",
    "# Bewertungsfunktion\n",
    "def objective(trial):\n",
    "    original_batch_size = trial.suggest_int(\"batch_size\", 16, 128)\n",
    "    batch_size = int(2 ** round(np.log2(original_batch_size)))  # Transformierte Batch-Größe\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    number_epochs = trial.suggest_int(\"number_epochs\", 10, 100)\n",
    "    use_dropout = trial.suggest_categorical(\"use_dropout\", [True, False])\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    use_batch_norm = trial.suggest_categorical(\"use_batch_norm\", [True, False])\n",
    "\n",
    "    # Speichern der transformierten Batch-Größe als Attribut\n",
    "    trial.set_user_attr(\"transformed_batch_size\", batch_size)\n",
    "\n",
    "    print(f\"Evaluating: batch_size={batch_size}, lr={lr:.5f}, number_epochs={number_epochs}, \"\n",
    "          f\"use_dropout={use_dropout}, dropout_rate={dropout_rate}, use_batch_norm={use_batch_norm}\")\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    len_dataset = len(dataset)\n",
    "\n",
    "    input_size = combined_features.shape[1]\n",
    "    avg_val_loss = 0.0  # Durchschnittlicher Validierungsverlust\n",
    "    start_time = time.time()  # Zeitmessung starten\n",
    "    max_memory_usage = 0  # Maximale Speicher-Auslastung\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(range(len_dataset)), 1):\n",
    "        print(f\"  Fold {fold}/{kf.n_splits}\")\n",
    "\n",
    "        model = torch.jit.script(MLP(input_size=input_size, use_dropout=use_dropout, dropout_rate=dropout_rate, use_batch_norm=use_batch_norm)).to(device)\n",
    "\n",
    "        train_fold_dataset = Subset(dataset, train_idx)\n",
    "        val_fold_dataset = Subset(dataset, val_idx)\n",
    "\n",
    "        train_loader = DataLoader(train_fold_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_fold_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        criterion = nn.HuberLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        for epoch in range(number_epochs):\n",
    "            model.train()\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if device == torch.device(\"cuda\"):\n",
    "                    memory_info = pynvml.nvmlDeviceGetMemoryInfo(gpu_handle)\n",
    "                    max_memory_usage = max(max_memory_usage, memory_info.used / 1024 ** 2)  # MB\n",
    "                    global gpu_used\n",
    "                    gpu_used = True\n",
    "                else:\n",
    "                    max_memory_usage = max(max_memory_usage, psutil.virtual_memory().used / 1024 ** 2)  # MB\n",
    "\n",
    "        model.eval() # deactivates Batch normalisation and Dropout\n",
    "        fold_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                val_outputs = model(batch_x)\n",
    "                fold_val_loss += criterion(val_outputs, batch_y).item()\n",
    "                \n",
    "        avg_val_loss += fold_val_loss / len(val_loader)\n",
    "\n",
    "    avg_val_loss /= kf.n_splits\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    weighted_score = 0.7 * avg_val_loss + 0.15 * elapsed_time + 0.15 * max_memory_usage\n",
    "\n",
    "    trial.set_user_attr(\"resource_usage\", max_memory_usage)\n",
    "    trial.set_user_attr(\"elapsed_time\", elapsed_time)\n",
    "    trial.set_user_attr(\"avg_val_loss\", avg_val_loss)\n",
    "    trial.set_user_attr(\"weighted_score\", weighted_score)\n",
    "\n",
    "    return weighted_score\n",
    "\n",
    "# Optuna-Optimierung starten\n",
    "best_val_loss = float(\"inf\")\n",
    "best_params = None\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=15, show_progress_bar=True)\n",
    "\n",
    "print(\"\\nBeste Parameterkombination:\")\n",
    "print(study.best_params)\n",
    "\n",
    "with open(\"best_params.json\", \"w\") as f:\n",
    "    json.dump(study.best_params, f)\n",
    "\n",
    "# Ausgabe des besten Validierungsverlusts\n",
    "best_trial = study.best_trial\n",
    "print(f\"Bester Validierungsverlust: {best_trial.user_attrs['avg_val_loss']}\")\n",
    "\n",
    "for trial in study.trials:\n",
    "    print(trial)\n",
    "\n",
    "# Debugging: Überprüfung, ob GPU verwendet wurde\n",
    "if gpu_used:\n",
    "    print(\"GPU wurde erfolgreich während des Trainings verwendet.\")\n",
    "else:\n",
    "    print(\"GPU wurde nicht verwendet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. With best hyperparameters: Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# from torch.nn import HuberLoss, MSELoss, L1Loss\n",
    "# import subprocess\n",
    "# import platform\n",
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# log_dir = \"runs\"\n",
    "\n",
    "# # TensorBoard-Prozess beenden\n",
    "# try:\n",
    "#     if platform.system() == \"Windows\":\n",
    "#         subprocess.run([\"taskkill\", \"/IM\", \"tensorboard.exe\", \"/F\"], check=True)\n",
    "#     else:\n",
    "#         subprocess.run([\"pkill\", \"-f\", \"tensorboard\"], check=True)\n",
    "#     print(\"Terminated TensorBoard process\")\n",
    "# except subprocess.CalledProcessError:\n",
    "#     print(\"No TensorBoard process found or could not be terminated\")\n",
    "\n",
    "# # Log-Verzeichnis löschen\n",
    "# if os.path.exists(log_dir):\n",
    "#     shutil.rmtree(log_dir)\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# subprocess.Popen([\"tensorboard\", \"--logdir=runs\", \"--bind_all\"])\n",
    "# print(\"TensorBoard started.\")\n",
    "\n",
    "# # TensorBoard-Writer starten\n",
    "# writer = SummaryWriter(f\"{log_dir}/final_training\")\n",
    "\n",
    "# # Daten splitten\n",
    "# train_x, test_x, train_y, test_y = train_test_split(combined_features, output, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "# with open(\"model1/best_params.json\", \"r\") as f:\n",
    "#     best_params = json.load(f)\n",
    "\n",
    "# # Modell initialisieren\n",
    "# model = MLP(\n",
    "#     input_size=train_x.shape[1],\n",
    "#     use_dropout=best_params[\"use_dropout\"],\n",
    "#     dropout_rate=best_params[\"dropout_rate\"],\n",
    "#     use_batch_norm=best_params[\"use_batch_norm\"]\n",
    "# ).to(device)\n",
    "\n",
    "# # Visualisierung des Modells\n",
    "# example_input = torch.randn(best_params[\"batch_size\"], train_x.shape[1]).to(device)\n",
    "# writer.add_graph(model, example_input)\n",
    "\n",
    "# # Trainings-Konfiguration\n",
    "# criterion = HuberLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=best_params[\"lr\"])\n",
    "\n",
    "# # Daten-Loader\n",
    "# train_loader = DataLoader(list(zip(train_x, train_y)), batch_size=best_params[\"batch_size\"], shuffle=True)\n",
    "# test_loader = DataLoader(list(zip(test_x, test_y)), batch_size=best_params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# # Training\n",
    "# num_epochs = best_params[\"number_epochs\"]\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     train_loss = 0\n",
    "\n",
    "#     for batch_x, batch_y in train_loader:\n",
    "#         batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "#         outputs = model(batch_x)\n",
    "#         loss = criterion(outputs, batch_y)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_loss += loss.item()\n",
    "\n",
    "#     train_loss /= len(train_loader)\n",
    "#     writer.add_scalar(\"Training Loss\", train_loss, epoch)\n",
    "\n",
    "# # Testen\n",
    "# model.eval()\n",
    "# mae_criterion = L1Loss()\n",
    "# mse_criterion = MSELoss()\n",
    "# huber_criterion = HuberLoss()\n",
    "\n",
    "# test_loss_mae, test_loss_mse, test_loss_huber = 0, 0, 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch_x, batch_y in test_loader:\n",
    "#         batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "#         preds = model(batch_x)\n",
    "        \n",
    "#         test_loss_mae += mae_criterion(preds, batch_y).item()\n",
    "#         test_loss_mse += mse_criterion(preds, batch_y).item()\n",
    "#         test_loss_huber += huber_criterion(preds, batch_y).item()\n",
    "\n",
    "# test_loss_mae /= len(test_loader)\n",
    "# test_loss_mse /= len(test_loader)\n",
    "# test_loss_huber /= len(test_loader)\n",
    "\n",
    "# # Ergebnisse anzeigen\n",
    "# print(f\"Endgültiger MAE: {test_loss_mae:.4f}\")\n",
    "# print(f\"Endgültiger MSE: {test_loss_mse:.4f}\")\n",
    "# print(f\"Endgültiger Huber Loss: {test_loss_huber:.4f}\")\n",
    "\n",
    "# # TensorBoard-Writer schließen\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. With all data: Training for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'use_dropout'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Modell initialisieren\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m MLP(\n\u001b[0;32m      3\u001b[0m     input_size\u001b[38;5;241m=\u001b[39mtrain_features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m----> 4\u001b[0m     use_dropout\u001b[38;5;241m=\u001b[39m\u001b[43mbest_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_dropout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[0;32m      5\u001b[0m     dropout_rate\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      6\u001b[0m     use_batch_norm\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_batch_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      7\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Trainings-Konfiguration\u001b[39;00m\n\u001b[0;32m     10\u001b[0m criterion \u001b[38;5;241m=\u001b[39m HuberLoss()\n",
      "\u001b[1;31mKeyError\u001b[0m: 'use_dropout'"
     ]
    }
   ],
   "source": [
    "# # Modell initialisieren\n",
    "# model = MLP(\n",
    "#     input_size=combined_features.shape[1],\n",
    "#     use_dropout=best_params[\"use_dropout\"],\n",
    "#     dropout_rate=best_params[\"dropout_rate\"],\n",
    "#     use_batch_norm=best_params[\"use_batch_norm\"]\n",
    "# ).to(device)\n",
    "\n",
    "# # Trainings-Konfiguration\n",
    "# criterion = HuberLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=best_params[\"lr\"])\n",
    "\n",
    "# # Daten-Loader für alle Trainingsdaten\n",
    "# train_loader = DataLoader(list(zip(combined_features, output)), batch_size=best_params[\"batch_size\"], shuffle=True)\n",
    "\n",
    "# # Training\n",
    "# num_epochs = best_params[\"number_epochs\"]\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     train_loss = 0\n",
    "\n",
    "#     for batch_x, batch_y in train_loader:\n",
    "#         batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "#         outputs = model(batch_x)\n",
    "#         loss = criterion(outputs, batch_y)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_loss += loss.item()\n",
    "\n",
    "#     train_loss /= len(train_loader)\n",
    "#     writer.add_scalar(\"Final Training Loss\", train_loss, epoch)\n",
    "\n",
    "# # Modell speichern\n",
    "# torch.save(model.state_dict(), \"trained_parameters.pth\")\n",
    "# print(\"Modell für Deployment gespeichert!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webapp_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
