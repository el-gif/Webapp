{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Build feature and output arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# lists for all data\n",
    "all_turbine_types = []\n",
    "all_hub_heights = []\n",
    "all_capacities = []\n",
    "all_commissioning_dates = []\n",
    "all_production_data = []\n",
    "\n",
    "with open(f\"data/WPPs+production+wind.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    WPP_production_wind = json.load(file)\n",
    "\n",
    "# collect data\n",
    "for wpp in WPP_production_wind:\n",
    "    all_turbine_types.append([wpp[\"Turbine\"] if pd.notna(wpp[\"Turbine\"]) else \"nan\" for wpp in WPP_production_wind])\n",
    "    all_hub_heights.append(wpp[\"Hub_height\"] if not pd.isna(wpp[\"Hub_height\"]) else 100)\n",
    "    all_capacities.append(wpp[\"Capacity\"])\n",
    "    all_commissioning_dates.append(\"2015/06\" if wpp[\"Commission_date\"] == \"nan\" else f\"{wpp['Commission_date']}/06\" if isinstance(wpp[\"Commission_date\"], str) and \"/\" not in wpp[\"Commission_date\"] else wpp[\"Commission_date\"])\n",
    "    all_production_data.append(wpp[\"Production\"])\n",
    "\n",
    "# One-Hot-Encoding for turbine types\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "turbine_types_onehot = encoder.fit_transform(np.array(all_turbine_types).reshape(-1, 1))\n",
    "\n",
    "# convert to datetime\n",
    "standardised_dates = pd.to_datetime(all_commissioning_dates, format='%Y/%m')\n",
    "\n",
    "# calculate age\n",
    "current_date = pd.Timestamp(\"2024-12-01\")\n",
    "ages = current_date.year * 12 + current_date.month - (standardised_dates.year * 12 + standardised_dates.month)\n",
    "\n",
    "# create combined features and output lists\n",
    "combined_features_raw = []\n",
    "output_raw = []\n",
    "\n",
    "# convert data in feature arrays\n",
    "for idx, production_data in enumerate(all_production_data):\n",
    "    num_rows = len(production_data)\n",
    "\n",
    "    # repetitions for common features\n",
    "    turbine_type_repeated = np.tile(turbine_types_onehot[idx], (num_rows, 1))\n",
    "    hub_height_repeated = np.full((num_rows, 1), all_hub_heights[idx])\n",
    "    age_repeated = np.full((num_rows, 1), ages[idx])\n",
    "\n",
    "    # extract production values and wind speeds\n",
    "    production_values = np.array([entry[1] for entry in production_data]).reshape(-1, 1) / all_capacities[idx]\n",
    "    wind_speeds = np.array([entry[2] for entry in production_data]).reshape(-1, 1)\n",
    "\n",
    "    # combine all features\n",
    "    combined_chunk = np.hstack((\n",
    "        turbine_type_repeated,\n",
    "        hub_height_repeated,\n",
    "        age_repeated,\n",
    "        wind_speeds\n",
    "    ))\n",
    "\n",
    "    # add the data\n",
    "    combined_features_raw.append(combined_chunk)\n",
    "    output_raw.append(production_values)\n",
    "\n",
    "np.save(\"turbine_types_order.npy\", encoder.categories_[0])\n",
    "\n",
    "# combine all data chunks to one array\n",
    "combined_features_raw = np.vstack(combined_features_raw)\n",
    "output_raw = np.vstack(output_raw)\n",
    "\n",
    "# round all values to two decimal places\n",
    "combined_features_raw = np.round(combined_features_raw, decimals=4)\n",
    "output_raw = np.round(output_raw, decimals=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Scale feature vector and define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import joblib\n",
    "\n",
    "combined_features = combined_features_raw.copy()\n",
    "output = output_raw.copy()\n",
    "\n",
    "# Separate Scaler für jedes Feature\n",
    "scaler_wind = StandardScaler()\n",
    "scaler_ages = StandardScaler()\n",
    "scaler_hub_heights = StandardScaler()\n",
    "\n",
    "# Skalieren der einzelnen Features\n",
    "combined_features[:, -1] = scaler_wind.fit_transform(combined_features[:, -1].reshape(-1, 1)).flatten() # scale wind speeds\n",
    "combined_features[:, -2] = scaler_ages.fit_transform(combined_features[:, -2].reshape(-1, 1)).flatten()  # scale ages\n",
    "combined_features[:, -3] = scaler_hub_heights.fit_transform(combined_features[:, -3].reshape(-1, 1)).flatten()  # scale hub heights\n",
    "\n",
    "# Speichere alle Scaler in einem Dictionary\n",
    "scalers = {\n",
    "    \"winds\": scaler_wind,\n",
    "    \"ages\": scaler_ages,\n",
    "    \"hub_heights\": scaler_hub_heights,\n",
    "}\n",
    "\n",
    "# Speichere das Dictionary mit Joblib\n",
    "joblib.dump(scalers, \"scalers.pkl\")\n",
    "\n",
    "# Dataset-Klasse für PyTorch\n",
    "class WindPowerDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.targets[index]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Erstellung der PyTorch-Datasets\n",
    "dataset = WindPowerDataset(combined_features, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, use_dropout=False, dropout_rate=0.3, \n",
    "                 use_batch_norm=False, activation_fn=nn.ReLU):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # Erste Schicht\n",
    "        layers.append(nn.Linear(input_size, 256))\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(256))\n",
    "        layers.append(activation_fn())\n",
    "\n",
    "        # Zweite Schicht\n",
    "        layers.append(nn.Linear(256, 128))\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(128))\n",
    "        layers.append(activation_fn())\n",
    "\n",
    "        # Dritte Schicht\n",
    "        layers.append(nn.Linear(128, 64))\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(64))\n",
    "        layers.append(activation_fn())\n",
    "\n",
    "        # Dropout nach der letzten versteckten Schicht (optional)\n",
    "        if use_dropout:\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        # Ausgabeschicht\n",
    "        layers.append(nn.Linear(64, 1))\n",
    "\n",
    "        # Modell zusammenstellen\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1. Hyperparameter search: Training, Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "import time\n",
    "import pynvml\n",
    "import psutil\n",
    "\n",
    "# Ressourcenüberwachung initialisieren\n",
    "gpu_used = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == torch.device(\"cuda\"):\n",
    "    pynvml.nvmlInit()\n",
    "    gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # GPU 0 verwenden\n",
    "\n",
    "n_splits = 2  # Anzahl der Folds für Kreuzvalidierung\n",
    "\n",
    "def system_info():\n",
    "    print(\"PyTorch Version:\", torch.__version__)\n",
    "    print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "    print(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"\\nCUDA Details:\")\n",
    "        print(\"CUDA Version:\", torch.version.cuda)\n",
    "        print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            print(f\"  - Compute Capability: {torch.cuda.get_device_capability(i)}\")\n",
    "            print(f\"  - Memory Allocated: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MB\")\n",
    "            print(f\"  - Memory Cached: {torch.cuda.memory_reserved(i) / 1024**2:.2f} MB\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    system_info()\n",
    "\n",
    "# Bewertungsfunktion\n",
    "def objective(trial):\n",
    "    original_batch_size = trial.suggest_int(\"batch_size\", 16, 128)\n",
    "    batch_size = int(2 ** round(np.log2(original_batch_size)))  # Transformierte Batch-Größe\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    number_epochs = trial.suggest_int(\"number_epochs\", 10, 100)\n",
    "    use_dropout = trial.suggest_categorical(\"use_dropout\", [True, False])\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    use_batch_norm = trial.suggest_categorical(\"use_batch_norm\", [True, False])\n",
    "\n",
    "    # Speichern der transformierten Batch-Größe als Attribut\n",
    "    trial.set_user_attr(\"transformed_batch_size\", batch_size)\n",
    "\n",
    "    print(f\"Evaluating: batch_size={batch_size}, lr={lr:.5f}, number_epochs={number_epochs}, \"\n",
    "          f\"use_dropout={use_dropout}, dropout_rate={dropout_rate}, use_batch_norm={use_batch_norm}\")\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    len_dataset = len(dataset)\n",
    "\n",
    "    input_size = combined_features.shape[1]\n",
    "    avg_val_loss = 0.0  # Durchschnittlicher Validierungsverlust\n",
    "    start_time = time.time()  # Zeitmessung starten\n",
    "    max_memory_usage = 0  # Maximale Speicher-Auslastung\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(range(len_dataset)), 1):\n",
    "        print(f\"  Fold {fold}/{kf.n_splits}\")\n",
    "\n",
    "        # use static instead of dynamic computational graphs\n",
    "        model = torch.jit.script(MLP(input_size=input_size, use_dropout=use_dropout, dropout_rate=dropout_rate, use_batch_norm=use_batch_norm)).to(device)\n",
    "\n",
    "        train_fold_dataset = Subset(dataset, train_idx)\n",
    "        val_fold_dataset = Subset(dataset, val_idx)\n",
    "\n",
    "        # shuffling doesn't matter here, has already taken place during KFold\n",
    "        train_loader = DataLoader(train_fold_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_fold_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        criterion = nn.HuberLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        for epoch in range(number_epochs):\n",
    "            model.train()\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if device == torch.device(\"cuda\"):\n",
    "                    memory_info = pynvml.nvmlDeviceGetMemoryInfo(gpu_handle)\n",
    "                    max_memory_usage = max(max_memory_usage, memory_info.used / 1024 ** 2)  # MB\n",
    "                    global gpu_used\n",
    "                    gpu_used = True\n",
    "                else:\n",
    "                    max_memory_usage = max(max_memory_usage, psutil.virtual_memory().used / 1024 ** 2)  # MB\n",
    "\n",
    "        model.eval() # deactivates Batch normalisation and Dropout\n",
    "        fold_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                val_outputs = model(batch_x)\n",
    "                fold_val_loss += criterion(val_outputs, batch_y).item()\n",
    "                \n",
    "        avg_val_loss += fold_val_loss / len(val_loader)\n",
    "\n",
    "    avg_val_loss /= kf.n_splits\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    weighted_score = 0.6 * avg_val_loss + 0.2 * elapsed_time + 0.2 * max_memory_usage\n",
    "\n",
    "    trial.set_user_attr(\"resource_usage\", max_memory_usage)\n",
    "    trial.set_user_attr(\"elapsed_time\", elapsed_time)\n",
    "    trial.set_user_attr(\"avg_val_loss\", avg_val_loss)\n",
    "    trial.set_user_attr(\"weighted_score\", weighted_score)\n",
    "\n",
    "    return weighted_score\n",
    "\n",
    "# Optuna-Optimierung starten\n",
    "best_val_loss = float(\"inf\")\n",
    "best_params = None\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=15, show_progress_bar=True)\n",
    "\n",
    "print(\"\\nBeste Parameterkombination:\")\n",
    "print(study.best_params)\n",
    "\n",
    "# Ausgabe des besten Validierungsverlusts\n",
    "best_trial = study.best_trial\n",
    "print(f\"Bester Validierungsverlust: {best_trial.user_attrs['avg_val_loss']}\")\n",
    "\n",
    "for trial in study.trials:\n",
    "    print(trial)\n",
    "\n",
    "# Debugging: Überprüfung, ob GPU verwendet wurde\n",
    "if gpu_used:\n",
    "    print(\"GPU wurde erfolgreich während des Trainings verwendet.\")\n",
    "else:\n",
    "    print(\"GPU wurde nicht verwendet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. With best hyperparameters: Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminated TensorBoard process\n",
      "TensorBoard started.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     92\u001b[0m loss_huber\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 93\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Accumulate losses for logging\u001b[39;00m\n\u001b[0;32m     96\u001b[0m train_loss_mae \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_mae\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\optim\\optimizer.py:472\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m cast(Optimizer, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    471\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile_name\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;66;03m# call optimizer step pre hooks\u001b[39;00m\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pre_hook \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[0;32m    475\u001b[0m         _global_optimizer_pre_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_pre_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    477\u001b[0m     ):\n\u001b[0;32m    478\u001b[0m         result \u001b[38;5;241m=\u001b[39m pre_hook(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\autograd\\profiler.py:750\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[0;32m    749\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[1;32m--> 750\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_exit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RecordFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    752\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_exit(record)\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\_ops.py:953\u001b[0m, in \u001b[0;36mTorchBindOpOverload.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    951\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_as_effectful_op_temporarily():\n\u001b[0;32m    952\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_in_python(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fallthrough_keys())\n\u001b[1;32m--> 953\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn import HuberLoss, MSELoss, L1Loss\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import subprocess\n",
    "import platform\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import pynvml\n",
    "    \n",
    "log_dir = \"runs\"\n",
    "\n",
    "# Ressourcenüberwachung initialisieren\n",
    "gpu_used = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == torch.device(\"cuda\"):\n",
    "    pynvml.nvmlInit()\n",
    "    gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # GPU 0 verwenden\n",
    "\n",
    "# TensorBoard-Prozess beenden\n",
    "try:\n",
    "    if platform.system() == \"Windows\":\n",
    "        subprocess.run([\"taskkill\", \"/IM\", \"tensorboard.exe\", \"/F\"], check=True)\n",
    "    else:\n",
    "        subprocess.run([\"pkill\", \"-f\", \"tensorboard\"], check=True)\n",
    "    print(\"Terminated TensorBoard process\")\n",
    "except subprocess.CalledProcessError:\n",
    "    print(\"No TensorBoard process found or could not be terminated\")\n",
    "\n",
    "# Log-Verzeichnis löschen\n",
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "subprocess.Popen([\"tensorboard\", \"--logdir=runs\", \"--bind_all\"])\n",
    "print(\"TensorBoard started.\")\n",
    "\n",
    "# TensorBoard-Writer starten\n",
    "writer = SummaryWriter(f\"{log_dir}/final_training\")\n",
    "\n",
    "# Daten splitten\n",
    "train_x, test_x, train_y, test_y = train_test_split(combined_features, output, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "# Convert to float32 to match PyTorch default dtype\n",
    "train_x = torch.tensor(train_x, dtype=torch.float32)\n",
    "test_x = torch.tensor(test_x, dtype=torch.float32)\n",
    "train_y = torch.tensor(train_y, dtype=torch.float32)\n",
    "test_y = torch.tensor(test_y, dtype=torch.float32)\n",
    "\n",
    "# manually define best parameters found in previous step\n",
    "batch_size = 128\n",
    "lr = 0.00010155300193027382\n",
    "num_epochs = 10\n",
    "use_dropout = True\n",
    "dropout_rate = 0.33659592356347234\n",
    "use_batch_norm = False\n",
    "\n",
    "# use static instead of dynamic computational graphs\n",
    "model = torch.jit.script(MLP(input_size=train_x.shape[1])).to(device)\n",
    "\n",
    "# Visualisierung des Modells\n",
    "example_input = torch.randn(batch_size, train_x.shape[1]).to(device)\n",
    "writer.add_graph(model, example_input)\n",
    "\n",
    "# Trainings-Konfiguration\n",
    "mae_criterion = L1Loss()\n",
    "mse_criterion = MSELoss()\n",
    "huber_criterion = HuberLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# shuffling doesn't matter here, has already taken place during train_test_split\n",
    "train_loader = DataLoader(list(zip(train_x, train_y)), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(list(zip(test_x, test_y)), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss_mae, train_loss_mse, train_loss_huber = 0, 0, 0\n",
    "\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        \n",
    "        # Calculate losses for each criterion\n",
    "        loss_mae = mae_criterion(outputs, batch_y)\n",
    "        loss_mse = mse_criterion(outputs, batch_y)\n",
    "        loss_huber = huber_criterion(outputs, batch_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_huber.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate losses for logging\n",
    "        train_loss_mae += loss_mae.item()\n",
    "        train_loss_mse += loss_mse.item()\n",
    "        train_loss_huber += loss_huber.item()\n",
    "\n",
    "    train_loss_mae /= len(train_loader)\n",
    "    train_loss_mse /= len(train_loader)\n",
    "    train_loss_huber /= len(train_loader)\n",
    "\n",
    "    writer.add_scalar(\"Training Loss\", train_loss_huber, epoch)\n",
    "\n",
    "    # Testen\n",
    "    model.eval()\n",
    "\n",
    "    test_loss_mae, test_loss_mse, test_loss_huber = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            preds = model(batch_x)\n",
    "            \n",
    "            test_loss_mae += mae_criterion(preds, batch_y).item()\n",
    "            test_loss_mse += mse_criterion(preds, batch_y).item()\n",
    "            test_loss_huber += huber_criterion(preds, batch_y).item()\n",
    "\n",
    "    test_loss_mae /= len(test_loader)\n",
    "    test_loss_mse /= len(test_loader)\n",
    "    test_loss_huber /= len(test_loader)\n",
    "\n",
    "    writer.add_scalar(\"Testing Loss\", test_loss_huber, epoch)\n",
    "\n",
    "print(\"Training\")\n",
    "print(f\"MAE: {train_loss_mae:.4f}\")\n",
    "print(f\"MSE: {train_loss_mse:.4f}\")\n",
    "print(f\"Huber Loss: {train_loss_huber:.4f}\")\n",
    "\n",
    "print(\"Testing\")\n",
    "print(f\"MAE: {test_loss_mae:.4f}\")\n",
    "print(f\"MSE: {test_loss_mse:.4f}\")\n",
    "print(f\"Huber Loss: {test_loss_huber:.4f}\")\n",
    "\n",
    "# TensorBoard-Writer schließen\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. With all data: Training for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x00000169A90A9E80>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\alexa\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_22864\\3635992353.py\", line 39, in <forward op>\n    def forward(self, x):\n        return self.model(x)\n               ~~~~~~~~~~ <--- HERE\n  File \"c:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\container.py\", line 250, in forward\n    def forward(self, input):\n        for module in self:\n            input = module(input)\n                    ~~~~~~ <--- HERE\n        return input\n  File \"c:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 125, in forward\n    def forward(self, input: Tensor) -> Tensor:\n        return F.linear(input, self.weight, self.bias)\n               ~~~~~~~~ <--- HERE\nRuntimeError: mat1 and mat2 must have the same dtype, but got Double and Float\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     24\u001b[0m     batch_x, batch_y \u001b[38;5;241m=\u001b[39m batch_x\u001b[38;5;241m.\u001b[39mto(device), batch_y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 25\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n\u001b[0;32m     27\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_22864\\3635992353.py\", line 39, in <forward op>\n    def forward(self, x):\n        return self.model(x)\n               ~~~~~~~~~~ <--- HERE\n  File \"c:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\container.py\", line 250, in forward\n    def forward(self, input):\n        for module in self:\n            input = module(input)\n                    ~~~~~~ <--- HERE\n        return input\n  File \"c:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 125, in forward\n    def forward(self, input: Tensor) -> Tensor:\n        return F.linear(input, self.weight, self.bias)\n               ~~~~~~~~ <--- HERE\nRuntimeError: mat1 and mat2 must have the same dtype, but got Double and Float\n\n"
     ]
    }
   ],
   "source": [
    "# manually define best parameters found in previous step\n",
    "batch_size = 128\n",
    "lr = 0.00010155300193027382\n",
    "num_epochs = 10\n",
    "use_dropout = True\n",
    "dropout_rate = 0.33659592356347234\n",
    "use_batch_norm = False\n",
    "\n",
    "# use static instead of dynamic computational graphs\n",
    "model = torch.jit.script(MLP(input_size=combined_features.shape[1], use_dropout=use_dropout, dropout_rate=dropout_rate, use_batch_norm=use_batch_norm)).to(device)\n",
    "\n",
    "# Trainings-Konfiguration\n",
    "criterion = HuberLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Daten-Loader für alle Trainingsdaten\n",
    "train_loader = DataLoader(list(zip(combined_features, output)), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "# Modell speichern\n",
    "torch.save(model.state_dict(), \"trained_parameters.pth\")\n",
    "print(\"Modell für Deployment gespeichert!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webapp_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
