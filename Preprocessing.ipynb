{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1. Preprocessing of historical production data: discard data of unwanted power plants, retain monthly files</h1>\n",
    "<ul>\n",
    "    <li>WPPs are considered unique if they have the same name and installed capacity (capacity can change for the same WPP --> consider it as a new WPP)</li>\n",
    "    <ul>\n",
    "        <li>real duplicates are avoided by creating a set unique_windfarms_set and comparing, if already added</li>\n",
    "        <li>distinguish between real duplicates (same 'GenerationUnitName', 'GenerationUnitCode', 'GenerationUnitType', 'GenerationUnitInstalledCapacity(MW)', only different 'AreaCode', 'AreaDisplayName', 'AreaTypeCode' and 'MapCode') and WPPs that have the same 'GenerationUnitName', 'GenerationUnitCode', 'GenerationUnitType', but a different 'GenerationUnitInstalledCapacity(MW)' (changed over time)</li>\n",
    "    </ul>\n",
    "    <li>takes 1 to 2 minutes per month, partially because only values for full hours are retained</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Basisverzeichnisse\n",
    "input_dir = r\"E:\\MA_data\\raw production history ENTSO-E\"\n",
    "output_dir = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\"\n",
    "\n",
    "# Liste der Monate von 2015-01 bis 2024-10 generieren\n",
    "months = pd.date_range(start=\"2019-09\", end=\"2019-09\", freq=\"MS\").strftime(\"%Y_%m\").tolist()\n",
    "\n",
    "# For-Schleife für jede Datei\n",
    "for month in months:\n",
    "    # Dateipfad erstellen\n",
    "    input_file = os.path.join(input_dir, f\"{month}_ActualGenerationOutputPerGenerationUnit_16.1.A_r2.1.csv\")\n",
    "    output_file = os.path.join(output_dir, f\"production_summary_{month}.json\")\n",
    "\n",
    "    # Überprüfen, ob die Datei existiert\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Datei nicht gefunden: {input_file}\")\n",
    "        continue\n",
    "\n",
    "    # Datei einlesen\n",
    "    print(f\"Bearbeite Datei: {input_file}\")\n",
    "    data = pd.read_csv(input_file, sep='\\t')\n",
    "\n",
    "    # Filtere nach GenerationUnitType == 'Wind Onshore' oder 'Wind Offshore'\n",
    "    filtered_data = data[(data['GenerationUnitType'] == 'Wind Onshore ') | (data['GenerationUnitType'] == 'Wind Offshore ')]\n",
    "\n",
    "    # Konvertiere 'DateTime (UTC)' direkt in das ISO-8601-Format\n",
    "    filtered_data.loc[:, 'DateTime (UTC)'] = pd.to_datetime(filtered_data['DateTime (UTC)']).dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "    # Wichtige Spalten identifizieren, 'AreaCode', 'AreaDisplayName', 'AreaTypeCode' and 'MapCode' of identical WPPs may differ --> use at least one of them as a criterion to identify unique windfarms, and sort out the duplicates manually, because otherwise, the production data are appended twice to the same wind farm\n",
    "    unique_windfarms = filtered_data[['GenerationUnitName', 'GenerationUnitCode', 'GenerationUnitType', 'GenerationUnitInstalledCapacity(MW)', 'AreaCode']].drop_duplicates()\n",
    "    unique_windfarms_set = set(unique_windfarms['GenerationUnitName'])\n",
    "\n",
    "    # Listen für die Produktion zu jeder Stunde hinzufügen\n",
    "    production_data = []\n",
    "    for _, row in unique_windfarms.iterrows():\n",
    "        # Filtern der Daten für das aktuelle Windkraftwerk\n",
    "        windfarm_data = filtered_data[\n",
    "            (filtered_data['GenerationUnitName'] == row['GenerationUnitName']) &\n",
    "            (filtered_data['AreaCode'] == row['AreaCode']) # important to avoid adding to a wind farm production data of all its duplicates\n",
    "        ]\n",
    "\n",
    "        # Erstelle 2D-Array (Liste von Listen) mit Zeit und Produktion, da JSON keine Arrays speichern kann\n",
    "        production_array = [\n",
    "            [time, production]\n",
    "            for time, production in zip(\n",
    "                windfarm_data['DateTime (UTC)'],\n",
    "                windfarm_data['ActualGenerationOutput(MW)']\n",
    "            )\n",
    "            if pd.notna(production) and pd.to_datetime(time).minute == 0  # Nur volle Stunden übernehmen (Resolution of weather data is hourly), although it significantly increases the execution time of the programme, und fehlende Werte überspringen\n",
    "        ]\n",
    "\n",
    "        # Daten für das Windkraftwerk hinzufügen\n",
    "        row_data = {\n",
    "            'GenerationUnitName': row['GenerationUnitName'],\n",
    "            'GenerationUnitCode': row['GenerationUnitCode'],\n",
    "            'GenerationUnitType': row['GenerationUnitType'],\n",
    "            'GenerationUnitInstalledCapacity(MW)': row['GenerationUnitInstalledCapacity(MW)'],\n",
    "            'Production': production_array\n",
    "        }\n",
    "\n",
    "        # don't add duplicates\n",
    "        if row_data['GenerationUnitName'] in unique_windfarms_set:\n",
    "            production_data.append(row_data)\n",
    "            unique_windfarms_set.discard(row_data['GenerationUnitName'])\n",
    "\n",
    "    # JSON-Datei speichern\n",
    "    with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(production_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"JSON-Datei wurde erfolgreich erstellt: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. merge all monthly production data files to one combined file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Verzeichnisse\n",
    "input_dir = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\"\n",
    "output_file = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\production_summary_all.json\"\n",
    "\n",
    "# Liste der Monate von 2015_01 bis 2024_10\n",
    "months = pd.date_range(start=\"2015-01\", end=\"2024-10\", freq=\"MS\").strftime(\"%Y_%m\").tolist()\n",
    "\n",
    "columns_merge = ['GenerationUnitName', 'GenerationUnitCode', 'GenerationUnitType', 'GenerationUnitInstalledCapacity(MW)']\n",
    "final_data = {}\n",
    "\n",
    "# Einlesen der einzelnen Dateien\n",
    "for month in months:\n",
    "    input_file = os.path.join(input_dir, f\"production_summary_{month}.json\")\n",
    "\n",
    "    # Überprüfen, ob die Datei existiert\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Datei nicht gefunden: {input_file}\")\n",
    "        continue\n",
    "\n",
    "    # Datei einlesen\n",
    "    print(f\"Verarbeite Datei: {input_file}\")\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        monthly_data = json.load(file)\n",
    "\n",
    "    # Zusammenführen: Gleiche Windkraftanlagen zusammenführen\n",
    "    for windfarm in monthly_data:\n",
    "        key = tuple(windfarm[col] for col in columns_merge) # unique key per WPP, defined by column_merge (name, code, type, capacity) --> duplicates are not added (already assured in previous cell)\n",
    "        if key not in final_data:\n",
    "            # Neu hinzufügen\n",
    "            final_data[key] = windfarm\n",
    "        else:\n",
    "            # Produktion zusammenführen\n",
    "            final_data[key]['Production'].extend(windfarm['Production'])\n",
    "\n",
    "# Finales JSON-Datenformat vorbereiten\n",
    "merged_data = list(final_data.values())\n",
    "\n",
    "# JSON-Datei speichern\n",
    "with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(merged_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Zusammengeführte JSON-Datei wurde erfolgreich gespeichert unter: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. Create Excel File with WPPs in JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Datei laden\n",
    "file_path = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\production_summary_all.json\"\n",
    "output_excel_path = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\JSON_File.xlsx\"\n",
    "\n",
    "# JSON-Datei einlesen\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# assign an ID to each WPP in the JSON file that corresponds to its position in the list of dictionaries\n",
    "for i, item in enumerate(data):\n",
    "    item['JSON-ID'] = i\n",
    "\n",
    "# Relevante Daten extrahieren\n",
    "df = pd.DataFrame(data)[['GenerationUnitName', 'GenerationUnitCode', 'GenerationUnitType', 'GenerationUnitInstalledCapacity(MW)', 'JSON-ID']]\n",
    "\n",
    "# Nach 'GenerationUnitName' sortieren\n",
    "df_sorted = df.sort_values(by='GenerationUnitName')\n",
    "\n",
    "# Daten in eine Excel-Datei speichern\n",
    "df_sorted.to_excel(output_excel_path, index=False)\n",
    "\n",
    "# Fertigmeldung\n",
    "output_excel_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>4. Download table to find common name for UK power plants from website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL der Webseite\n",
    "url = \"https://osuked.github.io/Power-Station-Dictionary/dictionary.html\"\n",
    "\n",
    "# Abrufen der Webseite\n",
    "response = requests.get(url)\n",
    "if response.status_code != 200:\n",
    "    print(\"Fehler beim Abrufen der Webseite.\")\n",
    "    exit()\n",
    "\n",
    "# Parsing der Webseite mit BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Finden aller Tabellen auf der Seite\n",
    "tables = soup.find_all('table')\n",
    "if len(tables) < 3:\n",
    "    print(\"Weniger als 3 Tabellen auf der Seite gefunden.\")\n",
    "    exit()\n",
    "\n",
    "# Die dritte Tabelle auswählen (Index 2, da Python nullbasiert zählt)\n",
    "table = tables[2]\n",
    "\n",
    "# Spaltennamen extrahieren\n",
    "headers = [header.text.strip() for header in table.find_all('th')]\n",
    "\n",
    "# Zeilen extrahieren\n",
    "rows = []\n",
    "for row in table.find_all('tr')[1:]:  # Überspringt die Kopfzeile\n",
    "    cells = [cell.text.strip() for cell in row.find_all('td')]\n",
    "    rows.append(cells)\n",
    "\n",
    "# DataFrame erstellen\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "# Datei speichern\n",
    "output_path = \"data/WPPs/Power_Station_Dictionary.xlsx\"\n",
    "df.to_excel(output_path, index=False)\n",
    "print(f\"Die dritte Tabelle wurde erfolgreich als '{output_path}' gespeichert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>5. Add common names from downloaded to JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two provided files\n",
    "file_1_path = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\WPPs\\Power_Station_Dictionary.xlsx\"\n",
    "file_2_path = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\JSON_File.xlsx\"\n",
    "\n",
    "# read the data\n",
    "df1 = pd.read_excel(file_1_path)\n",
    "df2 = pd.read_excel(file_2_path)\n",
    "\n",
    "# introduce new columns at the end\n",
    "df2['Names_UK_Plants'] = None\n",
    "df2['ID_The-Wind-Power'] = None\n",
    "df2['Comment'] = None\n",
    "\n",
    "# Iterate over rows in df2 to match and update the \"Names_UK_Plants\" column\n",
    "for index, row in df2.iterrows():\n",
    "    generation_unit_name = row['GenerationUnitName']\n",
    "    \n",
    "    # Check if this name appears in the \"National Grid BMU ID\" of the first file\n",
    "    matching_rows = df1[df1['National Grid BMU ID'].str.contains(generation_unit_name, na=False, case=False)]\n",
    "    \n",
    "    if not matching_rows.empty:\n",
    "        # Get the \"Common Name\" value(s) and update the \"Names_UK_Plants\" column in df2\n",
    "        common_names = matching_rows['Common Name'].tolist()\n",
    "        df2.at[index, 'Names_UK_Plants'] = ', '.join(common_names)\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "output_path = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\Assignment.xlsx\"\n",
    "df2.to_excel(output_path, index=False)\n",
    "\n",
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>6. Perform manual assignment to The Wind Power database indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>7. save WPPs in parquet file (The Wind Power)</h1>\n",
    "<ul>\n",
    "    <li>saves a lot of time when loading the map</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lade die Excel-Datei nur einmal, filtere die relevanten Daten und speichere sie als Parquet-Datei\n",
    "WPP_file = \"data/WPPs/Windfarms_Europe_20241123.xlsx\"\n",
    "df = pd.read_excel(WPP_file, sheet_name='Windfarms', na_values=[\"#ND\"])\n",
    "df = df.drop(0) # remove first row (after headlines)\n",
    "\n",
    "# Wähle nur die benötigten Spalten aus\n",
    "df = df[['ID', 'Name', '2nd name', 'Latitude', 'Longitude', 'Manufacturer', 'Turbine', 'Hub height', 'Number of turbines', 'Total power', 'Developer', 'Operator', 'Owner', 'Commissioning date', 'Status']]\n",
    "\n",
    "# Entferne Zeilen, bei denen Name, Total power, Latitude oder Longitude NaN ist\n",
    "df = df.dropna(subset=['Name', 'Total power', 'Latitude', 'Longitude'])\n",
    "\n",
    "# Behalte nur Zeilen, bei denen Status == \"Production\"\n",
    "df = df.loc[df['Status'] == 'Production']\n",
    "\n",
    "# Konvertiere Spalten explizit in ihre entsprechenden Datentypen\n",
    "df['ID'] = df['ID'].astype(int)\n",
    "df['Name'] = df['Name'].astype(str)\n",
    "df['2nd name'] = df['2nd name'].astype(str)\n",
    "df['Latitude'] = df['Latitude'].astype(float)\n",
    "df['Longitude'] = df['Longitude'].astype(float)\n",
    "df['Manufacturer'] = df['Manufacturer'].astype(str)\n",
    "df['Turbine'] = df['Turbine'].astype(str)\n",
    "df['Hub height'] = df['Hub height'].astype(float)\n",
    "df['Number of turbines'] = df['Number of turbines'].fillna(0).astype(int)\n",
    "df['Total power'] = df['Total power'].astype(float)\n",
    "df['Developer'] = df['Developer'].astype(str)\n",
    "df['Operator'] = df['Operator'].astype(str)\n",
    "df['Owner'] = df['Owner'].astype(str)\n",
    "df['Commissioning date'] = df['Commissioning date'].astype(str)\n",
    "df['Status'] = df['Status'].astype(str)\n",
    "\n",
    "print(f\"number of wind turbines: {len(df['ID'])})\")\n",
    "\n",
    "# Speichere die gefilterten Daten im Parquet-Format (deutlich schneller zu lesen und schreiben, als Excel-Dateien, und auch platzsparender)\n",
    "df.to_parquet(\"data/WPPs/The_Wind_Power.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>8. load WPPs and assignment file and combine the information</h1>\n",
    "<ul>\n",
    "    <li>assignment file matches parquet file (ID_The-Wind-Power) to json file (JSON-ID)</li>\n",
    "    <li>all three files are uploaded and a new json file is created</li>\n",
    "    <li>the rows in the excel file correspond excatly to the rows in the json file (same number)</li>\n",
    "    <li>JSON-IDs in produced JSON-file are those of WPPs with a matching in the wind power database (no \"not found\"), and more specifically those of the first WPPs, in case several WPP production data are added</li>\n",
    "    <li>When the capacity of a wind park changes over time due to extension (see e. g. CLDCW-1), its name and generation unit code number remain the same, but the wind farm must be considered as a new one, resulting in a new row in the excel file / new dictionary in the json file.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Laden der Daten\n",
    "df_wind_power = pd.read_parquet(\"data/WPPs/The_Wind_Power.parquet\")\n",
    "df_assignment = pd.read_excel(\"data/Assignment_manual.xlsx\", sheet_name=\"Sheet1\")\n",
    "with open(r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\production_summary_all.json\", \"r\") as file:\n",
    "    df_json = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number potential WPPs: 116\n",
      "number actual WPPs: 115\n",
      "number suspended WPPs (no name, location, capacity or status not in operation): 1\n",
      "number WPPs after clustering 106\n",
      "Zusammengeführte JSON-Datei wurde erfolgreich gespeichert unter: data/WPPs+production.json\n"
     ]
    }
   ],
   "source": [
    "output_file = \"data/WPPs+production.json\"\n",
    "\n",
    "# Filtere nur Zeilen, bei denen \"ID_The-Wind-Power\" nicht \"not found\" ist\n",
    "df_assignment = df_assignment[df_assignment[\"ID_The-Wind-Power\"] != \"not found\"]\n",
    "\n",
    "# set wirh unique generation unit codes\n",
    "generation_unit_code_set = set(df_assignment['GenerationUnitCode'])\n",
    "\n",
    "# Extrahiere und entpacke alle gültigen IDs aus der Spalte \"ID_The-Wind-Power\"\n",
    "def extract_ids(value):\n",
    "    # Überprüfen, ob der Wert eine Liste ist, und ggf. in einzelne IDs zerlegen\n",
    "    if isinstance(value, str) and value.startswith(\"[\") and value.endswith(\"]\"):\n",
    "        return eval(value)  # Konvertiert die Zeichenkette in eine Liste\n",
    "    elif isinstance(value, (int, str)):\n",
    "        return [int(value)]  # Einzelne IDs werden in eine Liste gewandelt\n",
    "    return []\n",
    "\n",
    "valid_ids = set()\n",
    "df_assignment[\"ID_The-Wind-Power\"].apply(lambda x: valid_ids.update(extract_ids(x)))\n",
    "\n",
    "df_filtered = df_wind_power[df_wind_power['ID'].isin(valid_ids)].copy()\n",
    "actual_ids = set(df_filtered['ID'])\n",
    "suspended_ids = valid_ids - actual_ids\n",
    "\n",
    "print(\"number potential WPPs:\", len(valid_ids))\n",
    "print(\"number actual WPPs:\", len(actual_ids))\n",
    "print(\"number suspended WPPs (no name, location, capacity or status not in operation):\", len(suspended_ids))\n",
    "\n",
    "production_data = [] # neues JSON-File mit Produktionsdaten für die WPPs\n",
    "temporal_wpps = [] # WPPs, die temporär gespeichert werden, um sie später zu aktualisieren\n",
    "\n",
    "# Gehe durch jede Zeile der Assignment-Datei und füge Produktionsdaten hinzu\n",
    "for _, row in df_assignment.iterrows():\n",
    "    \n",
    "    ids_in_row = extract_ids(row[\"ID_The-Wind-Power\"])\n",
    "    first_id = ids_in_row[0] # dismiss other ids in the same row, because the capacity of the WPP is not taken from the wind power database anyway and other statistics should be the same for all indices\n",
    "\n",
    "    if first_id in suspended_ids:\n",
    "        continue # jump to next iteration, because following line would fail for suspended_ids\n",
    "\n",
    "    production_array = df_json[row['JSON-ID']]['Production']\n",
    "    capacity = row['GenerationUnitInstalledCapacity(MW)']\n",
    "\n",
    "    if first_id not in actual_ids: # several lines in assignment files for one WPP in The Wind Power file\n",
    "        if row['GenerationUnitCode'] not in generation_unit_code_set: # another row with the same generation unit code as a previous row --> create new WPP although its first_id is identical, because the capacity differs\n",
    "            pass # continue at current_index = ...\n",
    "        else: # add production data to existing WPP\n",
    "            pass\n",
    "            for _, wpp in enumerate(production_data):\n",
    "                if wpp['ID_The-Wind-Power'] == first_id:\n",
    "\n",
    "                    existing_production = wpp['Production']\n",
    "\n",
    "                    # Vergleiche Zeitstempel und addiere nur bei Übereinstimmung\n",
    "                    i, j = 0, 0  # Zwei Zeiger für existing_production und production_array\n",
    "                    updated_production = []\n",
    "\n",
    "                    while i < len(existing_production) and j < len(production_array):\n",
    "                        time, existing_value = existing_production[i]\n",
    "                        time_comp, new_value = production_array[j]\n",
    "\n",
    "                        if time == time_comp:\n",
    "                            updated_production.append([time, existing_value + new_value])\n",
    "                            i += 1\n",
    "                            j += 1\n",
    "                        elif time < time_comp:\n",
    "                            i += 1\n",
    "                        else:\n",
    "                            j += 1\n",
    "\n",
    "                    if updated_production != []:\n",
    "                        wpp['Production'] = updated_production # update production data (# Ergebnisliste enthält nur Einträge mit übereinstimmenden Zeitstempeln)\n",
    "                        wpp['Capacity'] = wpp['Capacity'] + capacity # update capacity\n",
    "                        temporal_wpps.append(wpp)\n",
    "            continue # don't add another time to the production data\n",
    "    else: # after wpps' production has been changed, treat temporal_wpps. Only possible now, because some wpps were needed multiple times\n",
    "        if len(temporal_wpps) > 0:\n",
    "            for wpp_new in temporal_wpps:\n",
    "                # if available, delete the wpp from production data (recognised by GenerationUnitCode and GenerationUnitInstalledCapacity(MW))\n",
    "                production_data = [wpp for wpp in production_data if not (wpp['Code'] == wpp_new['Code'] and wpp['Capacity'] == wpp_new['Capacity'])]\n",
    "                production_data.append(wpp_new)\n",
    "            temporal_wpps = []\n",
    "\n",
    "    current_index = df_filtered.loc[df_filtered['ID'] == first_id].index[0]\n",
    "\n",
    "    # Daten für das Windkraftwerk hinzufügen\n",
    "    row_data = {\n",
    "        'Name': row['GenerationUnitName'], # from assignment file\n",
    "        'ID_The-Wind-Power': first_id, # from assignment file\n",
    "        'JSON-ID': row['JSON-ID'], # from assignment file\n",
    "        'Code': row['GenerationUnitCode'], # from assignment file\n",
    "        'Type': row['GenerationUnitType'], # from assignment file\n",
    "        'Capacity': capacity, # from assignment file\n",
    "        'Hub_height': df_filtered.at[current_index, \"Hub height\"], # from The Wind Power file\n",
    "        'Commissioning_date': df_filtered.at[current_index, \"Commissioning date\"], # from The Wind Power file\n",
    "        'Number_of_turbines': int(df_filtered.at[current_index, \"Number of turbines\"]), # from The Wind Power file (value only valid for latest WPPs)\n",
    "        'Turbine': df_filtered.at[current_index, \"Turbine\"], # from The Wind Power file\n",
    "        'Latitude': df_filtered.at[current_index, \"Latitude\"], # from The Wind Power file\n",
    "        'Longitude': df_filtered.at[current_index, \"Longitude\"], # from The Wind Power file\n",
    "        'Production': production_array # from JSON file\n",
    "    }\n",
    "\n",
    "    production_data.append(row_data)\n",
    "\n",
    "    # keep track of treated generation unit codes\n",
    "    generation_unit_code_set.discard(row['GenerationUnitCode'])\n",
    "\n",
    "    # keep track of treated IDs to not try deleting rows twice \n",
    "    for id in ids_in_row:\n",
    "        if id in actual_ids:\n",
    "            actual_ids.discard(id)\n",
    "\n",
    "print(\"number WPPs after clustering\", len(production_data))\n",
    "\n",
    "# JSON-Datei speichern\n",
    "with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(production_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Zusammengeführte JSON-Datei wurde erfolgreich gespeichert unter: {output_file}\")\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df_production_data = pd.DataFrame(production_data)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df_production_data.to_excel(\"data/WPPs+production.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>9. Add weather data</h1>\n",
    "<ul>\n",
    "<li>programme execution accelerated by</li>\n",
    "        <ul>\n",
    "                <li>interp2d instead of RegularGridInterpolator</li>\n",
    "                <li>interpolation method linear instead of cubic</li>\n",
    "                <li>only perform the single most time consuming step (extraction of u and v values) once at the beginning outside of any loop</li>\n",
    "                <li>outer iteration over WPPs, even if this means creating a new interpolator not only for each time step, but also for each WPP, because it saves the search for a matching time step among all time steps in the production array of a WPP for all WPPs in the inner WPP loop</li>\n",
    "        </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp2d\n",
    "import warnings\n",
    "import json\n",
    "import xarray as xr\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Basisverzeichnisse für Input und Output\n",
    "input_dir = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\weather_history\"\n",
    "output_dir = \"data\"\n",
    "\n",
    "# Schleife über die Jahre 2015 bis 2024\n",
    "for year in range(2015, 2025):\n",
    "    print(f\"Processing year {year}...\")\n",
    "\n",
    "    # Dateinamen für Input- und Output-Dateien\n",
    "    wind_speed_file = os.path.join(input_dir, f\"{year}.grib\")\n",
    "    output_file_json = os.path.join(output_dir, f\"WPPs+production+wind_{year}.json\")\n",
    "    output_file_excel = os.path.join(output_dir, f\"WPPs+production+wind_{year}.xlsx\")\n",
    "\n",
    "    # Windgeschwindigkeitsdaten laden\n",
    "    wind_speed = xr.open_dataset(wind_speed_file, engine=\"cfgrib\", chunks={\"time\": 100})\n",
    "    times = pd.to_datetime(wind_speed['time'].values)\n",
    "    latitudes = wind_speed['latitude'].values\n",
    "    longitudes = wind_speed['longitude'].values\n",
    "    u = wind_speed['u100'].values\n",
    "    v = wind_speed['v100'].values\n",
    "\n",
    "    # Produktionsdaten laden\n",
    "    WPP_production = pd.read_json(\"data/WPPs+production.json\")\n",
    "\n",
    "    WPP_production_wind = []\n",
    "\n",
    "    # Iteration über alle Windkraftwerke\n",
    "    for i, wpp in WPP_production.iterrows():\n",
    "        lon = wpp['Longitude']\n",
    "        lat = wpp['Latitude']\n",
    "        production = wpp['Production']\n",
    "\n",
    "        # Filtere Produktionsdaten für das aktuelle Jahr\n",
    "        production_subset = [entry for entry in production if str(year) in entry[0]]\n",
    "\n",
    "        if not production_subset:\n",
    "            print(f\"Wind power plant {i+1}/{len(WPP_production)} has no production data for {year}, skipping...\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Wind power plant {i+1}/{len(WPP_production)} for year {year}\")\n",
    "            interpolated_production = []\n",
    "            for j, entry in enumerate(production_subset):\n",
    "                time_str, production_value = entry\n",
    "                timestep = pd.to_datetime(time_str)\n",
    "                if timestep in times:\n",
    "                    time_index = times.get_loc(timestep)\n",
    "\n",
    "                    wind_speeds = np.sqrt(u[time_index]**2 + v[time_index]**2)\n",
    "                    spatial_interpolator = interp2d(longitudes, latitudes, wind_speeds, kind='linear')\n",
    "                    wind_speed_value = spatial_interpolator(lon, lat)[0]\n",
    "                    wind_speed_value = round(wind_speed_value, 2)\n",
    "\n",
    "                    interpolated_production.append([time_str, production_value, wind_speed_value])\n",
    "\n",
    "            # Produktionsdaten aktualisieren\n",
    "            wpp['Production'] = interpolated_production\n",
    "            WPP_production_wind.append(wpp.to_dict())\n",
    "\n",
    "    # Speichere die aktualisierten Produktionsdaten\n",
    "    with open(output_file_json, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(WPP_production_wind, json_file, ensure_ascii=False, indent=4)\n",
    "    print(f\"Updated JSON file for {year} saved to: {output_file_json}\")\n",
    "\n",
    "    # Konvertiere die Liste in einen DataFrame und speichere als Excel\n",
    "    df_WPP_production = pd.DataFrame(WPP_production_wind)\n",
    "    df_WPP_production.to_excel(output_file_excel, index=False)\n",
    "    print(f\"Updated Excel file for {year} saved to: {output_file_excel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>10. Merge all data to one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zusammengeführte JSON-Datei gespeichert unter: C:\\Users\\alexa\\Documents\\Webapp\\data\\WPPs+production+wind.json\n",
      "Zusammengeführte Excel-Datei gespeichert unter: C:\\Users\\alexa\\Documents\\Webapp\\data\\WPPs+production+wind.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Verzeichnis und Dateien\n",
    "input_dir = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\"\n",
    "output_json = os.path.join(input_dir, \"WPPs+production+wind.json\")\n",
    "output_excel = os.path.join(input_dir, \"WPPs+production+wind.xlsx\")\n",
    "\n",
    "# Daten sammeln\n",
    "all_wpp_data = {}\n",
    "columns = [\n",
    "    \"Name\",\n",
    "    \"ID_The-Wind-Power\",\n",
    "    \"JSON-ID\",\n",
    "    \"Code\",\n",
    "    \"Type\",\n",
    "    \"Capacity\",\n",
    "    \"Hub_height\",\n",
    "    \"Commission_date\",\n",
    "    \"Number_of_turbines\",\n",
    "    \"Turbine\",\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "]\n",
    "excel_rows = []\n",
    "json_ids = [] # json ids added to all_wpp_data\n",
    "\n",
    "# Durch die JSON-Dateien iterieren\n",
    "for year in range(2015, 2025):\n",
    "    file_path = os.path.join(input_dir, f\"WPPs+production+wind_{year}.json\")\n",
    "\n",
    "    # Prüfen, ob die Datei existiert\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"Datei {file_path} nicht gefunden, überspringe...\")\n",
    "        continue\n",
    "\n",
    "    # JSON-Datei laden\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        yearly_data = json.load(file)\n",
    "\n",
    "    # Daten verarbeiten\n",
    "    for wpp in yearly_data:\n",
    "        json_id = wpp[\"JSON-ID\"]\n",
    "        if json_id not in json_ids:\n",
    "            all_wpp_data[json_id] = wpp\n",
    "            json_ids.append(json_id)\n",
    "\n",
    "            # Excel-Zeilen sammeln\n",
    "            excel_rows.append(\n",
    "                [\n",
    "                    wpp.get(\"Name\"),\n",
    "                    wpp.get(\"ID_The-Wind-Power\"),\n",
    "                    json_id,\n",
    "                    wpp.get(\"Code\"),\n",
    "                    wpp.get(\"Type\"),\n",
    "                    wpp.get(\"Capacity\"),\n",
    "                    wpp.get(\"Hub_height\"),\n",
    "                    wpp.get(\"Commission_date\"),\n",
    "                    wpp.get(\"Number_of_turbines\"),\n",
    "                    wpp.get(\"Turbine\"),\n",
    "                    wpp.get(\"Latitude\"),\n",
    "                    wpp.get(\"Longitude\"),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # Produktionsdaten anhängen\n",
    "            all_wpp_data[json_id][\"Production\"].extend(wpp[\"Production\"])\n",
    "\n",
    "# JSON-Datei schreiben\n",
    "with open(output_json, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(list(all_wpp_data.values()), file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Excel-Datei schreiben\n",
    "excel_df = pd.DataFrame(excel_rows, columns=columns)\n",
    "excel_df.to_excel(output_excel, index=False)\n",
    "\n",
    "print(f\"Zusammengeführte JSON-Datei gespeichert unter: {output_json}\")\n",
    "print(f\"Zusammengeführte Excel-Datei gespeichert unter: {output_excel}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>11. Manually add as many missing technical specifications as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>12. Adopt technical specifications in JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aktualisierte JSON-Datei gespeichert unter: C:\\Users\\alexa\\Documents\\Webapp\\data\\WPPs+production+wind.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Verzeichnis und Dateien\n",
    "input_dir = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\"\n",
    "json_file = os.path.join(input_dir, \"WPPs+production+wind.json\")\n",
    "excel_file = os.path.join(input_dir, \"WPPs+production+wind.xlsx\")\n",
    "\n",
    "# Dateien laden\n",
    "with open(json_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    wpp_data = json.load(file)\n",
    "\n",
    "excel_df = pd.read_excel(excel_file)\n",
    "\n",
    "# Daten aktualisieren\n",
    "for wpp in wpp_data:\n",
    "    json_id = wpp[\"JSON-ID\"]\n",
    "\n",
    "    # Entsprechenden Excel-Eintrag finden\n",
    "    matching_row = excel_df.loc[excel_df[\"JSON-ID\"] == json_id]\n",
    "\n",
    "    updated_hub_height = matching_row[\"Hub_height\"].values[0]\n",
    "    updated_turbine = matching_row[\"Turbine\"].values[0]\n",
    "\n",
    "    wpp[\"Hub_height\"] = updated_hub_height\n",
    "    wpp[\"Turbine\"] = updated_turbine\n",
    "\n",
    "# Aktualisierte JSON-Datei speichern\n",
    "with open(os.path.join(input_dir, \"WPPs+production+wind.json\"), \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(wpp_data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Aktualisierte JSON-Datei gespeichert unter: {os.path.join(input_dir, \"WPPs+production+wind.json\")}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>13. Verify that datapoints where production > capacity are not unreasonably numerous and significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Überkapazitätswerte erfolgreich gespeichert unter: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\over_capacity_entries.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# JSON-Datei laden\n",
    "with open(f\"data/WPPs+production+wind.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    WPP_production_wind = json.load(file)\n",
    "\n",
    "# Listen zur Speicherung der Ergebnisse\n",
    "entries_with_over_capacity = []\n",
    "\n",
    "# Überkapazitätsprüfung\n",
    "for wpp in WPP_production_wind:\n",
    "    capacity = wpp[\"Capacity\"]\n",
    "    for record in wpp[\"Production\"]:\n",
    "        timestamp, production = record[0], record[1]\n",
    "        if production > capacity:\n",
    "            entries_with_over_capacity.append({\n",
    "                \"Name\": wpp[\"Name\"],\n",
    "                \"Timestamp\": timestamp,\n",
    "                \"Production (MW)\": production,\n",
    "                \"Capacity (MW)\": capacity\n",
    "            })\n",
    "\n",
    "# Wenn Überkapazitätswerte gefunden werden, in Excel speichern\n",
    "output_excel_path = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\over_capacity_entries.xlsx\"\n",
    "\n",
    "if entries_with_over_capacity:\n",
    "    df_over_capacity = pd.DataFrame(entries_with_over_capacity)\n",
    "    df_over_capacity.to_excel(output_excel_path, index=False)\n",
    "    print(f\"Überkapazitätswerte erfolgreich gespeichert unter: {output_excel_path}\")\n",
    "else:\n",
    "    print(\"Keine Werte mit Überkapazität gefunden.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webapp_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
