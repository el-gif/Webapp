{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Build feature and output arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# lists for all data\n",
    "all_turbine_types = []\n",
    "all_hub_heights = []\n",
    "all_capacities = []\n",
    "all_commissioning_dates = []\n",
    "all_production_data = []\n",
    "\n",
    "with open(f\"../data/WPPs+production+wind.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    WPP_production_wind = json.load(file)\n",
    "\n",
    "# collect data\n",
    "for wpp in WPP_production_wind:\n",
    "    all_turbine_types.append([wpp[\"Turbine\"] if pd.notna(wpp[\"Turbine\"]) else \"nan\" for wpp in WPP_production_wind])\n",
    "    all_hub_heights.append(wpp[\"Hub_height\"] if not pd.isna(wpp[\"Hub_height\"]) else 100)\n",
    "    all_capacities.append(wpp[\"Capacity\"])\n",
    "    all_commissioning_dates.append(\"2015/06\" if wpp[\"Commission_date\"] == \"nan\" else f\"{wpp['Commission_date']}/06\" if isinstance(wpp[\"Commission_date\"], str) and \"/\" not in wpp[\"Commission_date\"] else wpp[\"Commission_date\"])\n",
    "    all_production_data.append(wpp[\"Production\"])\n",
    "\n",
    "# One-Hot-Encoding for turbine types\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "turbine_types_onehot = encoder.fit_transform(np.array(all_turbine_types).reshape(-1, 1))\n",
    "\n",
    "# convert to datetime\n",
    "standardised_dates = pd.to_datetime(all_commissioning_dates, format='%Y/%m')\n",
    "\n",
    "# calculate age\n",
    "current_date = pd.Timestamp(\"2024-12-01\")\n",
    "ages = current_date.year * 12 + current_date.month - (standardised_dates.year * 12 + standardised_dates.month)\n",
    "\n",
    "# create combined features and output lists\n",
    "combined_features_raw = []\n",
    "output_raw = []\n",
    "\n",
    "# convert data in feature arrays\n",
    "for idx, production_data in enumerate(all_production_data):\n",
    "    num_rows = len(production_data)\n",
    "\n",
    "    # repetitions for common features\n",
    "    turbine_type_repeated = np.tile(turbine_types_onehot[idx], (num_rows, 1))\n",
    "    hub_height_repeated = np.full((num_rows, 1), all_hub_heights[idx])\n",
    "    age_repeated = np.full((num_rows, 1), ages[idx])\n",
    "\n",
    "    # extract production values and wind speeds\n",
    "    production_values = np.array([entry[1] for entry in production_data]).reshape(-1, 1) / all_capacities[idx]\n",
    "    wind_speeds = np.array([entry[2] for entry in production_data]).reshape(-1, 1)\n",
    "\n",
    "    # combine all features\n",
    "    combined_chunk = np.hstack((\n",
    "        turbine_type_repeated,\n",
    "        hub_height_repeated,\n",
    "        age_repeated,\n",
    "        wind_speeds\n",
    "    ))\n",
    "\n",
    "    # add the data\n",
    "    combined_features_raw.append(combined_chunk)\n",
    "    output_raw.append(production_values)\n",
    "\n",
    "np.save(\"turbine_types_order.npy\", encoder.categories_[0])\n",
    "\n",
    "# combine all data chunks to one array\n",
    "combined_features_raw = np.vstack(combined_features_raw)\n",
    "output_raw = np.vstack(output_raw)\n",
    "\n",
    "# round all values to two decimal places\n",
    "combined_features_raw = np.round(combined_features_raw, decimals=4)\n",
    "output_raw = np.round(output_raw, decimals=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Scale feature vector and define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import joblib\n",
    "\n",
    "combined_features = combined_features_raw.copy()\n",
    "output = output_raw.copy()\n",
    "\n",
    "# Separate Scaler f端r jedes Feature\n",
    "scaler_wind = StandardScaler()\n",
    "scaler_ages = StandardScaler()\n",
    "scaler_hub_heights = StandardScaler()\n",
    "\n",
    "# Skalieren der einzelnen Features\n",
    "combined_features[:, -1] = scaler_wind.fit_transform(combined_features[:, -1].reshape(-1, 1)).flatten() # scale wind speeds\n",
    "combined_features[:, -2] = scaler_ages.fit_transform(combined_features[:, -2].reshape(-1, 1)).flatten()  # scale ages\n",
    "combined_features[:, -3] = scaler_hub_heights.fit_transform(combined_features[:, -3].reshape(-1, 1)).flatten()  # scale hub heights\n",
    "\n",
    "# Speichere alle Scaler in einem Dictionary\n",
    "scalers = {\n",
    "    \"winds\": scaler_wind,\n",
    "    \"ages\": scaler_ages,\n",
    "    \"hub_heights\": scaler_hub_heights,\n",
    "}\n",
    "\n",
    "# Speichere das Dictionary mit Joblib\n",
    "joblib.dump(scalers, \"scalers.pkl\")\n",
    "\n",
    "# Dataset-Klasse f端r PyTorch\n",
    "class WindPowerDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.targets[index]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Erstellung der PyTorch-Datasets\n",
    "dataset = WindPowerDataset(combined_features, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, use_dropout=False, dropout_rate=0.3, \n",
    "                 use_batch_norm=False, activation_fn=nn.ReLU):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # Erste Schicht\n",
    "        layers.append(nn.Linear(input_size, 256))\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(256))\n",
    "        layers.append(activation_fn())\n",
    "\n",
    "        # Zweite Schicht\n",
    "        layers.append(nn.Linear(256, 128))\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(128))\n",
    "        layers.append(activation_fn())\n",
    "\n",
    "        # Dritte Schicht\n",
    "        layers.append(nn.Linear(128, 64))\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(64))\n",
    "        layers.append(activation_fn())\n",
    "\n",
    "        # Dropout nach der letzten versteckten Schicht (optional)\n",
    "        if use_dropout:\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        # Ausgabeschicht\n",
    "        layers.append(nn.Linear(64, 1))\n",
    "\n",
    "        # Modell zusammenstellen\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. With all data: Training for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x00000169A90A9E80>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\alexa\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_22864\\3635992353.py\", line 39, in <forward op>\n    def forward(self, x):\n        return self.model(x)\n               ~~~~~~~~~~ <--- HERE\n  File \"c:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\container.py\", line 250, in forward\n    def forward(self, input):\n        for module in self:\n            input = module(input)\n                    ~~~~~~ <--- HERE\n        return input\n  File \"c:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 125, in forward\n    def forward(self, input: Tensor) -> Tensor:\n        return F.linear(input, self.weight, self.bias)\n               ~~~~~~~~ <--- HERE\nRuntimeError: mat1 and mat2 must have the same dtype, but got Double and Float\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     24\u001b[0m     batch_x, batch_y \u001b[38;5;241m=\u001b[39m batch_x\u001b[38;5;241m.\u001b[39mto(device), batch_y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 25\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n\u001b[0;32m     27\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_22864\\3635992353.py\", line 39, in <forward op>\n    def forward(self, x):\n        return self.model(x)\n               ~~~~~~~~~~ <--- HERE\n  File \"c:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\container.py\", line 250, in forward\n    def forward(self, input):\n        for module in self:\n            input = module(input)\n                    ~~~~~~ <--- HERE\n        return input\n  File \"c:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 125, in forward\n    def forward(self, input: Tensor) -> Tensor:\n        return F.linear(input, self.weight, self.bias)\n               ~~~~~~~~ <--- HERE\nRuntimeError: mat1 and mat2 must have the same dtype, but got Double and Float\n\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import HuberLoss\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# manually define best parameters found in previous step\n",
    "batch_size = 128\n",
    "lr = 0.00010155300193027382\n",
    "num_epochs = 10\n",
    "use_dropout = True\n",
    "dropout_rate = 0.33659592356347234\n",
    "use_batch_norm = False\n",
    "\n",
    "# use static instead of dynamic computational graphs\n",
    "model = torch.jit.script(MLP(input_size=combined_features.shape[1], use_dropout=use_dropout, dropout_rate=dropout_rate, use_batch_norm=use_batch_norm)).to(device)\n",
    "\n",
    "# Trainings-Konfiguration\n",
    "criterion = HuberLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Daten-Loader f端r alle Trainingsdaten\n",
    "train_loader = DataLoader(list(zip(combined_features, output)), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "# Modell speichern\n",
    "torch.save(model.state_dict(), \"trained_parameters.pth\")\n",
    "print(\"Modell f端r Deployment gespeichert!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webapp_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
