{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1. Preprocessing of historical production data: discard data of unwanted power plants, retain monthly files</h1>\n",
    "<p>duplicates are avoided by creating a set unique_windfarms_set and comparing, if already added<br>\n",
    "takes 1 to 2 minutes per month, partially because only values for full hours are retained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bearbeite Datei: E:\\MA_data\\raw production history ENTSO-E\\2019_09_ActualGenerationOutputPerGenerationUnit_16.1.A_r2.1.csv\n",
      "JSON-Datei wurde erfolgreich erstellt: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2019_09.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Basisverzeichnisse\n",
    "input_dir = r\"E:\\MA_data\\raw production history ENTSO-E\"\n",
    "output_dir = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\"\n",
    "\n",
    "# Liste der Monate von 2015-01 bis 2024-10 generieren\n",
    "months = pd.date_range(start=\"2019-09\", end=\"2019-09\", freq=\"MS\").strftime(\"%Y_%m\").tolist()\n",
    "\n",
    "# For-Schleife für jede Datei\n",
    "for month in months:\n",
    "    # Dateipfad erstellen\n",
    "    input_file = os.path.join(input_dir, f\"{month}_ActualGenerationOutputPerGenerationUnit_16.1.A_r2.1.csv\")\n",
    "    output_file = os.path.join(output_dir, f\"production_summary_{month}.json\")\n",
    "\n",
    "    # Überprüfen, ob die Datei existiert\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Datei nicht gefunden: {input_file}\")\n",
    "        continue\n",
    "\n",
    "    # Datei einlesen\n",
    "    print(f\"Bearbeite Datei: {input_file}\")\n",
    "    data = pd.read_csv(input_file, sep='\\t')\n",
    "\n",
    "    # Filtere nach GenerationUnitType == 'Wind Onshore' oder 'Wind Offshore'\n",
    "    filtered_data = data[(data['GenerationUnitType'] == 'Wind Onshore ') | (data['GenerationUnitType'] == 'Wind Offshore ')]\n",
    "\n",
    "    # Konvertiere 'DateTime (UTC)' direkt in das ISO-8601-Format\n",
    "    filtered_data.loc[:, 'DateTime (UTC)'] = pd.to_datetime(filtered_data['DateTime (UTC)']).dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "    # Wichtige Spalten identifizieren, 'AreaCode', 'AreaDisplayName', 'AreaTypeCode' and 'MapCode' of identical WPPs may differ --> use at least one of them as a criterion to identify unique windfarms, and sort out the duplicates manually, because otherwise, the production data are appended twice to the same wind farm\n",
    "    unique_windfarms = filtered_data[['GenerationUnitName', 'GenerationUnitCode', 'GenerationUnitType', 'GenerationUnitInstalledCapacity(MW)', 'AreaCode']].drop_duplicates()\n",
    "    unique_windfarms_set = set(unique_windfarms['GenerationUnitName'])\n",
    "\n",
    "    # Listen für die Produktion zu jeder Stunde hinzufügen\n",
    "    production_data = []\n",
    "    for _, row in unique_windfarms.iterrows():\n",
    "        # Filtern der Daten für das aktuelle Windkraftwerk\n",
    "        windfarm_data = filtered_data[\n",
    "            (filtered_data['GenerationUnitName'] == row['GenerationUnitName']) &\n",
    "            (filtered_data['AreaCode'] == row['AreaCode']) # important to avoid adding to a wind farm production data of all its duplicates\n",
    "        ]\n",
    "\n",
    "        # Erstelle 2D-Array (Liste von Listen) mit Zeit und Produktion, da JSON keine Arrays speichern kann\n",
    "        production_array = [\n",
    "            [time, production]\n",
    "            for time, production in zip(\n",
    "                windfarm_data['DateTime (UTC)'],\n",
    "                windfarm_data['ActualGenerationOutput(MW)']\n",
    "            )\n",
    "            if pd.notna(production) and pd.to_datetime(time).minute == 0  # Nur volle Stunden übernehmen (Resolution of weather data is hourly), although it significantly increases the execution time of the programme, und fehlende Werte überspringen\n",
    "        ]\n",
    "\n",
    "        # Daten für das Windkraftwerk hinzufügen\n",
    "        row_data = {\n",
    "            'GenerationUnitName': row['GenerationUnitName'],\n",
    "            'GenerationUnitCode': row['GenerationUnitCode'],\n",
    "            'GenerationUnitType': row['GenerationUnitType'],\n",
    "            'GenerationUnitInstalledCapacity(MW)': row['GenerationUnitInstalledCapacity(MW)'],\n",
    "            'Production': production_array\n",
    "        }\n",
    "\n",
    "        # don't add duplicates\n",
    "        if row_data['GenerationUnitName'] in unique_windfarms_set:\n",
    "            production_data.append(row_data)\n",
    "            unique_windfarms_set.discard(row_data['GenerationUnitName'])\n",
    "\n",
    "    # JSON-Datei speichern\n",
    "    with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(production_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"JSON-Datei wurde erfolgreich erstellt: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. merge all monthly production data files to one combined file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2015_01.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2015_02.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2015_03.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2015_04.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2015_05.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2015_06.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2015_07.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2015_08.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2015_09.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2015_10.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2015_11.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2015_12.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2016_01.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2016_02.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2016_03.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2016_04.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2016_05.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2016_06.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2016_07.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2016_08.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2016_09.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2016_10.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2016_11.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2016_12.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2017_01.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2017_02.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2017_03.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2017_04.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2017_05.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2017_06.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2017_07.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2017_08.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2017_09.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2017_10.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2017_11.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2017_12.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2018_01.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2018_02.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2018_03.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2018_04.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2018_05.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2018_06.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2018_07.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2018_08.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2018_09.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2018_10.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2018_11.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2018_12.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2019_01.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2019_02.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2019_03.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2019_04.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2019_05.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2019_06.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2019_07.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2019_08.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2019_09.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2019_10.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2019_11.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2019_12.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2020_01.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2020_02.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2020_03.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2020_04.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2020_05.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2020_06.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2020_07.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2020_08.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2020_09.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2020_10.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2020_11.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2020_12.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2021_01.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2021_02.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2021_03.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2021_04.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2021_05.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2021_06.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2021_07.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2021_08.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2021_09.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2021_10.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2021_11.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2021_12.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2022_01.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2022_02.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2022_03.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2022_04.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2022_05.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2022_06.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2022_07.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2022_08.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2022_09.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2022_10.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2022_11.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2022_12.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2023_01.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2023_02.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2023_03.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2023_04.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2023_05.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2023_06.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2023_07.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2023_08.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2023_09.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2023_10.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2023_11.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2023_12.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2024_01.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2024_02.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2024_03.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2024_04.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2024_05.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2024_06.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2024_07.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2024_08.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2024_09.json\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\\production_summary_2024_10.json\n",
      "Zusammengeführte JSON-Datei wurde erfolgreich gespeichert unter: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\production_summary_all.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Verzeichnisse\n",
    "input_dir = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\"\n",
    "output_file = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\production_summary_all.json\"\n",
    "\n",
    "# Liste der Monate von 2015_01 bis 2024_10\n",
    "months = pd.date_range(start=\"2015-01\", end=\"2024-10\", freq=\"MS\").strftime(\"%Y_%m\").tolist()\n",
    "\n",
    "columns_merge = ['GenerationUnitName', 'GenerationUnitCode', 'GenerationUnitType', 'GenerationUnitInstalledCapacity(MW)']\n",
    "final_data = {}\n",
    "\n",
    "# Einlesen der einzelnen Dateien\n",
    "for month in months:\n",
    "    input_file = os.path.join(input_dir, f\"production_summary_{month}.json\")\n",
    "\n",
    "    # Überprüfen, ob die Datei existiert\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Datei nicht gefunden: {input_file}\")\n",
    "        continue\n",
    "\n",
    "    # Datei einlesen\n",
    "    print(f\"Verarbeite Datei: {input_file}\")\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        monthly_data = json.load(file)\n",
    "\n",
    "    # Zusammenführen: Gleiche Windkraftanlagen zusammenführen\n",
    "    for windfarm in monthly_data:\n",
    "        key = tuple(windfarm[col] for col in columns_merge) # unique key per WPP, defined by column_merge (name, code, type, capacity) --> duplicates are not added (already assured in previous cell)\n",
    "        if key not in final_data:\n",
    "            # Neu hinzufügen\n",
    "            final_data[key] = windfarm\n",
    "        else:\n",
    "            # Produktion zusammenführen\n",
    "            final_data[key]['Production'].extend(windfarm['Production'])\n",
    "\n",
    "# Finales JSON-Datenformat vorbereiten\n",
    "merged_data = list(final_data.values())\n",
    "\n",
    "# JSON-Datei speichern\n",
    "with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(merged_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Zusammengeführte JSON-Datei wurde erfolgreich gespeichert unter: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. Perform manual assignment to The Wind Power database indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Excel File with WPPs in JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\alexa\\\\Documents\\\\Webapp\\\\data\\\\production_history\\\\JSON_File.xlsx'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Datei laden\n",
    "file_path = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\production_summary_all.json\"\n",
    "output_excel_path = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\JSON_File.xlsx\"\n",
    "\n",
    "# JSON-Datei einlesen\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# assign an ID to each WPP in the JSON file that corresponds to its position in the list of dictionaries\n",
    "for i, item in enumerate(data):\n",
    "    item['JSON-ID'] = i\n",
    "\n",
    "# Relevante Daten extrahieren\n",
    "df = pd.DataFrame(data)[['GenerationUnitName', 'GenerationUnitCode', 'GenerationUnitType', 'GenerationUnitInstalledCapacity(MW)', 'JSON-ID']]\n",
    "\n",
    "# Nach 'GenerationUnitName' sortieren\n",
    "df_sorted = df.sort_values(by='GenerationUnitName')\n",
    "\n",
    "# Daten in eine Excel-Datei speichern\n",
    "df_sorted.to_excel(output_excel_path, index=False)\n",
    "\n",
    "# Fertigmeldung\n",
    "output_excel_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download table to find common name for UK power plants from website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL der Webseite\n",
    "url = \"https://osuked.github.io/Power-Station-Dictionary/dictionary.html\"\n",
    "\n",
    "# Abrufen der Webseite\n",
    "response = requests.get(url)\n",
    "if response.status_code != 200:\n",
    "    print(\"Fehler beim Abrufen der Webseite.\")\n",
    "    exit()\n",
    "\n",
    "# Parsing der Webseite mit BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Finden aller Tabellen auf der Seite\n",
    "tables = soup.find_all('table')\n",
    "if len(tables) < 3:\n",
    "    print(\"Weniger als 3 Tabellen auf der Seite gefunden.\")\n",
    "    exit()\n",
    "\n",
    "# Die dritte Tabelle auswählen (Index 2, da Python nullbasiert zählt)\n",
    "table = tables[2]\n",
    "\n",
    "# Spaltennamen extrahieren\n",
    "headers = [header.text.strip() for header in table.find_all('th')]\n",
    "\n",
    "# Zeilen extrahieren\n",
    "rows = []\n",
    "for row in table.find_all('tr')[1:]:  # Überspringt die Kopfzeile\n",
    "    cells = [cell.text.strip() for cell in row.find_all('td')]\n",
    "    rows.append(cells)\n",
    "\n",
    "# DataFrame erstellen\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "# Datei speichern\n",
    "output_path = \"data/WPPs/Power_Station_Dictionary.xlsx\"\n",
    "df.to_excel(output_path, index=False)\n",
    "print(f\"Die dritte Tabelle wurde erfolgreich als '{output_path}' gespeichert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add common names from downloaded to assignment file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\alexa\\\\Documents\\\\Webapp\\\\data\\\\Assignment.xlsx'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two provided files\n",
    "file_1_path = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\WPPs\\Power_Station_Dictionary.xlsx\"\n",
    "file_2_path = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\JSON_File.xlsx\"\n",
    "\n",
    "# read the data\n",
    "df1 = pd.read_excel(file_1_path)\n",
    "df2 = pd.read_excel(file_2_path)\n",
    "\n",
    "# introduce new columns at the end\n",
    "df2['Names_UK_Plants'] = None\n",
    "df2['ID_The-Wind-Power'] = None\n",
    "df2['Comment'] = None\n",
    "\n",
    "# Iterate over rows in df2 to match and update the \"Names_UK_Plants\" column\n",
    "for index, row in df2.iterrows():\n",
    "    generation_unit_name = row['GenerationUnitName']\n",
    "    \n",
    "    # Check if this name appears in the \"National Grid BMU ID\" of the first file\n",
    "    matching_rows = df1[df1['National Grid BMU ID'].str.contains(generation_unit_name, na=False, case=False)]\n",
    "    \n",
    "    if not matching_rows.empty:\n",
    "        # Get the \"Common Name\" value(s) and update the \"Names_UK_Plants\" column in df2\n",
    "        common_names = matching_rows['Common Name'].tolist()\n",
    "        df2.at[index, 'Names_UK_Plants'] = ', '.join(common_names)\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "output_path = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\Assignment.xlsx\"\n",
    "df2.to_excel(output_path, index=False)\n",
    "\n",
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perform manual assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load WPPs and assignment file (after manual assignment has been conducted) and combine the information - To Do !!!!!!!!!!<br>\n",
    "assignment file matches parquet file (ID_The-Wind-Power) to json file (JSON-ID)<br>\n",
    "all three files are uploaded and a new json file is created<br>\n",
    "the rows in the excel file correspond excatly to the rows in the json file (same number)<br>\n",
    "JSON-IDs in outgoing JSON-file are those of WPPs with matching in the wind power database (no \"not found\"), and more specifically that of the first WPP when WPP production data are added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Laden der Daten\n",
    "df_wind_power = pd.read_parquet(\"data/WPPs/The_Wind_Power.parquet\")\n",
    "df_assignment = pd.read_excel(\"data/Assignment.xlsx\", sheet_name=\"Sheet1\")\n",
    "with open(r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\production_summary_all.json\", \"r\") as file:\n",
    "    df_json = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number potential WPPs: 116\n",
      "number actual WPPs: 115\n",
      "number suspended WPPs (no name, location, capacity or status not in operation): 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[151], line 90\u001b[0m\n\u001b[0;32m     87\u001b[0m             actual_ids\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mid\u001b[39m)\n\u001b[0;32m     89\u001b[0m actual_cluster_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(production_data)\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber WPPs after clustering\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mactual_cluster_ids\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# JSON-Datei speichern\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "output_file = \"data/WPPs+production.json\"\n",
    "\n",
    "# Filtere nur Zeilen, bei denen \"ID_The-Wind-Power\" nicht \"not found\" ist\n",
    "df_assignment = df_assignment[df_assignment[\"ID_The-Wind-Power\"] != \"not found\"]\n",
    "\n",
    "# set wirh unique generation unit codes\n",
    "generation_unit_code_set = set(df_assignment['GenerationUnitCode'])\n",
    "\n",
    "# Extrahiere und entpacke alle gültigen IDs aus der Spalte \"ID_The-Wind-Power\"\n",
    "def extract_ids(value):\n",
    "    # Überprüfen, ob der Wert eine Liste ist, und ggf. in einzelne IDs zerlegen\n",
    "    if isinstance(value, str) and value.startswith(\"[\") and value.endswith(\"]\"):\n",
    "        return eval(value)  # Konvertiert die Zeichenkette in eine Liste\n",
    "    elif isinstance(value, (int, str)):\n",
    "        return [int(value)]  # Einzelne IDs werden in eine Liste gewandelt\n",
    "    return []\n",
    "\n",
    "valid_ids = set()\n",
    "df_assignment[\"ID_The-Wind-Power\"].apply(lambda x: valid_ids.update(extract_ids(x)))\n",
    "\n",
    "df_filtered = df_wind_power[df_wind_power['ID'].isin(valid_ids)].copy()\n",
    "actual_ids = set(df_filtered['ID'])\n",
    "suspended_ids = valid_ids - actual_ids\n",
    "\n",
    "print(\"number potential WPPs:\", len(valid_ids))\n",
    "print(\"number actual WPPs:\", len(actual_ids))\n",
    "print(\"number suspended WPPs (no name, location, capacity or status not in operation):\", len(suspended_ids))\n",
    "\n",
    "# Erstelle neues JSON-File mit Produktionsdaten für die WPPs\n",
    "production_data = []\n",
    "\n",
    "# Gehe durch jede Zeile der Assignment-Datei und füge Produktionsdaten hinzu\n",
    "for _, row in df_assignment.iterrows():\n",
    "    \n",
    "    ids_in_row = extract_ids(row[\"ID_The-Wind-Power\"])\n",
    "    first_id = ids_in_row[0] # dismiss other ids in the same row, because the capacity of the WPP is not taken from the wind power database anyway and other statistics should be the same for all indices\n",
    "\n",
    "    if first_id in suspended_ids:\n",
    "        continue # jump to next iteration, because following line would fail for suspended_ids\n",
    "\n",
    "    production_array = df_json[row['JSON-ID']]['Production']\n",
    "    capacity = row['GenerationUnitInstalledCapacity(MW)']\n",
    "    if first_id not in actual_ids: # several lines in assignment files for one WPP in The Wind Power file\n",
    "        if row['GenerationUnitCode'] not in generation_unit_code_set: # another row with the same generation unit code as a previous row --> create new WPP although its first_id is identical, because the capacity differs\n",
    "            break # continue at current_index = ...\n",
    "        else: # add production data to existing WPP\n",
    "            for i, wpp in enumerate(production_data):\n",
    "                if wpp['ID_The-Wind-Power'] == first_id:\n",
    "                    existing_production = wpp['Production']\n",
    "                    production_array = [\n",
    "                        [time, existing_value + new_value]\n",
    "                        for (time, existing_value), (time_comp, new_value) in zip(existing_production, production_array)\n",
    "                        if time == time_comp\n",
    "                    ]\n",
    "                    production_data[i]['Production'] = production_array # update production data\n",
    "\n",
    "                    existing_capacity = wpp['Capacity']\n",
    "                    capacity = existing_capacity + capacity\n",
    "                    production_data[i]['Capacity'] = capacity # update capacity\n",
    "    else:\n",
    "        continue # don't add another time to the production data\n",
    "\n",
    "    current_index = df_filtered.loc[df_filtered['ID'] == first_id].index[0]\n",
    "\n",
    "    # Daten für das Windkraftwerk hinzufügen\n",
    "    row_data = {\n",
    "        'Name': row['GenerationUnitName'], # from assignment file\n",
    "        'ID_The-Wind-Power': first_id, # from assignment file\n",
    "        'JSON-ID': row['JSON-ID'], # from assignment file\n",
    "        'Code': row['GenerationUnitCode'], # from assignment file\n",
    "        'Type': row['GenerationUnitType'], # from assignment file\n",
    "        'Capacity': capacity, # from assignment file\n",
    "        'Hub_height': df_filtered.at[current_index, \"Hub height\"], # from The Wind Power file\n",
    "        'Commission_date': df_filtered.at[current_index, \"Commission date\"], # from The Wind Power file\n",
    "        'Number_of_turbines': df_filtered.at[current_index, \"Number of turbines\"], # from The Wind Power file (value only valid for latest WPPs)\n",
    "        'Production': production_array # from JSON file\n",
    "    }\n",
    "\n",
    "    production_data.append(row_data)\n",
    "\n",
    "    # keep track of treated generation unit codes\n",
    "    generation_unit_code_set.discard(row_data['GenerationUnitName'])\n",
    "\n",
    "    # keep track of treated IDs to not try deleting rows twice \n",
    "    for id in ids_in_row:\n",
    "        if id in actual_ids:\n",
    "            actual_ids.discard(id)\n",
    "\n",
    "actual_cluster_ids = len(production_data)\n",
    "print(\"number WPPs after clustering\", len(actual_cluster_ids))\n",
    "\n",
    "# JSON-Datei speichern\n",
    "with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(production_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Zusammengeführte JSON-Datei wurde erfolgreich gespeichert unter: {output_file}\")\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df_production_data = pd.DataFrame(production_data)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df_production_data.to_excel(\"data/WPPs+production.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webapp_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
