{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1. Preprocessing of historical production data: discard data of unwanted power plants, retain monthly files</h1>\n",
    "<p>duplicates are avoided by creating a set unique_windfarms_set and comparing, if already added<br>\n",
    "takes 1 to 2 minutes per month, partially because only values for full hours are retained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Basisverzeichnisse\n",
    "input_dir = r\"E:\\MA_data\\raw production history ENTSO-E\"\n",
    "output_dir = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\"\n",
    "\n",
    "# Liste der Monate von 2015-01 bis 2024-10 generieren\n",
    "months = pd.date_range(start=\"2019-09\", end=\"2019-09\", freq=\"MS\").strftime(\"%Y_%m\").tolist()\n",
    "\n",
    "# For-Schleife für jede Datei\n",
    "for month in months:\n",
    "    # Dateipfad erstellen\n",
    "    input_file = os.path.join(input_dir, f\"{month}_ActualGenerationOutputPerGenerationUnit_16.1.A_r2.1.csv\")\n",
    "    output_file = os.path.join(output_dir, f\"production_summary_{month}.json\")\n",
    "\n",
    "    # Überprüfen, ob die Datei existiert\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Datei nicht gefunden: {input_file}\")\n",
    "        continue\n",
    "\n",
    "    # Datei einlesen\n",
    "    print(f\"Bearbeite Datei: {input_file}\")\n",
    "    data = pd.read_csv(input_file, sep='\\t')\n",
    "\n",
    "    # Filtere nach GenerationUnitType == 'Wind Onshore' oder 'Wind Offshore'\n",
    "    filtered_data = data[(data['GenerationUnitType'] == 'Wind Onshore ') | (data['GenerationUnitType'] == 'Wind Offshore ')]\n",
    "\n",
    "    # Konvertiere 'DateTime (UTC)' direkt in das ISO-8601-Format\n",
    "    filtered_data.loc[:, 'DateTime (UTC)'] = pd.to_datetime(filtered_data['DateTime (UTC)']).dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "    # Wichtige Spalten identifizieren, 'AreaCode', 'AreaDisplayName', 'AreaTypeCode' and 'MapCode' of identical WPPs may differ --> use at least one of them as a criterion to identify unique windfarms, and sort out the duplicates manually, because otherwise, the production data are appended twice to the same wind farm\n",
    "    unique_windfarms = filtered_data[['GenerationUnitName', 'GenerationUnitCode', 'GenerationUnitType', 'GenerationUnitInstalledCapacity(MW)', 'AreaCode']].drop_duplicates()\n",
    "    unique_windfarms_set = set(unique_windfarms['GenerationUnitName'])\n",
    "\n",
    "    # Listen für die Produktion zu jeder Stunde hinzufügen\n",
    "    production_data = []\n",
    "    for _, row in unique_windfarms.iterrows():\n",
    "        # Filtern der Daten für das aktuelle Windkraftwerk\n",
    "        windfarm_data = filtered_data[\n",
    "            (filtered_data['GenerationUnitName'] == row['GenerationUnitName']) &\n",
    "            (filtered_data['AreaCode'] == row['AreaCode']) # important to avoid adding to a wind farm production data of all its duplicates\n",
    "        ]\n",
    "\n",
    "        # Erstelle 2D-Array (Liste von Listen) mit Zeit und Produktion, da JSON keine Arrays speichern kann\n",
    "        production_array = [\n",
    "            [time, production]\n",
    "            for time, production in zip(\n",
    "                windfarm_data['DateTime (UTC)'],\n",
    "                windfarm_data['ActualGenerationOutput(MW)']\n",
    "            )\n",
    "            if pd.notna(production) and pd.to_datetime(time).minute == 0  # Nur volle Stunden übernehmen (Resolution of weather data is hourly), although it significantly increases the execution time of the programme, und fehlende Werte überspringen\n",
    "        ]\n",
    "\n",
    "        # Daten für das Windkraftwerk hinzufügen\n",
    "        row_data = {\n",
    "            'GenerationUnitName': row['GenerationUnitName'],\n",
    "            'GenerationUnitCode': row['GenerationUnitCode'],\n",
    "            'GenerationUnitType': row['GenerationUnitType'],\n",
    "            'GenerationUnitInstalledCapacity(MW)': row['GenerationUnitInstalledCapacity(MW)'],\n",
    "            'Production': production_array\n",
    "        }\n",
    "\n",
    "        # don't add duplicates\n",
    "        if row_data['GenerationUnitName'] in unique_windfarms_set:\n",
    "            production_data.append(row_data)\n",
    "            unique_windfarms_set.discard(row_data['GenerationUnitName'])\n",
    "\n",
    "    # JSON-Datei speichern\n",
    "    with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(production_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"JSON-Datei wurde erfolgreich erstellt: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. merge all monthly production data files to one combined file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Verzeichnisse\n",
    "input_dir = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\JSON\"\n",
    "output_file = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\production_summary_all.json\"\n",
    "\n",
    "# Liste der Monate von 2015_01 bis 2024_10\n",
    "months = pd.date_range(start=\"2015-01\", end=\"2024-10\", freq=\"MS\").strftime(\"%Y_%m\").tolist()\n",
    "\n",
    "columns_merge = ['GenerationUnitName', 'GenerationUnitCode', 'GenerationUnitType', 'GenerationUnitInstalledCapacity(MW)']\n",
    "final_data = {}\n",
    "\n",
    "# Einlesen der einzelnen Dateien\n",
    "for month in months:\n",
    "    input_file = os.path.join(input_dir, f\"production_summary_{month}.json\")\n",
    "\n",
    "    # Überprüfen, ob die Datei existiert\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Datei nicht gefunden: {input_file}\")\n",
    "        continue\n",
    "\n",
    "    # Datei einlesen\n",
    "    print(f\"Verarbeite Datei: {input_file}\")\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        monthly_data = json.load(file)\n",
    "\n",
    "    # Zusammenführen: Gleiche Windkraftanlagen zusammenführen\n",
    "    for windfarm in monthly_data:\n",
    "        key = tuple(windfarm[col] for col in columns_merge) # unique key per WPP, defined by column_merge (name, code, type, capacity) --> duplicates are not added (already assured in previous cell)\n",
    "        if key not in final_data:\n",
    "            # Neu hinzufügen\n",
    "            final_data[key] = windfarm\n",
    "        else:\n",
    "            # Produktion zusammenführen\n",
    "            final_data[key]['Production'].extend(windfarm['Production'])\n",
    "\n",
    "# Finales JSON-Datenformat vorbereiten\n",
    "merged_data = list(final_data.values())\n",
    "\n",
    "# JSON-Datei speichern\n",
    "with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(merged_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Zusammengeführte JSON-Datei wurde erfolgreich gespeichert unter: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. Perform manual assignment to The Wind Power database indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Excel File with WPPs in JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Datei laden\n",
    "file_path = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\production_summary_all.json\"\n",
    "output_excel_path = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\JSON_File.xlsx\"\n",
    "\n",
    "# JSON-Datei einlesen\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# assign an ID to each WPP in the JSON file that corresponds to its position in the list of dictionaries\n",
    "for i, item in enumerate(data):\n",
    "    item['JSON-ID'] = i\n",
    "\n",
    "# Relevante Daten extrahieren\n",
    "df = pd.DataFrame(data)[['GenerationUnitName', 'GenerationUnitCode', 'GenerationUnitType', 'GenerationUnitInstalledCapacity(MW)', 'JSON-ID']]\n",
    "\n",
    "# Nach 'GenerationUnitName' sortieren\n",
    "df_sorted = df.sort_values(by='GenerationUnitName')\n",
    "\n",
    "# Daten in eine Excel-Datei speichern\n",
    "df_sorted.to_excel(output_excel_path, index=False)\n",
    "\n",
    "# Fertigmeldung\n",
    "output_excel_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download table to find common name for UK power plants from website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL der Webseite\n",
    "url = \"https://osuked.github.io/Power-Station-Dictionary/dictionary.html\"\n",
    "\n",
    "# Abrufen der Webseite\n",
    "response = requests.get(url)\n",
    "if response.status_code != 200:\n",
    "    print(\"Fehler beim Abrufen der Webseite.\")\n",
    "    exit()\n",
    "\n",
    "# Parsing der Webseite mit BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Finden aller Tabellen auf der Seite\n",
    "tables = soup.find_all('table')\n",
    "if len(tables) < 3:\n",
    "    print(\"Weniger als 3 Tabellen auf der Seite gefunden.\")\n",
    "    exit()\n",
    "\n",
    "# Die dritte Tabelle auswählen (Index 2, da Python nullbasiert zählt)\n",
    "table = tables[2]\n",
    "\n",
    "# Spaltennamen extrahieren\n",
    "headers = [header.text.strip() for header in table.find_all('th')]\n",
    "\n",
    "# Zeilen extrahieren\n",
    "rows = []\n",
    "for row in table.find_all('tr')[1:]:  # Überspringt die Kopfzeile\n",
    "    cells = [cell.text.strip() for cell in row.find_all('td')]\n",
    "    rows.append(cells)\n",
    "\n",
    "# DataFrame erstellen\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "# Datei speichern\n",
    "output_path = \"data/WPPs/Power_Station_Dictionary.xlsx\"\n",
    "df.to_excel(output_path, index=False)\n",
    "print(f\"Die dritte Tabelle wurde erfolgreich als '{output_path}' gespeichert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add common names from downloaded to assignment file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two provided files\n",
    "file_1_path = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\WPPs\\Power_Station_Dictionary.xlsx\"\n",
    "file_2_path = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\JSON_File.xlsx\"\n",
    "\n",
    "# read the data\n",
    "df1 = pd.read_excel(file_1_path)\n",
    "df2 = pd.read_excel(file_2_path)\n",
    "\n",
    "# introduce new columns at the end\n",
    "df2['Names_UK_Plants'] = None\n",
    "df2['ID_The-Wind-Power'] = None\n",
    "df2['Comment'] = None\n",
    "\n",
    "# Iterate over rows in df2 to match and update the \"Names_UK_Plants\" column\n",
    "for index, row in df2.iterrows():\n",
    "    generation_unit_name = row['GenerationUnitName']\n",
    "    \n",
    "    # Check if this name appears in the \"National Grid BMU ID\" of the first file\n",
    "    matching_rows = df1[df1['National Grid BMU ID'].str.contains(generation_unit_name, na=False, case=False)]\n",
    "    \n",
    "    if not matching_rows.empty:\n",
    "        # Get the \"Common Name\" value(s) and update the \"Names_UK_Plants\" column in df2\n",
    "        common_names = matching_rows['Common Name'].tolist()\n",
    "        df2.at[index, 'Names_UK_Plants'] = ', '.join(common_names)\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "output_path = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\Assignment.xlsx\"\n",
    "df2.to_excel(output_path, index=False)\n",
    "\n",
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perform manual assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load WPPs and assignment file (after manual assignment has been conducted) and combine the information - To Do !!!!!!!!!!<br>\n",
    "assignment file matches parquet file (ID_The-Wind-Power) to json file (JSON-ID)<br>\n",
    "all three files are uploaded and a new json file is created<br>\n",
    "the rows in the excel file correspond excatly to the rows in the json file (same number)<br>\n",
    "JSON-IDs in outgoing JSON-file are those of WPPs with matching in the wind power database (no \"not found\"), and more specifically that of the first WPP when WPP production data are added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Laden der Daten\n",
    "df_wind_power = pd.read_parquet(\"data/WPPs/The_Wind_Power.parquet\")\n",
    "df_assignment = pd.read_excel(\"data/Assignment.xlsx\", sheet_name=\"Sheet1\")\n",
    "with open(r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\production_summary_all.json\", \"r\") as file:\n",
    "    df_json = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"data/WPPs+production.json\"\n",
    "\n",
    "# Filtere nur Zeilen, bei denen \"ID_The-Wind-Power\" nicht \"not found\" ist\n",
    "df_assignment = df_assignment[df_assignment[\"ID_The-Wind-Power\"] != \"not found\"]\n",
    "\n",
    "# set wirh unique generation unit codes\n",
    "generation_unit_code_set = set(df_assignment['GenerationUnitCode'])\n",
    "\n",
    "# Extrahiere und entpacke alle gültigen IDs aus der Spalte \"ID_The-Wind-Power\"\n",
    "def extract_ids(value):\n",
    "    # Überprüfen, ob der Wert eine Liste ist, und ggf. in einzelne IDs zerlegen\n",
    "    if isinstance(value, str) and value.startswith(\"[\") and value.endswith(\"]\"):\n",
    "        return eval(value)  # Konvertiert die Zeichenkette in eine Liste\n",
    "    elif isinstance(value, (int, str)):\n",
    "        return [int(value)]  # Einzelne IDs werden in eine Liste gewandelt\n",
    "    return []\n",
    "\n",
    "valid_ids = set()\n",
    "df_assignment[\"ID_The-Wind-Power\"].apply(lambda x: valid_ids.update(extract_ids(x)))\n",
    "\n",
    "df_filtered = df_wind_power[df_wind_power['ID'].isin(valid_ids)].copy()\n",
    "actual_ids = set(df_filtered['ID'])\n",
    "suspended_ids = valid_ids - actual_ids\n",
    "\n",
    "print(\"number potential WPPs:\", len(valid_ids))\n",
    "print(\"number actual WPPs:\", len(actual_ids))\n",
    "print(\"number suspended WPPs (no name, location, capacity or status not in operation):\", len(suspended_ids))\n",
    "\n",
    "production_data = [] # neues JSON-File mit Produktionsdaten für die WPPs\n",
    "temporal_wpps = [] # WPPs, die temporär gespeichert werden, um sie später zu aktualisieren\n",
    "\n",
    "# Gehe durch jede Zeile der Assignment-Datei und füge Produktionsdaten hinzu\n",
    "for _, row in df_assignment.iterrows():\n",
    "    \n",
    "    ids_in_row = extract_ids(row[\"ID_The-Wind-Power\"])\n",
    "    first_id = ids_in_row[0] # dismiss other ids in the same row, because the capacity of the WPP is not taken from the wind power database anyway and other statistics should be the same for all indices\n",
    "\n",
    "    if first_id in suspended_ids:\n",
    "        continue # jump to next iteration, because following line would fail for suspended_ids\n",
    "\n",
    "    production_array = df_json[row['JSON-ID']]['Production']\n",
    "    capacity = row['GenerationUnitInstalledCapacity(MW)']\n",
    "\n",
    "    if first_id not in actual_ids: # several lines in assignment files for one WPP in The Wind Power file\n",
    "        if row['GenerationUnitCode'] not in generation_unit_code_set: # another row with the same generation unit code as a previous row --> create new WPP although its first_id is identical, because the capacity differs\n",
    "            pass # continue at current_index = ...\n",
    "        else: # add production data to existing WPP\n",
    "            pass\n",
    "            for _, wpp in enumerate(production_data):\n",
    "                if wpp['ID_The-Wind-Power'] == first_id:\n",
    "\n",
    "                    existing_production = wpp['Production']\n",
    "\n",
    "                    # Vergleiche Zeitstempel und addiere nur bei Übereinstimmung\n",
    "                    i, j = 0, 0  # Zwei Zeiger für existing_production und production_array\n",
    "                    updated_production = []\n",
    "\n",
    "                    while i < len(existing_production) and j < len(production_array):\n",
    "                        time, existing_value = existing_production[i]\n",
    "                        time_comp, new_value = production_array[j]\n",
    "\n",
    "                        if time == time_comp:\n",
    "                            updated_production.append([time, existing_value + new_value])\n",
    "                            i += 1\n",
    "                            j += 1\n",
    "                        elif time < time_comp:\n",
    "                            i += 1\n",
    "                        else:\n",
    "                            j += 1\n",
    "\n",
    "                    if updated_production != []:\n",
    "                        wpp['Production'] = updated_production # update production data (# Ergebnisliste enthält nur Einträge mit übereinstimmenden Zeitstempeln)\n",
    "                        wpp['Capacity'] = wpp['Capacity'] + capacity # update capacity\n",
    "                        temporal_wpps.append(wpp)\n",
    "            continue # don't add another time to the production data\n",
    "    else: # after wpps' production has been changed, treat temporal_wpps. Only possible now, because some wpps were needed multiple times\n",
    "        if len(temporal_wpps) > 0:\n",
    "            for wpp_new in temporal_wpps:\n",
    "                # if available, delete the wpp from production data (recognised by GenerationUnitCode and GenerationUnitInstalledCapacity(MW))\n",
    "                production_data = [wpp for wpp in production_data if not (wpp['Code'] == wpp_new['Code'] and wpp['Capacity'] == wpp_new['Capacity'])]\n",
    "                production_data.append(wpp_new)\n",
    "            temporal_wpps = []\n",
    "\n",
    "    current_index = df_filtered.loc[df_filtered['ID'] == first_id].index[0]\n",
    "\n",
    "    # Daten für das Windkraftwerk hinzufügen\n",
    "    row_data = {\n",
    "        'Name': row['GenerationUnitName'], # from assignment file\n",
    "        'ID_The-Wind-Power': first_id, # from assignment file\n",
    "        'JSON-ID': row['JSON-ID'], # from assignment file\n",
    "        'Code': row['GenerationUnitCode'], # from assignment file\n",
    "        'Type': row['GenerationUnitType'], # from assignment file\n",
    "        'Capacity': capacity, # from assignment file\n",
    "        'Hub_height': df_filtered.at[current_index, \"Hub height\"], # from The Wind Power file\n",
    "        'Commission_date': df_filtered.at[current_index, \"Commissioning date\"], # from The Wind Power file\n",
    "        'Number_of_turbines': int(df_filtered.at[current_index, \"Number of turbines\"]), # from The Wind Power file (value only valid for latest WPPs)\n",
    "        'Turbine': df_filtered.at[current_index, \"Turbine\"], # from The Wind Power file\n",
    "        'Latitude': df_filtered.at[current_index, \"Latitude\"], # from The Wind Power file\n",
    "        'Longitude': df_filtered.at[current_index, \"Longitude\"], # from The Wind Power file\n",
    "        'Production': production_array # from JSON file\n",
    "    }\n",
    "\n",
    "    production_data.append(row_data)\n",
    "\n",
    "    # keep track of treated generation unit codes\n",
    "    generation_unit_code_set.discard(row['GenerationUnitCode'])\n",
    "\n",
    "    # keep track of treated IDs to not try deleting rows twice \n",
    "    for id in ids_in_row:\n",
    "        if id in actual_ids:\n",
    "            actual_ids.discard(id)\n",
    "\n",
    "print(\"number WPPs after clustering\", len(production_data))\n",
    "\n",
    "# JSON-Datei speichern\n",
    "with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(production_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Zusammengeführte JSON-Datei wurde erfolgreich gespeichert unter: {output_file}\")\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df_production_data = pd.DataFrame(production_data)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df_production_data.to_excel(\"data/WPPs+production.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>4. Add weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Name of wind_speed_file must remain correct during execution of the next cell, because data are lazy loaded. Segmentation of files into years necessary, because datapoints at large indices of too large files can't be loaded into memory during lazy loading\n",
    "wind_speed_file = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\weather_history\\2015.grib\"\n",
    "# Chunkgröße für die Dimension \"time\" anpassen, sonst funktioniert die Berechnung wind_speeds = np.sqrt(wind_speeds_month['u100']**2 + wind_speeds_month['v100']**2) in der folgenden Zelle nicht\n",
    "wind_speed = xr.open_dataset(wind_speed_file, engine=\"cfgrib\", chunks={\"time\": 100})\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "times = pd.to_datetime(wind_speed['time'].values)\n",
    "latitudes = wind_speed['latitude'].values\n",
    "longitudes = wind_speed['longitude'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp2d\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "WPP_production = pd.read_json(\"data/WPPs+production.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each wind power plant\n",
    "for i, wpp in WPP_production.iterrows():\n",
    "    print(f\"Working on wind power plant {i+1}/{len(WPP_production)}\")\n",
    "    lon = wpp['Longitude']\n",
    "    lat = wpp['Latitude']\n",
    "    production = wpp['Production']\n",
    "\n",
    "    # Filter production data to keep only entries from 2015\n",
    "    production_2015 = [entry for entry in production if '2015' in entry[0]]\n",
    "\n",
    "    # Interpolate wind speeds for each production entry\n",
    "    interpolated_production = []\n",
    "    for j, entry in enumerate(production_2015):\n",
    "        print(f\"Interpolating wind speed for entry {j+1}/{len(production_2015)}\")\n",
    "        time_str, production_value = entry\n",
    "        time = pd.to_datetime(time_str)\n",
    "        if time in times:\n",
    "            time_index = times.get_loc(time)\n",
    "            wind_speeds = np.sqrt(wind_speed['u100'][time_index].values**2 + wind_speed['v100'][time_index].values**2)\n",
    "            spatial_interpolator = interp2d(longitudes, latitudes, wind_speeds, kind='linear')\n",
    "            wind_speed_value = spatial_interpolator(lon, lat)[0]\n",
    "            wind_speed_value = round(wind_speed_value, 2)\n",
    "            interpolated_production.append([time_str, production_value, wind_speed_value])\n",
    "\n",
    "    # Update the production data with interpolated wind speeds\n",
    "    WPP_production.at[index, 'Production'] = interpolated_production\n",
    "\n",
    "# Save the updated production data to a new JSON file\n",
    "output_file = 'data/WPPs+production+wind.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(WPP_production.to_dict(orient='records'), json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Updated JSON file with wind speeds saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import RegularGridInterpolator\n",
    "\n",
    "# Iterate over each wind power plant\n",
    "for index, wpp in WPP_production.iterrows():\n",
    "    print(f\"Working on wind power plant {index+1}/{len(WPP_production)}\")\n",
    "    lon = wpp['Longitude']\n",
    "    lat = wpp['Latitude']\n",
    "    production = wpp['Production']\n",
    "\n",
    "    # Filter production data to keep only entries from 2015\n",
    "    production_2015 = [entry for entry in production if '2015' in entry[0]]\n",
    "\n",
    "    # Interpolate wind speeds for each production entry\n",
    "    interpolated_production = []\n",
    "    for i, entry in enumerate(production_2015):\n",
    "        print(f\"Interpolating wind speed for entry {i+1}/{len(production_2015)}\")\n",
    "        time_str, production_value = entry\n",
    "        time = pd.to_datetime(time_str)\n",
    "        if time in times:\n",
    "            time_index = times.get_loc(time)\n",
    "            wind_speeds = np.sqrt(wind_speed['u100'][time_index].values**2 + wind_speed['v100'][time_index].values**2)\n",
    "            \n",
    "            # Round the wind speeds to reduce precision\n",
    "            wind_speeds = np.round(wind_speeds, 2)\n",
    "            \n",
    "            # Use RegularGridInterpolator for interpolation\n",
    "            spatial_interpolator = RegularGridInterpolator((latitudes, longitudes), wind_speeds, method='cubic')\n",
    "            wind_speed_value = spatial_interpolator((lat, lon))\n",
    "            wind_speed_value = np.round(wind_speed_value, 2)  # Round to two decimal places\n",
    "            interpolated_production.append([time_str, production_value, wind_speed_value])\n",
    "\n",
    "    # Update the production data with interpolated wind speeds\n",
    "    WPP_production.at[index, 'Production'] = interpolated_production\n",
    "\n",
    "# Save the updated production data to a new JSON file\n",
    "output_file = 'data/WPPs+production+wind.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(WPP_production.to_dict(orient='records'), json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Updated JSON file with wind speeds saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time step 1/8760: 2015-01-01 00:00:00\n",
      "Working on wind power plant 1/106\n",
      "Working on wind power plant 2/106\n",
      "Working on wind power plant 3/106\n",
      "Working on wind power plant 4/106\n",
      "Working on wind power plant 5/106\n",
      "Working on wind power plant 6/106\n",
      "Working on wind power plant 7/106\n",
      "Working on wind power plant 8/106\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[194], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m production_2015:\n\u001b[0;32m     29\u001b[0m     time_str, production_value \u001b[38;5;241m=\u001b[39m entry\n\u001b[1;32m---> 30\u001b[0m     entry_time \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m entry_time \u001b[38;5;241m==\u001b[39m time:\n\u001b[0;32m     32\u001b[0m         wind_speed_value \u001b[38;5;241m=\u001b[39m spatial_interpolator(lon, lat)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1101\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1099\u001b[0m         result \u001b[38;5;241m=\u001b[39m convert_listlike(argc, \u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1101\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43marg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, np\u001b[38;5;241m.\u001b[39mbool_):\n\u001b[0;32m   1103\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(result)  \u001b[38;5;66;03m# TODO: avoid this kludge.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:429\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    426\u001b[0m arg \u001b[38;5;241m=\u001b[39m ensure_object(arg)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 429\u001b[0m     \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43m_guess_datetime_format_for_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdayfirst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\webapp_env_conda\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:131\u001b[0m, in \u001b[0;36m_guess_datetime_format_for_array\u001b[1;34m(arr, dayfirst)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (first_non_null \u001b[38;5;241m:=\u001b[39m tslib\u001b[38;5;241m.\u001b[39mfirst_non_null(arr)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(first_non_nan_element \u001b[38;5;241m:=\u001b[39m arr[first_non_null]) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:  \u001b[38;5;66;03m# noqa: E721\u001b[39;00m\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;66;03m# GH#32264 np.str_ object\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m         guessed_format \u001b[38;5;241m=\u001b[39m \u001b[43mguess_datetime_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfirst_non_nan_element\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdayfirst\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m guessed_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m guessed_format\n",
      "File \u001b[1;32mparsing.pyx:947\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.guess_datetime_format\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsing.pyx:660\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.dateutil_parse\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\dateutil\\parser\\_parser.py:740\u001b[0m, in \u001b[0;36mparser._parse\u001b[1;34m(self, timestr, dayfirst, yearfirst, fuzzy, fuzzy_with_tokens)\u001b[0m\n\u001b[0;32m    736\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;66;03m# Numeric token\u001b[39;00m\n\u001b[1;32m--> 740\u001b[0m     i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_numeric_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mymd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuzzy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;66;03m# Check weekday\u001b[39;00m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m info\u001b[38;5;241m.\u001b[39mweekday(l[i]) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\dateutil\\parser\\_parser.py:929\u001b[0m, in \u001b[0;36mparser._parse_numeric_token\u001b[1;34m(self, tokens, idx, info, ymd, res, fuzzy)\u001b[0m\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m len_li \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m12\u001b[39m:\n\u001b[0;32m    927\u001b[0m             res\u001b[38;5;241m.\u001b[39msecond \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(s[\u001b[38;5;241m12\u001b[39m:])\n\u001b[1;32m--> 929\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_hms_idx\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_jump\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;66;03m# HH[ ]h or MM[ ]m or SS[.ss][ ]s\u001b[39;00m\n\u001b[0;32m    931\u001b[0m     hms_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_hms_idx(idx, tokens, info, allow_jump\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    932\u001b[0m     (idx, hms) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_hms(idx, tokens, info, hms_idx)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\dateutil\\parser\\_parser.py:1009\u001b[0m, in \u001b[0;36mparser._find_hms_idx\u001b[1;34m(self, idx, tokens, info, allow_jump)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_find_hms_idx\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx, tokens, info, allow_jump):\n\u001b[0;32m   1007\u001b[0m     len_l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens)\n\u001b[1;32m-> 1009\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m len_l \u001b[38;5;129;01mand\u001b[39;00m \u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1010\u001b[0m         \u001b[38;5;66;03m# There is an \"h\", \"m\", or \"s\" label following this token.  We take\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m         \u001b[38;5;66;03m# assign the upcoming label to the current token.\u001b[39;00m\n\u001b[0;32m   1012\u001b[0m         \u001b[38;5;66;03m# e.g. the \"12\" in 12h\"\u001b[39;00m\n\u001b[0;32m   1013\u001b[0m         hms_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1015\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (allow_jump \u001b[38;5;129;01mand\u001b[39;00m idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m<\u001b[39m len_l \u001b[38;5;129;01mand\u001b[39;00m tokens[idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m           info\u001b[38;5;241m.\u001b[39mhms(tokens[idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;66;03m# There is a space and then an \"h\", \"m\", or \"s\" label.\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m         \u001b[38;5;66;03m# e.g. the \"12\" in \"12 h\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\dateutil\\parser\\_parser.py:338\u001b[0m, in \u001b[0;36mparserinfo.hms\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhms\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 338\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hms[\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m    340\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy.interpolate import interp2d\n",
    "\n",
    "# Interpolierte Produktion für alle Windkraftwerke und Zeitschritte\n",
    "for time_index, time in enumerate(times):\n",
    "    print(f\"Processing time step {time_index + 1}/{len(times)}: {time}\")\n",
    "\n",
    "    # Berechne die Windgeschwindigkeiten für den aktuellen Zeitschritt\n",
    "    wind_speeds = np.sqrt(wind_speed['u100'][time_index]**2 + wind_speed['v100'][time_index]**2)\n",
    "    \n",
    "    # Erstelle den Interpolator für den aktuellen Zeitschritt\n",
    "    spatial_interpolator = interp2d(longitudes, latitudes, wind_speeds, kind='linear')\n",
    "    \n",
    "    # Iteriere über jedes Windkraftwerk\n",
    "    for index, wpp in WPP_production.iterrows():\n",
    "        print(f\"Working on wind power plant {index+1}/{len(WPP_production)}\")\n",
    "        lon = wpp['Longitude']\n",
    "        lat = wpp['Latitude']\n",
    "        production = wpp['Production']\n",
    "\n",
    "        # Filtere Produktionsdaten, um nur Einträge aus 2015 zu behalten\n",
    "        production_2015 = [entry for entry in production if '2015' in entry[0]]\n",
    "\n",
    "        # Interpoliere Windgeschwindigkeiten für jeden Produktionseintrag\n",
    "        interpolated_production = []\n",
    "        for entry in production_2015:\n",
    "            time_str, production_value = entry\n",
    "            entry_time = pd.to_datetime(time_str)\n",
    "            if entry_time == time:\n",
    "                wind_speed_value = spatial_interpolator(lon, lat)[0]\n",
    "                wind_speed_value = round(wind_speed_value, 2)\n",
    "                interpolated_production.append([time_str, production_value, wind_speed_value])\n",
    "\n",
    "        # Aktualisiere die Produktionsdaten mit interpolierten Windgeschwindigkeiten\n",
    "        if interpolated_production:\n",
    "            if 'InterpolatedProduction' not in WPP_production.columns:\n",
    "                WPP_production['InterpolatedProduction'] = [[] for _ in range(len(WPP_production))]\n",
    "            WPP_production.at[index, 'InterpolatedProduction'].extend(interpolated_production)\n",
    "\n",
    "# Speichere die aktualisierten Produktionsdaten in einer neuen JSON-Datei\n",
    "output_file = 'data/WPPs+production+wind.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(WPP_production.to_dict(orient='records'), json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Updated JSON file with wind speeds saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp2d\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "WPP_production = pd.read_excel(\"data/WPPs+production.xlsx\")\n",
    "\n",
    "# all WPPs\n",
    "# ids = WPP_production['ID'].values\n",
    "# lats_plants = WPP_production['Latitude'].values\n",
    "# lons_plants = WPP_production['Longitude'].values\n",
    "\n",
    "# only two WPPs for test reasons\n",
    "ids = WPP_production['ID'].iloc[0:2]\n",
    "lats_plants = WPP_production['Latitude'].iloc[0:2]\n",
    "lons_plants = WPP_production['Longitude'].iloc[0:2]\n",
    "\n",
    "months = [f\"{year}_{month:02d}\" for year in range(2024, 2025) for month in range(10, 11) # range(2015, 2025) for month in range(1, 13)\n",
    "            if f\"{year}_{month:02d}\" in WPP_production.columns]\n",
    "\n",
    "for month in months:\n",
    "    print(f\"month {month}\")\n",
    "\n",
    "    month_data = times[times.strftime('%Y_%m') == month]\n",
    "    start = times.get_loc(month_data[0])\n",
    "    end = times.get_loc(month_data[-1])\n",
    "    wind_speeds_month = wind_speed.isel(time=slice(start, end+1))\n",
    "    # this operation requires chunking\n",
    "    wind_speeds = xr.apply_ufunc(\n",
    "        np.sqrt,\n",
    "        wind_speed['u100']**2 + wind_speed['v100']**2,\n",
    "        dask=\"parallelized\")\n",
    "    wind_speeds = wind_speeds.load()\n",
    "\n",
    "    for j in range(len(ids)):\n",
    "        print(f\"Wind Power Plant {j+1} / {len(ids)}\")\n",
    "        lon = lons_plants[j]\n",
    "        lat = lats_plants[j]\n",
    "        if WPP_production.at[j, month] != \"[]\":  # Check if there is production data\n",
    "            interpolated_wind_speeds = np.zeros(len(month_data))\n",
    "            for i, _ in enumerate(month_data):\n",
    "                wind_speeds_i = wind_speeds[i].values\n",
    "                spatial_interpolator = interp2d(longitudes, latitudes, wind_speeds_i, kind='cubic') # time in first dimension, i. e. wind_speeds[index] = wind_speeds[index, :, :]. Lazy evaluation of spatial_interpolator justifies creation of a new one not only for each time step, but also for each wind power plant\n",
    "                interpolated_value = spatial_interpolator(lon, lat)[0]\n",
    "                interpolated_value = round(interpolated_value, 3) # saves memory and computing resources\n",
    "                interpolated_wind_speeds[i] = interpolated_value\n",
    "\n",
    "            # Daten als NumPy-Array speichern (weniger speicherintensiv und stellt sicher, dass wind_speed und wind_power die gleiche Länge haben, die von wind_speed vorgegeben wird)\n",
    "            production_data = np.array(eval(WPP_production.at[j, month]))  # Vorherige Daten als Array\n",
    "            combined_data = np.stack((production_data, interpolated_wind_speeds), axis=0)\n",
    "            WPP_production.at[j, month] = combined_data  # Kombinierte Daten speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to save\n",
    "columns_to_save = [\n",
    "    'ID', 'Name', '2nd name', 'Latitude', 'Longitude', 'Manufacturer', 'Turbine',\n",
    "    'Hub height', 'Number of turbines', 'Total power', 'Developer', 'Operator',\n",
    "    'Owner', 'Commissioning date', 'Status', '2024_09', '2024_10'\n",
    "]\n",
    "\n",
    "# Filter the DataFrame to include only the specified columns and rows where ids correspond to WPP_production['ID']\n",
    "filtered_WPP_production = WPP_production[WPP_production['ID'].isin(ids)][columns_to_save]\n",
    "\n",
    "# Save the filtered DataFrame to an Excel file\n",
    "filtered_WPP_production.to_excel(\"data/WPPs+production+weather.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webapp_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
