{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720\n",
      "720\n",
      "744\n",
      "744\n",
      "744\n",
      "742\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 2206 but corresponding boolean dimension is 2208",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 105\u001b[0m\n\u001b[0;32m    102\u001b[0m test_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39misin(chunk_indices, test_plants)\n\u001b[0;32m    104\u001b[0m train_features \u001b[38;5;241m=\u001b[39m combined_features[train_indices]\n\u001b[1;32m--> 105\u001b[0m train_targets \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    107\u001b[0m test_features \u001b[38;5;241m=\u001b[39m combined_features[test_indices]\n\u001b[0;32m    108\u001b[0m test_targets \u001b[38;5;241m=\u001b[39m output[test_indices]\n",
      "\u001b[1;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 2206 but corresponding boolean dimension is 2208"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "WPP_production_wind = pd.read_excel(\"data/WPPs+production+wind.xlsx\")\n",
    "\n",
    "#ids = WPP_production_wind['JSON-ID'].values\n",
    "turbine_types = WPP_production_wind['Turbine'].values\n",
    "hub_heights = WPP_production_wind['Hub_height'].values\n",
    "#number_of_turbines = WPP_production_wind['Number_of_turbines'].values\n",
    "capacities = WPP_production_wind['Capacity'].values\n",
    "commissioning_dates = WPP_production_wind['Commissioning_date'].values\n",
    "\n",
    "# One-Hot-Encoding for turbine types\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "turbine_types_onehot = encoder.fit_transform(turbine_types.reshape(-1, 1))\n",
    "\n",
    "# Standardise the dates\n",
    "standardised_dates = np.vectorize(lambda x: x if '/' in x else f\"{x}/06\")(commissioning_dates)\n",
    "\n",
    "# Convert to datetime\n",
    "standardised_dates = pd.to_datetime(standardised_dates, format='%Y/%m')\n",
    "\n",
    "# Calculate ages\n",
    "current_date = pd.Timestamp('2024-12-01')\n",
    "ages = current_date.year * 12 + current_date.month - (standardised_dates.year * 12 + standardised_dates.month)\n",
    "\n",
    "# Array für kombinierte Features erstellen\n",
    "combined_features = []\n",
    "output = []\n",
    "chunk_indices = []\n",
    "chunk_id = 0\n",
    "\n",
    "for idx, row in WPP_production_wind.iterrows():\n",
    "    for month in months:\n",
    "        wind_power_speed = eval(WPP_production_wind.at[idx, month])\n",
    "        if month in WPP_production_wind.columns and wind_power_speed != []:\n",
    "            chunk_id += 1 # every month of a different WPP is a chunk\n",
    "\n",
    "            # Windkraftwerks-spezifische Werte vorbereiten\n",
    "            num_hours = len(wind_power_speed[1])  # Länge der Stunden (z.B. 720 oder 744)\n",
    "            turbine_type_repeated = np.tile(turbine_types_onehot[idx, :], (num_hours, 1))\n",
    "            hub_height_repeated = np.repeat(hub_heights[idx], num_hours)\n",
    "            capacity_repeated = np.repeat(capacities[idx], num_hours)\n",
    "            age_repeated = np.repeat(ages[idx], num_hours)\n",
    "\n",
    "            # Werte für die letzte Spalte (wind_speeds) und Output (wind_power)\n",
    "            wind_speeds = np.array(wind_power_speed[1]).reshape(-1, 1)  # Zeilenvektor -> Spaltenvektor\n",
    "            wind_powers = np.array(wind_power_speed[0]).reshape(-1, 1)  # Zeilenvektor -> Spaltenvektor\n",
    "            print(len(wind_speeds))\n",
    "            print(len(wind_powers))\n",
    "\n",
    "            # Kombinierte Features erstellen\n",
    "            combined_features_chunk = np.column_stack((\n",
    "                turbine_type_repeated,\n",
    "                hub_height_repeated,\n",
    "                capacity_repeated,\n",
    "                age_repeated,\n",
    "                wind_speeds\n",
    "            ))\n",
    "\n",
    "            # Outputs erstellen\n",
    "            output_chunk = wind_powers\n",
    "\n",
    "            # Chunk-Indizes erstellen (einfach `chunk_id` mehrfach zur Liste hinzufügen)\n",
    "            chunk_indices.extend([chunk_id] * num_hours)\n",
    "\n",
    "            # Daten speichern\n",
    "            combined_features.append(combined_features_chunk)\n",
    "            output.append(output_chunk)\n",
    "\n",
    "# Arrays zusammenführen\n",
    "combined_features = np.vstack(combined_features) # Gibt ein 2D-NumPy-Array zurück, in dem die einzelnen Arrays in der Liste vertikal gestapelt werden.\n",
    "output = np.concatenate(output) # Gibt ein 1D-NumPy-Array zurück, in dem die einzelnen Arrays in der Liste hintereinander verknüpft werden.\n",
    "chunk_indices = np.array(chunk_indices)  # Chunk-Indizes als NumPy-Array\n",
    "\n",
    "# Standardisieren der numerischen Features (hub_heights, ages, capacities, wind_speeds)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Die numerischen Spalten, die skaliert werden sollen (die letzten 4)\n",
    "numerical_columns = [turbine_types_onehot.shape[1], turbine_types_onehot.shape[1] + 1,\n",
    "                     turbine_types_onehot.shape[1] + 2, turbine_types_onehot.shape[1] + 3]\n",
    "\n",
    "# Werte extrahieren, skalieren und zurücksetzen\n",
    "combined_features[:, numerical_columns] = scaler.fit_transform(combined_features[:, numerical_columns])\n",
    "\n",
    "# Windkraftwerke aufteilen (Train/Test)\n",
    "train_plants, test_plants = train_test_split(chunk_indices, test_size=0.25, random_state=1)\n",
    "\n",
    "# Trainingsdaten und Testdaten\n",
    "train_indices = np.isin(chunk_indices, train_plants)\n",
    "test_indices = np.isin(chunk_indices, test_plants)\n",
    "\n",
    "train_features = combined_features[train_indices]\n",
    "train_targets = output[train_indices]\n",
    "\n",
    "test_features = combined_features[test_indices]\n",
    "test_targets = output[test_indices]\n",
    "\n",
    "# Dataset erstellen\n",
    "class WindPowerDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.targets[index]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "train_val_dataset = WindPowerDataset(features=train_features, targets=train_targets)\n",
    "test_dataset = WindPowerDataset(features=test_features, targets=test_targets)\n",
    "\n",
    "# MLP-Modell definieren\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.fc1(x))\n",
    "        x = self.leaky_relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# # create lists of wind powers and wind speeds for each WPP, containing sublists for each month\n",
    "# wind_powers = []\n",
    "# wind_speeds = []\n",
    "# for index in range(len(ids)): # for all WPPs. Empty sublists only originate from sparse data which won't be present later on\n",
    "#     wind_powers.append([])\n",
    "#     wind_speeds.append([])\n",
    "#     for month in months:\n",
    "#         data = eval(WPP_production_weather.at[index, month])\n",
    "#         if data != []:\n",
    "#             wind_powers[index].append(data[0] / number_of_turbines[index])\n",
    "#             wind_speeds[index].append(data[1])\n",
    "#     # concatenate sublists\n",
    "#     wind_powers[index] = [item for sublist in wind_powers[index] for item in sublist]\n",
    "#     wind_speeds[index] = [item for sublist in wind_speeds[index] for item in sublist]\n",
    "\n",
    "# # Create chunks\n",
    "# chunks = []\n",
    "# for index in range(len(ids)): # for all WPPs. Empty sublists only originate from sparse data which won't be present later on\n",
    "#     wind_power_speed = eval(WPP_production_weather.at[index, month])\n",
    "#     for month in months:\n",
    "#         wind_power_speed = eval(WPP_production_weather.at[index, month])\n",
    "#         counter = 0\n",
    "#         if data != []:\n",
    "#             counter += 1\n",
    "#             wind_powers[index].append(data[0] / number_of_turbines[index])\n",
    "#             wind_speeds[index].append(data[1])\n",
    "#         chunks.append(data)\n",
    "#     wind_powers.append([])\n",
    "#     wind_speeds.append([])\n",
    "#     for month in months:\n",
    "#         data = eval(WPP_production_weather.at[index, month])\n",
    "#         if data != []:\n",
    "#             wind_powers[index].append(data[0] / number_of_turbines[index])\n",
    "#             wind_speeds[index].append(data[1])\n",
    "#     # concatenate sublists\n",
    "#     wind_powers[index] = [item for sublist in wind_powers[index] for item in sublist]\n",
    "#     wind_speeds[index] = [item for sublist in wind_speeds[index] for item in sublist]\n",
    "\n",
    "# for i, (ws, wp) in enumerate(zip(wind_speeds, wind_powers)):\n",
    "#     for month in months:\n",
    "#         if len(ws) > 0:  # Check if there is data for the month\n",
    "#             ws_scaled = scaler.fit_transform(ws.reshape(-1, 1)).flatten()\n",
    "#             wp_flat = wp.flatten()\n",
    "#             turbine_type_repeated = np.repeat(turbine_types_onehot[i], len(ws)).reshape(-1, turbine_types_onehot.shape[1])\n",
    "#             features_repeated = np.repeat(features_scaled[i], len(ws)).reshape(-1, features_scaled.shape[1])\n",
    "#             combined_features = np.concatenate([turbine_type_repeated, features_repeated, ws_scaled.reshape(-1, 1)], axis=1)\n",
    "#             chunks.append((combined_features, wp_flat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m wind_speeds_scaled \u001b[38;5;241m=\u001b[39m [scaler\u001b[38;5;241m.\u001b[39mfit_transform(np\u001b[38;5;241m.\u001b[39marray(ws)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;28;01mfor\u001b[39;00m ws \u001b[38;5;129;01min\u001b[39;00m wind_speeds]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Flatten der Windgeschwindigkeit\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m wind_speeds_scaled_flat \u001b[38;5;241m=\u001b[39m \u001b[43mwind_speeds_scaled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shape: (t * n, 1)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Wiederhole statische Features und Turbinentypen\u001b[39;00m\n\u001b[0;32m     15\u001b[0m turbine_types_repeated \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(turbine_types_onehot, repeats\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m48\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Shape: (t * n, ...)\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Global scaling of features (global means scaling over all features)\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(np.stack([hub_heights, ages, capacities], axis=1))\n",
    "# Skalierung jeder Liste separat\n",
    "wind_speeds_scaled = [scaler.fit_transform(np.array(ws).reshape(-1, 1)).flatten() for ws in wind_speeds]\n",
    "\n",
    "# Flatten der Windgeschwindigkeit\n",
    "wind_speeds_scaled_flat = wind_speeds_scaled.reshape(-1, 1)  # Shape: (t * n, 1)\n",
    "\n",
    "# Wiederhole statische Features und Turbinentypen\n",
    "turbine_types_repeated = np.repeat(turbine_types_onehot, repeats=48, axis=0)  # Shape: (t * n, ...)\n",
    "features_repeated = np.repeat(features_scaled, repeats=48, axis=0)  # Shape: (t * n, ...)\n",
    "\n",
    "# Kombinierte Eingabedaten\n",
    "combined_features = np.concatenate([turbine_types_repeated, features_repeated, wind_speeds_scaled_flat], axis=1)\n",
    "\n",
    "# Zielwerte flatten\n",
    "wind_powers_flat = wind_powers.T.flatten()  # Shape: (t * n,)\n",
    "\n",
    "# Windkraftwerke aufteilen (Train/Test)\n",
    "chunk_indices = np.arange(number_chunks)\n",
    "train_plants, test_plants = train_test_split(chunk_indices, test_size=0.25, random_state=1)\n",
    "\n",
    "# Trainingsdaten und Testdaten\n",
    "train_indices = np.isin(plant_indices, train_plants)\n",
    "test_indices = np.isin(plant_indices, test_plants)\n",
    "\n",
    "train_features = combined_features[train_indices.repeat(48)]\n",
    "train_targets = wind_powers_flat[train_indices.repeat(48)]\n",
    "\n",
    "test_features = combined_features[test_indices.repeat(48)]\n",
    "test_targets = wind_powers_flat[test_indices.repeat(48)]\n",
    "\n",
    "# Dataset erstellen\n",
    "class WindPowerDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.targets[index]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "train_val_dataset = WindPowerDataset(features=train_features, targets=train_targets)\n",
    "test_dataset = WindPowerDataset(features=test_features, targets=test_targets)\n",
    "\n",
    "# MLP-Modell definieren\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.fc1(x))\n",
    "        x = self.leaky_relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainingseinstellungen\n",
    "number_epochs = 50\n",
    "batch_size = 32 # as a power of 2 for higher efficiency\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "input_size = len(train_val_dataset[0][0]) # train_dataset[0] is a tuple with one tensor with input features and a tensor with output, input_size here 20 + 3 + 1\n",
    "len_train_val_dataset = len(train_val_dataset)\n",
    "\n",
    "# Cross-Validation\n",
    "fold = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for train_idx, val_idx in kf.split(range(len_train_val_dataset)):  # Indizes für KFold\n",
    "    print(f\"Fold {fold}/{kf.n_splits}\")\n",
    "    fold += 1\n",
    "\n",
    "    # Train- und Validierungsdaten erstellen\n",
    "    train_fold_dataset = Subset(train_val_dataset, train_idx)\n",
    "    val_fold_dataset = Subset(train_val_dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_fold_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_fold_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Modell, Loss und Optimizer\n",
    "    model = MLP(input_size=input_size, hidden_size=64, output_size=1).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(number_epochs):\n",
    "        model.train()\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs.squeeze(), batch_y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validierung\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                val_outputs = model(batch_x)\n",
    "                val_loss += criterion(val_outputs.squeeze(), batch_y).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch [{epoch+1}/50], Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Modell speichern\n",
    "torch.save(model.state_dict(), \"mlp_wind_power_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test mit Testdaten\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        test_outputs = model(batch_x)\n",
    "        test_loss += criterion(test_outputs.squeeze(), batch_y).item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webapp_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
