{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from scipy.interpolate import griddata\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from datetime import datetime\n",
    "\n",
    "# Daten laden\n",
    "lons = np.load(\"data/weather_history/COSMO_REA6/lons.npy\").flatten()\n",
    "lats = np.load(\"data/weather_history/COSMO_REA6/lats.npy\").flatten()\n",
    "times = np.load(\"data/weather_history/COSMO_REA6/times.npy\")\n",
    "wind_speeds = np.load(\"data/weather_history/COSMO_REA6/wind_speeds.npy\")\n",
    "\n",
    "# 20 turbine types\n",
    "turbine_type_database = [''.join(random.choices(string.ascii_letters + string.digits, k=5)) for _ in range(20)]\n",
    "\n",
    "# coordinates for Europe (more restrictive, because the covered area in the COSMO_REA6 dataset is not rectangular)\n",
    "lat_min, lat_max = 35, 72\n",
    "lon_min, lon_max = -10, 35\n",
    "\n",
    "# 100 wind power plants\n",
    "lons_plants = np.random.uniform(lon_min, lon_max, 100)\n",
    "lats_plants = np.random.uniform(lat_min, lat_max, 100)\n",
    "turbine_types = np.random.choice(turbine_type_database, size=100)\n",
    "hub_heights = np.random.normal(100, 10, 100)  # mean, standard deviation, number of samples\n",
    "commission_dates = np.random.uniform(1990, 2024, 100)\n",
    "rotor_diameters = np.random.normal(50, 5, 100)\n",
    "capacities = np.random.normal(2000, 300, 100)\n",
    "wind_power = np.random.normal(1000, 150, (48, 100))\n",
    "\n",
    "ages = datetime.now().year * 12 + datetime.now().month - commission_dates * 12\n",
    "points = np.column_stack((lons, lats))  # Eingabekoordinaten als (lon, lat)-Paare\n",
    "\n",
    "wind_speeds_plants = []\n",
    "\n",
    "# interpolation functions\n",
    "for t in range(len(times)):\n",
    "    wind_speeds_plants.append(griddata(points, wind_speeds[t, :, :].flatten(), (lons_plants, lats_plants), method=\"nearest\")) # nearest method much faster than cubic or even linear\n",
    "\n",
    "wind_speeds_plants = np.array(wind_speeds_plants)  # Convert to numpy array (time x plants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot-Encoding für Turbinentypen\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "turbine_types_onehot = encoder.fit_transform(turbine_types.reshape(-1, 1))\n",
    "\n",
    "# Skalierung der Features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(np.stack([hub_heights, ages, rotor_diameters, capacities], axis=1))\n",
    "wind_speeds_plants_scaled = scaler.fit_transform(wind_speeds_plants)\n",
    "\n",
    "# Flatten der Windgeschwindigkeit\n",
    "wind_speeds_plants_scaled_flat = wind_speeds_plants_scaled.reshape(-1, 1)  # Shape: (48 * 100, 1)\n",
    "\n",
    "# Wiederhole statische Features und Turbinentypen\n",
    "turbine_types_repeated = np.repeat(turbine_types_onehot, repeats=48, axis=0)  # Shape: (48 * 100, ...)\n",
    "features_repeated = np.repeat(features_scaled, repeats=48, axis=0)  # Shape: (48 * 100, ...)\n",
    "\n",
    "# Kombinierte Eingabedaten\n",
    "combined_features = np.concatenate([turbine_types_repeated, features_repeated, wind_speeds_plants_scaled_flat], axis=1)\n",
    "\n",
    "# Zielwerte flatten\n",
    "wind_power_flat = wind_power.T.flatten()  # Shape: (48 * 100,)\n",
    "\n",
    "# Windkraftwerke aufteilen (Train/Test)\n",
    "plant_indices = np.arange(100)\n",
    "train_plants, test_plants = train_test_split(plant_indices, test_size=0.25, random_state=42)\n",
    "\n",
    "# Trainingsdaten und Testdaten\n",
    "train_indices = np.isin(plant_indices, train_plants)\n",
    "test_indices = np.isin(plant_indices, test_plants)\n",
    "\n",
    "train_features = combined_features[train_indices.repeat(48)]\n",
    "train_targets = wind_power_flat[train_indices.repeat(48)]\n",
    "\n",
    "test_features = combined_features[test_indices.repeat(48)]\n",
    "test_targets = wind_power_flat[test_indices.repeat(48)]\n",
    "\n",
    "# Dataset erstellen\n",
    "class WindPowerDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.targets[index]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "train_val_dataset = WindPowerDataset(features=train_features, targets=train_targets)\n",
    "test_dataset = WindPowerDataset(features=test_features, targets=test_targets)\n",
    "\n",
    "# MLP-Modell definieren\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.fc1(x))\n",
    "        x = self.leaky_relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainingseinstellungen\n",
    "number_epochs = 50\n",
    "batch_size = 32 # as a power of 2 for higher efficiency\n",
    "kf = KFold(n_splits=5)\n",
    "input_size = len(train_val_dataset[0][0]) # train_dataset[0] is a tuple with two one tensor with input features and a tensor with output, input_size here 20 + 4 + 1\n",
    "len_train_val_dataset = len(train_val_dataset)\n",
    "\n",
    "# Cross-Validation\n",
    "fold = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for train_idx, val_idx in kf.split(range(len_train_val_dataset)):  # Indizes für KFold\n",
    "    print(f\"Fold {fold}/{kf.n_splits}\")\n",
    "    fold += 1\n",
    "\n",
    "    # Train- und Validierungsdaten erstellen\n",
    "    train_fold_dataset = torch.utils.data.Subset(train_val_dataset, train_idx)\n",
    "    val_fold_dataset = torch.utils.data.Subset(train_val_dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_fold_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_fold_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Modell, Loss und Optimizer\n",
    "    model = MLP(input_size=input_size, hidden_size=64, output_size=1).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(number_epochs):\n",
    "        model.train()\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs.squeeze(), batch_y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validierung\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                val_outputs = model(batch_x)\n",
    "                val_loss += criterion(val_outputs.squeeze(), batch_y).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch [{epoch+1}/50], Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Modell speichern\n",
    "torch.save(model.state_dict(), \"mlp_wind_power_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test mit Testdaten\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        test_outputs = model(batch_x)\n",
    "        test_loss += criterion(test_outputs.squeeze(), batch_y).item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webapp_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
