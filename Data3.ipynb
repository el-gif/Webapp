{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bearbeite Datei: E:\\MA_data\\raw production history ENTSO-E\\2015_02_ActualGenerationOutputPerGenerationUnit_16.1.A_r2.1.csv\n",
      "Excel-Datei wurde erfolgreich erstellt: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\production_summary_2015_02.xlsx\n",
      "Bearbeite Datei: E:\\MA_data\\raw production history ENTSO-E\\2015_03_ActualGenerationOutputPerGenerationUnit_16.1.A_r2.1.csv\n",
      "Excel-Datei wurde erfolgreich erstellt: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\production_summary_2015_03.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Basisverzeichnisse\n",
    "input_dir = r\"E:\\MA_data\\raw production history ENTSO-E\"\n",
    "output_dir = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\"\n",
    "\n",
    "# Liste der Monate von 2015-01 bis 2024-10 generieren\n",
    "months = pd.date_range(start=\"2015-02\", end=\"2015-03\", freq=\"MS\").strftime(\"%Y_%m\").tolist()\n",
    "\n",
    "# For-Schleife für jede Datei\n",
    "for month in months:\n",
    "    # Dateipfad erstellen\n",
    "    input_file = os.path.join(input_dir, f\"{month}_ActualGenerationOutputPerGenerationUnit_16.1.A_r2.1.csv\")\n",
    "    output_file = os.path.join(output_dir, f\"production_summary_{month}.xlsx\")\n",
    "\n",
    "    # Überprüfen, ob die Datei existiert\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Datei nicht gefunden: {input_file}\")\n",
    "        continue\n",
    "\n",
    "    # Datei einlesen\n",
    "    print(f\"Bearbeite Datei: {input_file}\")\n",
    "    data = pd.read_csv(input_file, sep='\\t')\n",
    "\n",
    "    # Filtere nach GenerationUnitType == 'Wind Onshore' oder 'Wind Offshore'\n",
    "    filtered_data = data[(data['GenerationUnitType'] == 'Wind Onshore ') | (data['GenerationUnitType'] == 'Wind Offshore ')]\n",
    "\n",
    "    # Konvertiere 'DateTime (UTC)' in datetime\n",
    "    filtered_data.loc[:, 'DateTime (UTC)'] = pd.to_datetime(filtered_data['DateTime (UTC)'])\n",
    "\n",
    "    # Wichtige Spalten identifizieren, 'AreaCode', 'AreaDisplayName', 'AreaTypeCode' and 'MapCode' of identical WPPs may differ --> use at least one of them as a criterion to identify unique windfarms, and sort out the duplicates manually, because otherwise, the production data are appended twice to the same wind farm\n",
    "    unique_windfarms = filtered_data[['GenerationUnitName', 'GenerationUnitCode', 'GenerationUnitType', 'GenerationUnitInstalledCapacity(MW)', 'AreaCode']].drop_duplicates()\n",
    "\n",
    "    # Listen für die Produktion zu jeder Stunde hinzufügen\n",
    "    production_data = []\n",
    "    for _, row in unique_windfarms.iterrows():\n",
    "        # Filtern der Daten für das aktuelle Windkraftwerk\n",
    "        windfarm_data = filtered_data[\n",
    "            (filtered_data['GenerationUnitName'] == row['GenerationUnitName']) &\n",
    "            (filtered_data['AreaCode'] == row['AreaCode']) # important to avoid adding to a wind farm production data of all its duplicates\n",
    "        ]\n",
    "\n",
    "        # Erstelle Liste mit Sublisten mit Zeit und Produktion. 2D-Array nicht sinnvoll, da keine Kommas eingefügt werden und Zeilenumbrüche zwischen den Sublisten entstehen --> Schwiereigkeiten beim erneuten Einlesen\n",
    "        production_array = [\n",
    "            [time, production]\n",
    "            for time, production in zip(\n",
    "                windfarm_data['DateTime (UTC)'],\n",
    "                windfarm_data['ActualGenerationOutput(MW)']\n",
    "            )\n",
    "            if pd.notna(production) and time.minute == 0  # Nur volle Stunden übernehmen (Resolution of weather data is hourly) und fehlende Werte überspringen\n",
    "        ]\n",
    "\n",
    "        # Daten für das Windkraftwerk hinzufügen\n",
    "        row_data = {\n",
    "            'GenerationUnitName': row['GenerationUnitName'],\n",
    "            'GenerationUnitCode': row['GenerationUnitCode'],\n",
    "            'GenerationUnitType': row['GenerationUnitType'],\n",
    "            'GenerationUnitInstalledCapacity(MW)': row['GenerationUnitInstalledCapacity(MW)'],\n",
    "            'Production': production_array\n",
    "        }\n",
    "        production_data.append(row_data)\n",
    "\n",
    "    # DataFrame erstellen und in Excel speichern\n",
    "    output_df = pd.DataFrame(production_data)\n",
    "    output_df.to_excel(output_file, index=False)\n",
    "\n",
    "    print(f\"Excel-Datei wurde erfolgreich erstellt: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\production_summary_2015_01.xlsx\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\production_summary_2015_02.xlsx\n",
      "Verarbeite Datei: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\\production_summary_2015_03.xlsx\n",
      "Zusammengeführte Excel-Tabelle wurde gespeichert unter: C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\production_summary_all.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "\n",
    "# Verzeichnisse\n",
    "input_dir = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\processed_new\"\n",
    "output_file = r\"C:\\Users\\alexa\\Documents\\Webapp\\data\\production_history\\production_summary_all.xlsx\"\n",
    "\n",
    "# Liste der Monate\n",
    "months = pd.date_range(start=\"2015-01\", end=\"2015-03\", freq=\"MS\").strftime(\"%Y_%m\").tolist()\n",
    "columns_merge = ['GenerationUnitName', 'GenerationUnitCode', 'GenerationUnitType', 'GenerationUnitInstalledCapacity(MW)']\n",
    "final_df = pd.DataFrame(columns=columns_merge + ['Production'])\n",
    "\n",
    "# Sicherstellen, dass Production korrekt geparst wird\n",
    "def safe_eval_production(value):\n",
    "    time = []\n",
    "    production = []\n",
    "    for _, element in enumerate(value):\n",
    "        \n",
    "        element = element.replace(\"[\", \"\", 1)\n",
    "        element = element[:-1]\n",
    "        element = element.replace(\"Timestamp(\", \"\").replace(\")\", \"\")\n",
    "        \n",
    "        element = ast.literal_eval(f\"[{element}]\")\n",
    "        \n",
    "        time.append([item[0] for item in element])\n",
    "        production.append([item[1] for item in element])\n",
    "\n",
    "    return time, production\n",
    "\n",
    "# Verarbeitung der Dateien\n",
    "for month in months:\n",
    "    input_file = os.path.join(input_dir, f\"production_summary_{month}.xlsx\")\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Datei nicht gefunden: {input_file}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Verarbeite Datei: {input_file}\")\n",
    "    df = pd.read_excel(input_file)\n",
    "    \n",
    "    # Processing der Production-Spalte\n",
    "    time, production = safe_eval_production(df['Production'])\n",
    "\n",
    "    # Zusammenführen: Gleiche Windkraftanlagen zusammenführen\n",
    "    if final_df.empty:\n",
    "        final_df = df\n",
    "    else:\n",
    "        final_df = final_df.merge(df, on=columns_merge, how='outer', suffixes=('', '_new'))\n",
    "        final_df['Production'] = final_df.apply(\n",
    "            lambda row: (row['Production'] if isinstance(row['Production'], list) else []) +\n",
    "                        (row['Production_new'] if isinstance(row.get('Production_new', []), list) else []),\n",
    "            axis=1\n",
    "        )\n",
    "        final_df.drop(columns=['Production_new'], inplace=True)\n",
    "\n",
    "# Ergebnis speichern\n",
    "final_df.to_excel(output_file, index=False)\n",
    "print(f\"Zusammengeführte Excel-Tabelle wurde gespeichert unter: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['2015-01-01 00:00:00', 39.73], ['2015-01-01 00:30:00', 35.32]] <class 'str'>\n",
      "['2015-01-01 00:00:00', 39.73], ['2015-01-01 00:30:00', 35.32]\n",
      "[['2015-01-01 00:00:00', 39.73], ['2015-01-01 00:30:00', 35.32]] <class 'list'>\n",
      "['2015-01-01 00:00:00', '2015-01-01 00:30:00']\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "a = \"[['2015-01-01 00:00:00', 39.73], ['2015-01-01 00:30:00', 35.32]]\"\n",
    "print(a, type(a))\n",
    "a = a.replace(\"[\", \"\", 1)\n",
    "a = a[:-1]\n",
    "print(a)\n",
    "\n",
    "a = ast.literal_eval(f\"[{a}]\")\n",
    "print(a, type(a))\n",
    "# Daten extrahieren\n",
    "dates = [item[0] for item in a]\n",
    "\n",
    "print(dates)\n",
    "values = [item[1] for item in a]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webapp_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
